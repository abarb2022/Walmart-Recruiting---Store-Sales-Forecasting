{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65025fa2b45144b0ba875080b3fc0409": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_83cb5129ee034977aa2b1090b6e9ef79",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠋\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠋</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "83cb5129ee034977aa2b1090b6e9ef79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NonOZG-sO2Ge",
        "outputId": "b1e2d446-811c-4eee-fbfb-86ad587197fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBBQx6oZeTSo",
        "outputId": "0a5f2d7c-484a-4633-b42b-3fb90f0e3618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "DgWVRrzwea7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "TSux6kCiebnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IcQHU1D7ej98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wRUZjnMerUI",
        "outputId": "1b11bef1-bd54-4ae8-c5ae-0e0ad163501b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 565MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HULksBzPet-k",
        "outputId": "e1529281-970a-46e1-858f-58c36f460e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder # For Type encoding if not using category dtype directly\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc # For garbage collection\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "xuTK-EQZe2Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores = pd.read_csv('stores.csv')\n",
        "train = pd.read_csv(\"train.csv.zip\")\n",
        "features = pd.read_csv('features.csv.zip')\n",
        "sample = pd.read_csv('sampleSubmission.csv.zip')\n",
        "test = pd.read_csv('test.csv.zip')"
      ],
      "metadata": {
        "id": "oE1sZq8me7nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' columns to datetime objects for easier manipulation\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "# Merge features with train and test data.\n",
        "# Note: 'IsHoliday' is present in both train/test and features.csv.\n",
        "# We'll merge on it to ensure consistency, but if there were discrepancies,\n",
        "# we'd need a more careful merge strategy.\n",
        "train_df = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "# Merge store information\n",
        "train_df = pd.merge(train_df, stores, on='Store', how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "\n",
        "print(\"\\n--- Merged Train Data Head ---\")\n",
        "print(train_df.head())\n",
        "print(\"\\n--- Merged Test Data Head ---\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\n--- Merged Train Data Info ---\")\n",
        "print(train_df.info())\n",
        "print(\"\\n--- Merged Test Data Info ---\")\n",
        "print(test_df.info())\n",
        "\n",
        "# After merging but before preprocessing, add this:\n",
        "train_df['IsHoliday'] = train_df['IsHoliday_x'] | train_df['IsHoliday_y']\n",
        "test_df['IsHoliday'] = test_df['IsHoliday_x'] | test_df['IsHoliday_y']\n",
        "\n",
        "# Then drop the redundant columns\n",
        "train_df = train_df.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "test_df = test_df.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "\n",
        "# Free up memory\n",
        "del train, test, features, stores\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrKk86Yue_b3",
        "outputId": "b3c54910-452a-467a-d7dc-e975d6ed401c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merged Train Data Head ---\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday_x  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday_y Type    Size\n",
            "0      1     1 2010-02-05      24924.50        False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106        False    A  151315\n",
            "1      1     1 2010-02-12      46039.49         True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106         True    A  151315\n",
            "2      1     1 2010-02-19      41595.55        False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106        False    A  151315\n",
            "3      1     1 2010-02-26      19403.54        False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106        False    A  151315\n",
            "4      1     1 2010-03-05      21827.90        False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106        False    A  151315\n",
            "\n",
            "--- Merged Test Data Head ---\n",
            "   Store  Dept       Date  IsHoliday_x  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday_y Type    Size\n",
            "0      1     1 2012-11-02        False        55.32       3.386    6766.44    5147.70      50.82    3639.90    2737.42  223.462779         6.573        False    A  151315\n",
            "1      1     1 2012-11-09        False        61.24       3.314   11421.32    3370.89      40.28    4646.79    6154.16  223.481307         6.573        False    A  151315\n",
            "2      1     1 2012-11-16        False        52.92       3.252    9696.28     292.10     103.78    1133.15    6612.69  223.512911         6.573        False    A  151315\n",
            "3      1     1 2012-11-23         True        56.23       3.211     883.59       4.17   74910.32     209.91     303.32  223.561947         6.573         True    A  151315\n",
            "4      1     1 2012-11-30        False        52.34       3.207    2460.03        NaN    3838.35     150.57    6966.34  223.610984         6.573        False    A  151315\n",
            "\n",
            "--- Merged Train Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 17 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         421570 non-null  int64         \n",
            " 1   Dept          421570 non-null  int64         \n",
            " 2   Date          421570 non-null  datetime64[ns]\n",
            " 3   Weekly_Sales  421570 non-null  float64       \n",
            " 4   IsHoliday_x   421570 non-null  bool          \n",
            " 5   Temperature   421570 non-null  float64       \n",
            " 6   Fuel_Price    421570 non-null  float64       \n",
            " 7   MarkDown1     150681 non-null  float64       \n",
            " 8   MarkDown2     111248 non-null  float64       \n",
            " 9   MarkDown3     137091 non-null  float64       \n",
            " 10  MarkDown4     134967 non-null  float64       \n",
            " 11  MarkDown5     151432 non-null  float64       \n",
            " 12  CPI           421570 non-null  float64       \n",
            " 13  Unemployment  421570 non-null  float64       \n",
            " 14  IsHoliday_y   421570 non-null  bool          \n",
            " 15  Type          421570 non-null  object        \n",
            " 16  Size          421570 non-null  int64         \n",
            "dtypes: bool(2), datetime64[ns](1), float64(10), int64(3), object(1)\n",
            "memory usage: 49.0+ MB\n",
            "None\n",
            "\n",
            "--- Merged Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         115064 non-null  int64         \n",
            " 1   Dept          115064 non-null  int64         \n",
            " 2   Date          115064 non-null  datetime64[ns]\n",
            " 3   IsHoliday_x   115064 non-null  bool          \n",
            " 4   Temperature   115064 non-null  float64       \n",
            " 5   Fuel_Price    115064 non-null  float64       \n",
            " 6   MarkDown1     114915 non-null  float64       \n",
            " 7   MarkDown2     86437 non-null   float64       \n",
            " 8   MarkDown3     105235 non-null  float64       \n",
            " 9   MarkDown4     102176 non-null  float64       \n",
            " 10  MarkDown5     115064 non-null  float64       \n",
            " 11  CPI           76902 non-null   float64       \n",
            " 12  Unemployment  76902 non-null   float64       \n",
            " 13  IsHoliday_y   115064 non-null  bool          \n",
            " 14  Type          115064 non-null  object        \n",
            " 15  Size          115064 non-null  int64         \n",
            "dtypes: bool(2), datetime64[ns](1), float64(9), int64(3), object(1)\n",
            "memory usage: 12.5+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "//todo: xgboost optimizes missing values itself so we can try with or without this MissingValueImputer"
      ],
      "metadata": {
        "id": "kCTSTrrSfJPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingMarkdownHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.markdown_cols:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n",
        "                X_copy[col] = X_copy[col].fillna(0)\n",
        "\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "QDy2aJ-tfpDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to handle missing values for specific columns.\n",
        "    - MarkDown columns: fill with 0.\n",
        "    - Other specified numerical columns: fill with ffill then bfill, fallback to mean.\n",
        "    \"\"\"\n",
        "    def __init__(self, numerical_cols_to_impute=None):\n",
        "        self.numerical_cols_to_impute = numerical_cols_to_impute if numerical_cols_to_impute is not None else ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        self.means = {} # To store means for fallback imputation during transform\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate means for fallback imputation from the training data\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X.columns:\n",
        "                self.means[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "        # Impute other numerical columns with ffill then bfill, fallback to mean\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
        "                # Fallback to mean if NaNs still exist (e.g., if all values were NaN in a column)\n",
        "                if X_copy[col].isnull().any() and col in self.means:\n",
        "                    X_copy[col] = X_copy[col].fillna(self.means[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "Nk-pd8apfFrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to extract temporal features from the 'Date' column.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='Date'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        if self.date_column not in X_copy.columns:\n",
        "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
        "\n",
        "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
        "\n",
        "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
        "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
        "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
        "\n",
        "        # Using .dt.isocalendar().week for consistent week numbering across years\n",
        "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
        "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
        "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
        "\n",
        "        X_copy['Week_sin'] = np.sin(2 * np.pi * X_copy['Week'] / 52)\n",
        "        X_copy['Week_cos'] = np.cos(2 * np.pi * X_copy['Week'] / 52)\n",
        "\n",
        "        # Markdown aggregation\n",
        "        X_copy['Total_MarkDown'] = X_copy[[f'MarkDown{i}' for i in range(1, 6)]].sum(axis=1)\n",
        "        X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (X_copy['Total_MarkDown'].mean() + 1)\n",
        "\n",
        "        # Economic indicators\n",
        "        X_copy['Fuel_CPI_Ratio'] = X_copy['Fuel_Price'] / X_copy['CPI']\n",
        "        X_copy['Economic_Index'] = (X_copy['CPI'] * 0.4 + (100 - X_copy['Unemployment']) * 0.6) / 100\n",
        "\n",
        "\n",
        "        # Convert IsHoliday to integer if it exists and is boolean\n",
        "        if 'IsHoliday' in X_copy.columns and X_copy['IsHoliday'].dtype == bool:\n",
        "            X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
        "\n",
        "        # Keep the 'Date' column for ARIMA\n",
        "        return X_copy # Removed .drop(columns=[self.date_column, \"Month\", \"Week\"])\n"
      ],
      "metadata": {
        "id": "qtlbTIpZf9U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "//todo: can change this to other encoders"
      ],
      "metadata": {
        "id": "3C9n_n4tltOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to encode categorical features for XGBoost.\n",
        "    XGBoost works better with label-encoded categoricals than pandas categories.\n",
        "    \"\"\"\n",
        "    def __init__(self, categorical_cols=None):\n",
        "        self.categorical_cols = categorical_cols if categorical_cols is not None else ['Store', 'Dept', 'Type']\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X.columns:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                self.label_encoders[col].fit(X[col].astype(str))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns and col in self.label_encoders:\n",
        "                # Handle unseen categories by using a default value\n",
        "                X_copy[col] = X_copy[col].astype(str)\n",
        "                known_categories = set(self.label_encoders[col].classes_)\n",
        "                X_copy[col] = X_copy[col].apply(lambda x: x if x in known_categories else 'unknown')\n",
        "\n",
        "                # Add 'unknown' to encoder if needed\n",
        "                if 'unknown' not in self.label_encoders[col].classes_:\n",
        "                    current_classes = list(self.label_encoders[col].classes_)\n",
        "                    current_classes.append('unknown')\n",
        "                    self.label_encoders[col].classes_ = np.array(current_classes)\n",
        "\n",
        "                X_copy[col] = self.label_encoders[col].transform(X_copy[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "egIpUsRElsJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "//todo: lest use target encoder instead of label encoder"
      ],
      "metadata": {
        "id": "UE1DNOhQ9n21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XGBoostTargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, categorical_cols=None, smoothing=1.0):\n",
        "        self.categorical_cols = categorical_cols if categorical_cols is not None else ['Store', 'Dept', 'Type']\n",
        "        self.smoothing = smoothing\n",
        "        self.target_encoders = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X.columns:\n",
        "                self.target_encoders[col] = TargetEncoder(smoothing=self.smoothing)\n",
        "                self.target_encoders[col].fit(X[col], y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns and col in self.target_encoders:\n",
        "                X_copy[col] = self.target_encoders[col].transform(X_copy[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "DeesxJ2y9hVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q dagshub"
      ],
      "metadata": {
        "id": "Ds0k_6iNTW4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdcab31f-e100-4ea2-8c32-da4ffe0c3e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow==2.7.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw1Jf1RGTbi2",
        "outputId": "49a5300d-7c6a-4149-b7c4-4e4ad10e12e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlflow==2.7.1 in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.2.1)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.18.0)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.44)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.0.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (4.25.8)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2023.4)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.32.3)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (23.2)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.11.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.5.3)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.16.2)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.1.3)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.3.3)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.26.4)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.15.3)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.2.2)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.2.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.0.41)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.6.1)\n",
            "Requirement already satisfied: pyarrow<14,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (13.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.8.2)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.10.0)\n",
            "Requirement already satisfied: gunicorn<22 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (21.2.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.6)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (4.14.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (3.3.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (1.17.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.4.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==2.7.1) (1.8.0)\n",
            "Requirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (1.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==2.7.1) (4.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.7.1) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow==2.7.1) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow==2.7.1) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.7.1) (3.2.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.7.1) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dagshub\n",
        "# Try to get credentials from environment first\n",
        "dagshub.init(\n",
        "    repo_owner='abarb22',\n",
        "    repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "    mlflow=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "65025fa2b45144b0ba875080b3fc0409",
            "83cb5129ee034977aa2b1090b6e9ef79"
          ]
        },
        "id": "MZNfbjLR-x5x",
        "outputId": "0770114c-65a3-4786-f3b7-e74aaf73eed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65025fa2b45144b0ba875080b3fc0409"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=d9a72f37-0de1-4ed0-9b6d-a282b90c0535&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=95a6593ddd55274f50d9439490e0bce8749b867c3aa9dec203707fd88e202dca\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as alaki22\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as alaki22\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Build the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('missing_markdown', MissingMarkdownHandler()),\n",
        "    ('missing_imputer', MissingValueImputer()),\n",
        "    ('date_features', DateFeatureExtractor()),\n",
        "    ('label_encoder', XGBoostLabelEncoder())  # or XGBoostTargetEncoder if you want target encoding\n",
        "])\n",
        "\n",
        "# Prepare the data\n",
        "print(\"Preparing training data...\")\n",
        "X_train = train_df.drop(['Weekly_Sales'], axis=1)\n",
        "y_train = train_df['Weekly_Sales']\n",
        "\n",
        "print(\"\\n--- Applying Preprocessing Pipeline to Train Data ---\")\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
        "\n",
        "# Remove raw Date column since we've already extracted meaningful features from it\n",
        "# The DateFeatureExtractor already created Year, Month, Week, Day, etc.\n",
        "# Raw datetime objects aren't useful numerical features for XGBoost\n",
        "if 'Date' in X_train_processed.columns:\n",
        "    dates = X_train_processed['Date']  # Keep for potential time-based validation\n",
        "    X_train_processed = X_train_processed.drop(['Date'], axis=1)\n",
        "\n",
        "print(f\"Training data shape: {X_train_processed.shape}\")\n",
        "print(f\"Features: {list(X_train_processed.columns)}\")\n",
        "\n",
        "print(\"\\n--- Applying Preprocessing Pipeline to Test Data ---\")\n",
        "# For the test set, we only call transform, as fit was done on the training data.\n",
        "X_test_processed = preprocessing_pipeline.transform(test_df.drop(columns=['Id'], errors='ignore'))\n",
        "\n",
        "if 'Date' in X_test_processed.columns:\n",
        "    dates = X_test_processed['Date']  # Keep for potential time-based validation\n",
        "    X_test_processed = X_test_processed.drop(['Date'], axis=1)\n",
        "\n",
        "print(\"\\nProcessed X_train_processed info:\")\n",
        "print(X_train_processed.info())\n",
        "print(\"\\nProcessed X_test_processed info:\")\n",
        "print(X_test_processed.info())\n",
        "\n",
        "# Verify no missing values in processed data\n",
        "print(\"\\nMissing values in processed X_train_processed:\\n\", X_train_processed.isnull().sum().sum())\n",
        "print(\"Missing values in processed X_test_processed:\\n\", X_test_processed.isnull().sum().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKzC5HozUvS5",
        "outputId": "eba88859-3808-4a0a-ef67-b0e128dcce38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training data...\n",
            "\n",
            "--- Applying Preprocessing Pipeline to Train Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-1560023455.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (421570, 32)\n",
            "Features: ['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'IsHoliday', 'MarkDown1_was_missing', 'MarkDown2_was_missing', 'MarkDown3_was_missing', 'MarkDown4_was_missing', 'MarkDown5_was_missing', 'Year', 'Month', 'Month_sin', 'Month_cos', 'Week', 'Day', 'DayOfWeek', 'Week_sin', 'Week_cos', 'Total_MarkDown', 'MarkDown_Intensity', 'Fuel_CPI_Ratio', 'Economic_Index']\n",
            "\n",
            "--- Applying Preprocessing Pipeline to Test Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-1560023455.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed X_train_processed info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 32 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   Store                  421570 non-null  int64  \n",
            " 1   Dept                   421570 non-null  int64  \n",
            " 2   Temperature            421570 non-null  float64\n",
            " 3   Fuel_Price             421570 non-null  float64\n",
            " 4   MarkDown1              421570 non-null  float64\n",
            " 5   MarkDown2              421570 non-null  float64\n",
            " 6   MarkDown3              421570 non-null  float64\n",
            " 7   MarkDown4              421570 non-null  float64\n",
            " 8   MarkDown5              421570 non-null  float64\n",
            " 9   CPI                    421570 non-null  float64\n",
            " 10  Unemployment           421570 non-null  float64\n",
            " 11  Type                   421570 non-null  int64  \n",
            " 12  Size                   421570 non-null  int64  \n",
            " 13  IsHoliday              421570 non-null  int64  \n",
            " 14  MarkDown1_was_missing  421570 non-null  int64  \n",
            " 15  MarkDown2_was_missing  421570 non-null  int64  \n",
            " 16  MarkDown3_was_missing  421570 non-null  int64  \n",
            " 17  MarkDown4_was_missing  421570 non-null  int64  \n",
            " 18  MarkDown5_was_missing  421570 non-null  int64  \n",
            " 19  Year                   421570 non-null  int32  \n",
            " 20  Month                  421570 non-null  int32  \n",
            " 21  Month_sin              421570 non-null  float64\n",
            " 22  Month_cos              421570 non-null  float64\n",
            " 23  Week                   421570 non-null  int64  \n",
            " 24  Day                    421570 non-null  int32  \n",
            " 25  DayOfWeek              421570 non-null  int32  \n",
            " 26  Week_sin               421570 non-null  float64\n",
            " 27  Week_cos               421570 non-null  float64\n",
            " 28  Total_MarkDown         421570 non-null  float64\n",
            " 29  MarkDown_Intensity     421570 non-null  float64\n",
            " 30  Fuel_CPI_Ratio         421570 non-null  float64\n",
            " 31  Economic_Index         421570 non-null  float64\n",
            "dtypes: float64(17), int32(4), int64(11)\n",
            "memory usage: 96.5 MB\n",
            "None\n",
            "\n",
            "Processed X_test_processed info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 32 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   Store                  115064 non-null  int64  \n",
            " 1   Dept                   115064 non-null  int64  \n",
            " 2   Temperature            115064 non-null  float64\n",
            " 3   Fuel_Price             115064 non-null  float64\n",
            " 4   MarkDown1              115064 non-null  float64\n",
            " 5   MarkDown2              115064 non-null  float64\n",
            " 6   MarkDown3              115064 non-null  float64\n",
            " 7   MarkDown4              115064 non-null  float64\n",
            " 8   MarkDown5              115064 non-null  float64\n",
            " 9   CPI                    115064 non-null  float64\n",
            " 10  Unemployment           115064 non-null  float64\n",
            " 11  Type                   115064 non-null  int64  \n",
            " 12  Size                   115064 non-null  int64  \n",
            " 13  IsHoliday              115064 non-null  int64  \n",
            " 14  MarkDown1_was_missing  115064 non-null  int64  \n",
            " 15  MarkDown2_was_missing  115064 non-null  int64  \n",
            " 16  MarkDown3_was_missing  115064 non-null  int64  \n",
            " 17  MarkDown4_was_missing  115064 non-null  int64  \n",
            " 18  MarkDown5_was_missing  115064 non-null  int64  \n",
            " 19  Year                   115064 non-null  int32  \n",
            " 20  Month                  115064 non-null  int32  \n",
            " 21  Month_sin              115064 non-null  float64\n",
            " 22  Month_cos              115064 non-null  float64\n",
            " 23  Week                   115064 non-null  int64  \n",
            " 24  Day                    115064 non-null  int32  \n",
            " 25  DayOfWeek              115064 non-null  int32  \n",
            " 26  Week_sin               115064 non-null  float64\n",
            " 27  Week_cos               115064 non-null  float64\n",
            " 28  Total_MarkDown         115064 non-null  float64\n",
            " 29  MarkDown_Intensity     115064 non-null  float64\n",
            " 30  Fuel_CPI_Ratio         115064 non-null  float64\n",
            " 31  Economic_Index         115064 non-null  float64\n",
            "dtypes: float64(17), int32(4), int64(11)\n",
            "memory usage: 26.3 MB\n",
            "None\n",
            "\n",
            "Missing values in processed X_train_processed:\n",
            " 0\n",
            "Missing values in processed X_test_processed:\n",
            " 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature names after preprocessing\n",
        "features_after_pipeline = X_train_processed.columns.tolist()\n",
        "categorical_features_after_pipeline = [col for col in ['Store', 'Dept', 'Type'] if col in features_after_pipeline]\n",
        "\n",
        "# Prepare weights for training (Walmart competition uses WMAE - holiday weeks get 5x weight)\n",
        "# We need the 'IsHoliday' column which is now an integer from DateFeatureExtractor\n",
        "train_weights = np.where(X_train_processed['IsHoliday'] == 1, 5, 1)\n",
        "\n",
        "# Store test IDs for submission\n",
        "test_ids = test_df['Store'].astype(str) + '_' + test_df['Dept'].astype(str) + '_' + test_df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Sort processed data by date for proper time-series splitting\n",
        "# We need to re-attach Date for splitting\n",
        "temp_train_df = X_train_processed.copy()\n",
        "temp_train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "\n",
        "# Get original dates back for sorting\n",
        "temp_train_df['Weekly_Sales'] = y_train\n",
        "temp_train_df = temp_train_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Define a cutoff date for validation (avoid random splits in time series)\n",
        "validation_cutoff_date = pd.to_datetime('2012-09-01')\n",
        "\n",
        "# Remove Date column from features list for training\n",
        "features_for_training = [col for col in features_after_pipeline if col != 'Date']\n",
        "\n",
        "# # Split data based on time\n",
        "X_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date][features_for_training]\n",
        "y_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date]['Weekly_Sales']\n",
        "X_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date][features_for_training]\n",
        "y_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date]['Weekly_Sales']\n",
        "\n",
        "# Calculate weights for validation split\n",
        "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "val_weights = np.where(X_val_split['IsHoliday'] == 1, 5, 1)\n",
        "train_weights_split = np.where(X_train_split['IsHoliday'] == 1, 5, 1)"
      ],
      "metadata": {
        "id": "bojaIRlCjPE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import make_scorer\n",
        "\n",
        "# wmae_scorer = make_scorer(weighted_mean_absolute_error, greater_is_better=False)"
      ],
      "metadata": {
        "id": "niASy2XNVd3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Base model parameters (cleaned up)\n",
        "xgb_base_params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 1,\n",
        "    'tree_method': 'hist',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'eval_metric': 'mae',\n",
        "    'early_stopping_rounds': 50  # Move here\n",
        "}\n",
        "\n",
        "# Grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [6, 8],\n",
        "    'learning_rate': [0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Best tracking\n",
        "best_score = float('inf')\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# Start MLflow experiment\n",
        "mlflow.set_experiment(\"XGBoost_Training\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_HyperParameter_Tuning\"):\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        subrun_name = f\"depth={params['max_depth']}_lr={params['learning_rate']}\"\n",
        "        with mlflow.start_run(run_name=subrun_name, nested=True):\n",
        "            model_params = {**xgb_base_params, **params}\n",
        "            mlflow.log_params(model_params)\n",
        "\n",
        "            model = xgb.XGBRegressor(**model_params)\n",
        "\n",
        "            model.fit(\n",
        "                X_train_split,\n",
        "                y_train_split,\n",
        "                sample_weight=train_weights_split,\n",
        "                eval_set=[(X_val_split, y_val_split)],\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Predict and evaluate\n",
        "            val_preds = model.predict(X_val_split)\n",
        "            train_preds = model.predict(X_train_split)\n",
        "\n",
        "            val_wmae = weighted_mean_absolute_error(y_val_split, val_preds, val_weights)\n",
        "            train_wmae = weighted_mean_absolute_error(y_train_split, train_preds, train_weights_split)\n",
        "\n",
        "            # Log metrics\n",
        "            mlflow.log_metrics({\n",
        "                \"val_wmae\": val_wmae,\n",
        "                \"train_wmae\": train_wmae,\n",
        "                \"best_iteration\": model.best_iteration,\n",
        "                \"n_estimators_used\": model.best_iteration + 1,  # +1 because iterations are 0-indexed\n",
        "                \"total_estimators\": model.n_estimators\n",
        "            })\n",
        "\n",
        "            # Log model\n",
        "            signature = mlflow.models.infer_signature(X_train_split, train_preds)\n",
        "            mlflow.xgboost.log_model(model, \"model\", signature=signature)\n",
        "\n",
        "            print(f\"WMAE - Train: {train_wmae:.4f} | Val: {val_wmae:.4f} | Params: {params}\")\n",
        "\n",
        "            if val_wmae < best_score:\n",
        "                best_score = val_wmae\n",
        "                best_params = model_params\n",
        "                best_model = model\n",
        "\n",
        "    # Log best score and best params\n",
        "    mlflow.log_metric(\"best_val_wmae\", best_score)\n",
        "    mlflow.log_params({\"best_\" + k: v for k, v in best_params.items()})\n",
        "\n",
        "    # Log best model in separate subrun\n",
        "    with mlflow.start_run(run_name=\"Best_Model\", nested=True):\n",
        "        mlflow.log_params(best_params)\n",
        "        mlflow.log_metrics({\n",
        "            \"val_wmae\": best_score,\n",
        "            \"best_iteration\": best_model.best_iteration\n",
        "        })\n",
        "\n",
        "        signature = mlflow.models.infer_signature(X_train_split, best_model.predict(X_train_split))\n",
        "        mlflow.xgboost.log_model(\n",
        "            best_model,\n",
        "            \"best_model\",\n",
        "            signature=signature,\n",
        "            input_example=X_train_split.iloc[:1]\n",
        "        )\n",
        "        print(f\"Best model logged with WMAE: {best_score:.4f}\")\n",
        "\n",
        "# Final print\n",
        "print(\"\\nFinal Results:\")\n",
        "print(f\"Best WMAE: {best_score:.4f}\")\n",
        "print(\"Best parameters:\", {k: v for k, v in best_params.items() if k in param_grid})"
      ],
      "metadata": {
        "id": "BBaT1MAEXC-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdd8371-5a14-4afa-fa6f-1483a8d5732e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  inputs = _infer_schema(model_input) if model_input is not None else None\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:03:40] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "2025/07/04 18:03:45 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmphw7cxvgd/model, flavor: xgboost), fall back to return ['xgboost==2.1.4']. Set logging level to DEBUG to see the full traceback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WMAE - Train: 4201.0748 | Val: 3915.0076 | Params: {'learning_rate': 0.05, 'max_depth': 6}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  inputs = _infer_schema(model_input) if model_input is not None else None\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:04:11] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "2025/07/04 18:04:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp6w03g3h7/model, flavor: xgboost), fall back to return ['xgboost==2.1.4']. Set logging level to DEBUG to see the full traceback.\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:16: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WMAE - Train: 2783.5565 | Val: 2764.9281 | Params: {'learning_rate': 0.05, 'max_depth': 8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  inputs = _infer_schema(model_input) if model_input is not None else None\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:04:33] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:16: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WMAE - Train: 3542.0185 | Val: 3391.6105 | Params: {'learning_rate': 0.1, 'max_depth': 6}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  inputs = _infer_schema(model_input) if model_input is not None else None\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:05:01] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:16: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WMAE - Train: 2239.7041 | Val: 2371.9634 | Params: {'learning_rate': 0.1, 'max_depth': 8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  inputs = _infer_schema(model_input) if model_input is not None else None\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [18:05:11] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:16: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model logged with WMAE: 2371.9634\n",
            "\n",
            "Final Results:\n",
            "Best WMAE: 2371.9634\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prepare test data for predictions\n",
        "# print(\"\\nPreparing test data...\")\n",
        "# X_test_processed = preprocessing_pipeline.transform(test_df)\n",
        "\n",
        "# # Remove Date column from test data\n",
        "# X_test_final = X_test_processed[features_for_training]\n",
        "\n",
        "# # Make predictions\n",
        "# test_predictions = final_model.predict(X_test_final)\n",
        "\n",
        "# # Create submission file using proper test IDs\n",
        "# submission = pd.DataFrame({\n",
        "#     'Id': test_df['Id'],  # Use the original Id from test data\n",
        "#     'Weekly_Sales': test_predictions\n",
        "# })\n",
        "\n",
        "# submission.to_csv('walmart_xgboost_submission.csv', index=False)\n",
        "# print(\"Submission file created: walmart_xgboost_submission.csv\")"
      ],
      "metadata": {
        "id": "VBCEQYR0jlkl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}