{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_experiment_arima.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e6ae7e670d64699a6291db5ad804ad7": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_490637f63c0043b4bd90d210d77a2311",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠧\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠧</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "490637f63c0043b4bd90d210d77a2311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abarb2022/Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_arima.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Kaggle data sets directly into Colab**"
      ],
      "metadata": {
        "id": "SyZxTF7lf7jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the kaggle python library"
      ],
      "metadata": {
        "id": "7lvdgeEMgCoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwSaX9akGfG",
        "outputId": "583312c0-0f9e-4ca0-8abe-47e075419bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google drive so you can store your kaggle API credentials for future use"
      ],
      "metadata": {
        "id": "rw0DfSAggHED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGineQt7dErh",
        "outputId": "1da9991c-60fc-411a-88b1-cd2ca6aa47e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a directory for kaggle at the temporary instance location on Colab drive.\n",
        "\n",
        "Download your kaggle API key (.json file). You can do this by going to your kaggle account page and clicking 'Create new API token' under the API section."
      ],
      "metadata": {
        "id": "Rvmi3WbigOmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vhywUxLXgjBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ZTkKggcylXfa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file to Google Drive and then copy to the temporary location."
      ],
      "metadata": {
        "id": "p3N4it0xrFmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IQq6ZMyTrEfO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the file permissions to read/write to the owner only"
      ],
      "metadata": {
        "id": "p3dHJgtLehrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7ncAtrq2lg5F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Competitions and Datasets are the two types of Kaggle data**"
      ],
      "metadata": {
        "id": "Rb3Zm9VMlu3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Download competition data**\n",
        "\n",
        "If you get 403 Forbidden error, you need to click 'Late Submission' on the Kaggle page for that competition."
      ],
      "metadata": {
        "id": "OrdSFfGjl3Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0yNdtoRln8A",
        "outputId": "86a68500-e010-4da2-819e-5707c87a57aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 1.12GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip, in case the downloaded file is zipped. Refresh the files on the left hand side to update the view."
      ],
      "metadata": {
        "id": "fRmXZnHghNAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAs9oVnNoziL",
        "outputId": "8d17e5fc-3ab4-43e9-e03e-f0cc44c0ce04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder # For Type encoding if not using category dtype directly\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc # For garbage collection\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "RAb9vK9B7YFb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores = pd.read_csv('stores.csv')\n",
        "train = pd.read_csv(\"train.csv.zip\")\n",
        "features = pd.read_csv('features.csv.zip')\n",
        "sample = pd.read_csv('sampleSubmission.csv.zip')\n",
        "test = pd.read_csv('test.csv.zip')"
      ],
      "metadata": {
        "id": "255em5G65SWD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' columns to datetime objects for easier manipulation\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "# Merge features with train and test data.\n",
        "# Note: 'IsHoliday' is present in both train/test and features.csv.\n",
        "# We'll merge on it to ensure consistency, but if there were discrepancies,\n",
        "# we'd need a more careful merge strategy.\n",
        "train_df = pd.merge(train, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "# Merge store information\n",
        "train_df = pd.merge(train_df, stores, on='Store', how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "\n",
        "print(\"\\n--- Merged Train Data Head ---\")\n",
        "print(train_df.head())\n",
        "print(\"\\n--- Merged Test Data Head ---\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\n--- Merged Train Data Info ---\")\n",
        "print(train_df.info())\n",
        "print(\"\\n--- Merged Test Data Info ---\")\n",
        "print(test_df.info())\n",
        "\n",
        "# Free up memory\n",
        "del train, test, features, stores\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "OFPJWG6V5nZ3",
        "outputId": "01b545be-63c8-42e6-dab9-1ad88142b6c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merged Train Data Head ---\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n",
            "0      1     1 2010-02-05      24924.50      False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106    A  151315\n",
            "1      1     1 2010-02-12      46039.49       True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106    A  151315\n",
            "2      1     1 2010-02-19      41595.55      False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106    A  151315\n",
            "3      1     1 2010-02-26      19403.54      False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106    A  151315\n",
            "4      1     1 2010-03-05      21827.90      False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106    A  151315\n",
            "\n",
            "--- Merged Test Data Head ---\n",
            "   Store  Dept       Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n",
            "0      1     1 2012-11-02      False        55.32       3.386    6766.44    5147.70      50.82    3639.90    2737.42  223.462779         6.573    A  151315\n",
            "1      1     1 2012-11-09      False        61.24       3.314   11421.32    3370.89      40.28    4646.79    6154.16  223.481307         6.573    A  151315\n",
            "2      1     1 2012-11-16      False        52.92       3.252    9696.28     292.10     103.78    1133.15    6612.69  223.512911         6.573    A  151315\n",
            "3      1     1 2012-11-23       True        56.23       3.211     883.59       4.17   74910.32     209.91     303.32  223.561947         6.573    A  151315\n",
            "4      1     1 2012-11-30      False        52.34       3.207    2460.03        NaN    3838.35     150.57    6966.34  223.610984         6.573    A  151315\n",
            "\n",
            "--- Merged Train Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         421570 non-null  int64         \n",
            " 1   Dept          421570 non-null  int64         \n",
            " 2   Date          421570 non-null  datetime64[ns]\n",
            " 3   Weekly_Sales  421570 non-null  float64       \n",
            " 4   IsHoliday     421570 non-null  bool          \n",
            " 5   Temperature   421570 non-null  float64       \n",
            " 6   Fuel_Price    421570 non-null  float64       \n",
            " 7   MarkDown1     150681 non-null  float64       \n",
            " 8   MarkDown2     111248 non-null  float64       \n",
            " 9   MarkDown3     137091 non-null  float64       \n",
            " 10  MarkDown4     134967 non-null  float64       \n",
            " 11  MarkDown5     151432 non-null  float64       \n",
            " 12  CPI           421570 non-null  float64       \n",
            " 13  Unemployment  421570 non-null  float64       \n",
            " 14  Type          421570 non-null  object        \n",
            " 15  Size          421570 non-null  int64         \n",
            "dtypes: bool(1), datetime64[ns](1), float64(10), int64(3), object(1)\n",
            "memory usage: 48.6+ MB\n",
            "None\n",
            "\n",
            "--- Merged Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 15 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         115064 non-null  int64         \n",
            " 1   Dept          115064 non-null  int64         \n",
            " 2   Date          115064 non-null  datetime64[ns]\n",
            " 3   IsHoliday     115064 non-null  bool          \n",
            " 4   Temperature   115064 non-null  float64       \n",
            " 5   Fuel_Price    115064 non-null  float64       \n",
            " 6   MarkDown1     114915 non-null  float64       \n",
            " 7   MarkDown2     86437 non-null   float64       \n",
            " 8   MarkDown3     105235 non-null  float64       \n",
            " 9   MarkDown4     102176 non-null  float64       \n",
            " 10  MarkDown5     115064 non-null  float64       \n",
            " 11  CPI           76902 non-null   float64       \n",
            " 12  Unemployment  76902 non-null   float64       \n",
            " 13  Type          115064 non-null  object        \n",
            " 14  Size          115064 non-null  int64         \n",
            "dtypes: bool(1), datetime64[ns](1), float64(9), int64(3), object(1)\n",
            "memory usage: 12.4+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DATA CLEANING**\n"
      ],
      "metadata": {
        "id": "XSajWBEMo4CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to handle missing values for specific columns.\n",
        "    - MarkDown columns: fill with 0.\n",
        "    - Other specified numerical columns: fill with ffill then bfill, fallback to mean.\n",
        "    \"\"\"\n",
        "    def __init__(self, markdown_cols=None, numerical_cols_to_impute=None):\n",
        "        self.markdown_cols = markdown_cols if markdown_cols is not None else [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        self.numerical_cols_to_impute = numerical_cols_to_impute if numerical_cols_to_impute is not None else ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        self.means = {} # To store means for fallback imputation during transform\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate means for fallback imputation from the training data\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X.columns:\n",
        "                self.means[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "\n",
        "        for col in self.markdown_cols:\n",
        "          if col in X_copy.columns:\n",
        "            X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n",
        "            X_copy[col] = X_copy[col].fillna(0)\n",
        "\n",
        "\n",
        "        # Impute other numerical columns with ffill then bfill, fallback to mean\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[col] = X_copy[col].ffill().bfill()\n",
        "                # Fallback to mean if NaNs still exist (e.g., if all values were NaN in a column)\n",
        "                if X_copy[col].isnull().any() and col in self.means:\n",
        "                    X_copy[col] = X_copy[col].fillna(self.means[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "CFCOb354o73R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to extract temporal features from the 'Date' column.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='Date'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        if self.date_column not in X_copy.columns:\n",
        "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
        "\n",
        "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
        "\n",
        "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
        "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
        "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
        "\n",
        "        # Using .dt.isocalendar().week for consistent week numbering across years\n",
        "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
        "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
        "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
        "\n",
        "        X_copy['Week_sin'] = np.sin(2 * np.pi * X_copy['Week'] / 52)\n",
        "        X_copy['Week_cos'] = np.cos(2 * np.pi * X_copy['Week'] / 52)\n",
        "\n",
        "        # Markdown aggregation\n",
        "        X_copy['Total_MarkDown'] = X_copy[[f'MarkDown{i}' for i in range(1, 6)]].sum(axis=1)\n",
        "        X_copy['MarkDown_Intensity'] = X_copy['Total_MarkDown'] / (X_copy['Total_MarkDown'].mean() + 1)\n",
        "\n",
        "        # Economic indicators\n",
        "        X_copy['Fuel_CPI_Ratio'] = X_copy['Fuel_Price'] / X_copy['CPI']\n",
        "        X_copy['Economic_Index'] = (X_copy['CPI'] * 0.4 + (100 - X_copy['Unemployment']) * 0.6) / 100\n",
        "\n",
        "\n",
        "        # Convert IsHoliday to integer if it exists and is boolean\n",
        "        if 'IsHoliday' in X_copy.columns and X_copy['IsHoliday'].dtype == bool:\n",
        "            X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
        "\n",
        "        # Keep the 'Date' column for ARIMA\n",
        "        return X_copy # Removed .drop(columns=[self.date_column, \"Month\", \"Week\"])"
      ],
      "metadata": {
        "id": "yzQNQ5a8tRsi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_df['Weekly_Sales']\n",
        "X_train = train_df.drop(columns=['Weekly_Sales', 'Id'], errors='ignore')\n",
        "\n",
        "temp_train_df = X_train.copy()\n",
        "temp_train_df['Date'] = pd.to_datetime(train_df['Date']) # Get original dates back for sorting\n",
        "temp_train_df['Weekly_Sales'] = y_train\n",
        "\n",
        "temp_train_df = temp_train_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Define a cutoff date for validation\n",
        "validation_cutoff_date = pd.to_datetime('2012-09-01')\n",
        "\n",
        "X_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date]\n",
        "y_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date]['Weekly_Sales']\n",
        "\n",
        "X_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date]\n",
        "y_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date]['Weekly_Sales']\n",
        "\n",
        "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "val_weights = np.where(X_val_split['IsHoliday'] == 1, 5, 1)\n"
      ],
      "metadata": {
        "id": "epPQKHMKz55M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning, ValueWarning\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "class ARIMAModelWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, order, seasonal_order=(0,0,0,0), verbose=True):\n",
        "        self.order = order\n",
        "        self.seasonal_order = seasonal_order\n",
        "        self.verbose = verbose\n",
        "        self.models = {}\n",
        "        self.last_values = {}\n",
        "        self.global_average = None  # <- NEW\n",
        "\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Suppress all statsmodels warnings\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "            warnings.simplefilter(\"ignore\", category=ValueWarning)\n",
        "\n",
        "            grouped = X.groupby(['Store', 'Dept'])\n",
        "\n",
        "            # Create progress bar if verbose\n",
        "            if self.verbose:\n",
        "                groups = tqdm(grouped, desc=\"Training ARIMA models\", unit=\"store-dept\")\n",
        "            else:\n",
        "                groups = grouped\n",
        "\n",
        "            if 'Weekly_Sales' in X.columns:\n",
        "                self.global_average = X['Weekly_Sales'].mean()\n",
        "\n",
        "\n",
        "            for (store, dept), group in groups:\n",
        "                ts_data = group.set_index('Date')['Weekly_Sales']\n",
        "\n",
        "                # Force weekly frequency to prevent warnings\n",
        "                ts_data = ts_data.asfreq('W-FRI')\n",
        "\n",
        "                if len(ts_data.dropna()) < 3:\n",
        "                  if self.verbose:\n",
        "                      print(f\"Skipping Store {store}, Dept {dept} due to insufficient data\")\n",
        "                  continue\n",
        "\n",
        "\n",
        "                if len(ts_data) > 0:\n",
        "                    try:\n",
        "                        with warnings.catch_warnings():\n",
        "                            warnings.simplefilter(\"ignore\")\n",
        "                            model = ARIMA(ts_data,\n",
        "                                         order=self.order,\n",
        "                                         seasonal_order=self.seasonal_order)\n",
        "                            fitted_model = model.fit()\n",
        "                            self.models[(store, dept)] = fitted_model\n",
        "                            self.last_values[(store, dept)] = ts_data.iloc[-1]\n",
        "                    except Exception as e:\n",
        "                        if self.verbose:\n",
        "                          print(f\"Failed on Store {store}, Dept {dept} - {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "                        continue\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # This will return ARIMA predictions for the existing dates\n",
        "        # For production, you might want a separate predict method\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions for each store-dept combination in X\n",
        "        predictions = []\n",
        "        for _, row in X.iterrows():\n",
        "            store = row['Store']\n",
        "            dept = row['Dept']\n",
        "            date = row['Date']\n",
        "\n",
        "            if (store, dept) in self.models and self.models[(store, dept)] is not None:\n",
        "                try:\n",
        "                     # Get the forecast for this specific date\n",
        "                    model = self.models[(store, dept)]\n",
        "                    # Calculate the number of steps from the last training data point to the prediction date\n",
        "                    # Assuming weekly data frequency\n",
        "                    # Find the last date the model was trained on\n",
        "                    last_train_date = model.model.data.dates[-1]\n",
        "                    steps = (date - last_train_date).days // 7\n",
        "\n",
        "\n",
        "                    if steps >= 0: # Predict from the last training date onwards\n",
        "                        forecast = model.forecast(steps=steps + 1) # Forecast up to the prediction date\n",
        "                        pred = forecast.iloc[-1]\n",
        "                    else: # If the date is before the last training date, use the observed value\n",
        "                         # This case should ideally not happen in a standard forecast scenario,\n",
        "                         # but included for robustness if predict is used on past dates.\n",
        "                         # We would need to find the closest date in the training data\n",
        "                        print(f\"Warning: Predicting for a date before the last training date for Store {store}, Dept {dept}, Date {date}\")\n",
        "                        if (store, dept) in self.last_values:\n",
        "                          pred = self.last_values[(store, dept)]\n",
        "                        else:\n",
        "                          # Safe fallback value (mean of all sales?)\n",
        "                          print(\"Safe fallback value (mean of all sales?)\")\n",
        "                          pred = global_average\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Prediction failed for Store {store}, Dept {dept}, Date {date}: {str(e)}\")\n",
        "                    pred = self.last_values[(store, dept)] if (store, dept) in self.last_values else np.nan # Fallback to last value or NaN\n",
        "            else:\n",
        "                # Fallback - use last known value or NaN if no model was fitted\n",
        "                pred = self.last_values[(store, dept)] if (store, dept) in self.last_values else np.nan\n",
        "\n",
        "\n",
        "            predictions.append(pred)\n",
        "\n",
        "        return np.array(predictions)"
      ],
      "metadata": {
        "id": "D9NzOmnG0cda"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the full pipeline\n",
        "arima_order = (1,0,1)  # Simplified order\n",
        "\n",
        "arima_seasonal_order=(0,0,0,0)\n",
        "# Preprocessing steps\n",
        "preprocessing = Pipeline([\n",
        "    ('missing_value_imputer', MissingValueImputer()),\n",
        "    ('date_feature_extractor', DateFeatureExtractor())\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "# Full pipeline with ARIMA\n",
        "full_pipeline = Pipeline([\n",
        "    ('preprocessing', preprocessing),\n",
        "    ('arima_model', ARIMAModelWrapper(order=arima_order, seasonal_order=arima_seasonal_order))\n",
        "])\n",
        "\n",
        "\n",
        "full_pipeline.fit(X_train_split, y_train_split)\n",
        "predictions = full_pipeline.predict(X_val_split)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBt4IY1E0vZy",
        "outputId": "451210fd-3328-4d18-fcf5-c1adec6046f4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
            "Training ARIMA models:   4%|▍         | 139/3326 [00:08<03:40, 14.47store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 2, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   6%|▋         | 214/3326 [00:14<02:42, 19.17store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 3, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   7%|▋         | 218/3326 [00:14<02:46, 18.62store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 3, Dept 83 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   8%|▊         | 266/3326 [00:16<02:33, 19.93store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 4, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  11%|█         | 366/3326 [00:24<02:11, 22.55store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 5, Dept 77 due to insufficient data\n",
            "Skipping Store 5, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  13%|█▎        | 439/3326 [00:28<02:32, 18.99store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 6, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  15%|█▌        | 515/3326 [00:35<02:47, 16.76store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 7, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  16%|█▌        | 533/3326 [00:36<02:14, 20.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 7, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  20%|█▉        | 665/3326 [00:43<01:58, 22.42store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 9, Dept 77 due to insufficient data\n",
            "Skipping Store 9, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  20%|██        | 674/3326 [00:45<06:26,  6.86store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 9, Dept 93 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  22%|██▏       | 740/3326 [00:49<02:06, 20.51store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 10, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  29%|██▊       | 951/3326 [01:04<02:15, 17.58store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 13, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  29%|██▉       | 970/3326 [01:05<02:19, 16.84store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 13, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  31%|███       | 1029/3326 [01:08<01:55, 19.81store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 14, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  33%|███▎      | 1101/3326 [01:15<01:43, 21.40store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 37 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  33%|███▎      | 1108/3326 [01:15<01:34, 23.36store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 43 due to insufficient data\n",
            "Skipping Store 15, Dept 48 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  34%|███▍      | 1146/3326 [01:17<02:02, 17.83store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  36%|███▋      | 1206/3326 [01:21<01:50, 19.15store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 16, Dept 77 due to insufficient data\n",
            "Skipping Store 16, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  37%|███▋      | 1220/3326 [01:21<02:01, 17.30store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 16, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  40%|████      | 1332/3326 [01:29<01:42, 19.42store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  40%|████      | 1341/3326 [01:30<01:42, 19.35store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 48 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  41%|████▏     | 1375/3326 [01:32<01:33, 20.78store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  42%|████▏     | 1413/3326 [01:33<01:41, 18.92store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 19, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  47%|████▋     | 1576/3326 [01:45<01:24, 20.66store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 48 due to insufficient data\n",
            "Skipping Store 21, Dept 50 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1590/3326 [01:46<01:38, 17.65store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1602/3326 [01:47<02:12, 12.99store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 96 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1607/3326 [01:47<02:18, 12.39store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  51%|█████     | 1686/3326 [01:53<01:23, 19.55store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 22, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  53%|█████▎    | 1762/3326 [01:57<01:19, 19.67store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 23, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  57%|█████▋    | 1897/3326 [02:07<01:16, 18.65store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 25, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  59%|█████▉    | 1975/3326 [02:11<01:06, 20.38store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 26, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  61%|██████    | 2027/3326 [02:16<00:59, 21.71store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 27, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  63%|██████▎   | 2109/3326 [02:20<01:02, 19.38store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 28, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  67%|██████▋   | 2221/3326 [02:29<01:03, 17.45store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 29, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  67%|██████▋   | 2240/3326 [02:31<01:21, 13.31store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 30, Dept 19 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  68%|██████▊   | 2252/3326 [02:32<00:56, 19.12store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 30, Dept 33 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  73%|███████▎  | 2422/3326 [02:44<00:50, 17.83store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 32, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  74%|███████▍  | 2464/3326 [02:47<00:57, 14.99store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 27 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  74%|███████▍  | 2477/3326 [02:48<00:58, 14.58store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 49 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  75%|███████▍  | 2484/3326 [02:49<01:00, 14.03store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  77%|███████▋  | 2562/3326 [02:55<00:34, 22.00store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 34, Dept 77 due to insufficient data\n",
            "Skipping Store 34, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2678/3326 [03:03<00:45, 14.28store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 29 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2683/3326 [03:03<00:47, 13.47store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 36 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2696/3326 [03:06<01:13,  8.51store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████▏ | 2704/3326 [03:07<01:06,  9.37store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 85 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  82%|████████▏ | 2716/3326 [03:08<00:41, 14.59store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  83%|████████▎ | 2758/3326 [03:11<00:36, 15.43store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 37, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  84%|████████▎ | 2779/3326 [03:12<00:32, 16.95store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 37, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  84%|████████▍ | 2809/3326 [03:15<00:38, 13.29store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 38, Dept 35 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  85%|████████▌ | 2842/3326 [03:19<00:38, 12.64store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 38, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  87%|████████▋ | 2898/3326 [03:24<00:27, 15.48store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 39, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  89%|████████▉ | 2974/3326 [03:29<00:26, 13.16store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 40, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  91%|█████████ | 3026/3326 [03:34<00:14, 20.42store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 41, Dept 37 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  93%|█████████▎| 3099/3326 [03:39<00:16, 13.79store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 42, Dept 34 due to insufficient data\n",
            "Skipping Store 42, Dept 41 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  95%|█████████▍| 3153/3326 [03:45<00:14, 11.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 43, Dept 24 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  95%|█████████▌| 3167/3326 [03:46<00:12, 13.11store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 43, Dept 55 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  97%|█████████▋| 3221/3326 [03:50<00:07, 14.14store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 44, Dept 34 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  98%|█████████▊| 3255/3326 [03:53<00:05, 14.18store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 44, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models: 100%|█████████▉| 3323/3326 [03:59<00:00, 14.15store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 45, Dept 96 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models: 100%|██████████| 3326/3326 [04:00<00:00, 13.86store-dept/s]\n",
            "/tmp/ipython-input-12-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "val_weights = np.where(X_val_split['IsHoliday'] == 1, 5, 1)\n",
        "print (weighted_mean_absolute_error(y_val_split, predictions, val_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9E4pjvW3wSv",
        "outputId": "c2db75b1-a65d-42d9-fe24-f75c4b39e482"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999.510169856433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dagshub\n"
      ],
      "metadata": {
        "id": "nRj0MDGdJcdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13611846-5cfd-433e-94d8-97169a1d6195"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dagshub\n",
            "  Downloading dagshub-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\n",
            "Collecting appdirs>=1.4.4 (from dagshub)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.44)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Collecting dacite~=1.6.0 (from dagshub)\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n",
            "Collecting gql[requests] (from dagshub)\n",
            "  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting dataclasses-json (from dagshub)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.2)\n",
            "Collecting treelib>=1.6.4 (from dagshub)\n",
            "  Downloading treelib-1.8.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Collecting boto3 (from dagshub)\n",
            "  Downloading boto3-1.39.4-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting semver (from dagshub)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n",
            "  Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (4.14.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from treelib>=1.6.4->dagshub) (1.17.0)\n",
            "Collecting botocore<1.40.0,>=1.39.4 (from boto3->dagshub)\n",
            "  Downloading botocore-1.39.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->dagshub)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting graphql-core<3.2.7,>=3.2 (from gql[requests]->dagshub)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests<3,>=2.26 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.4->boto3->dagshub) (2.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.26->gql[requests]->dagshub) (3.4.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
            "Downloading dagshub-0.5.10-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl (33 kB)\n",
            "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading treelib-1.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading boto3-1.39.4-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.39.4-py3-none-any.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: appdirs, treelib, semver, pathvalidate, mypy-extensions, marshmallow, jmespath, graphql-core, dacite, backoff, typing-inspect, gql, botocore, s3transfer, dataclasses-json, dagshub-annotation-converter, boto3, dagshub\n",
            "Successfully installed appdirs-1.4.4 backoff-2.2.1 boto3-1.39.4 botocore-1.39.4 dacite-1.6.0 dagshub-0.5.10 dagshub-annotation-converter-0.1.10 dataclasses-json-0.6.7 gql-3.5.3 graphql-core-3.2.6 jmespath-1.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 pathvalidate-3.3.1 s3transfer-0.13.0 semver-3.0.4 treelib-1.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow\n"
      ],
      "metadata": {
        "id": "RpMJLpuY4eCc",
        "outputId": "277d2d6a-d8ff-4f74-946d-cba616c8db1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==3.1.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading databricks_sdk-0.58.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.116.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.1)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.7.9)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.58.0-py3-none-any.whl (741 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m741.4/741.4 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gunicorn, graphql-relay, opentelemetry-api, graphene, docker, alembic, opentelemetry-semantic-conventions, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed alembic-1.16.4 databricks-sdk-0.58.0 docker-7.1.0 graphene-3.4.3 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.1 mlflow-skinny-3.1.1 opentelemetry-api-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import dagshub, mlflow\n",
        "# Try to get credentials from environment first\n",
        "dagshub.init(\n",
        "    repo_owner='abarb22',\n",
        "    repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "    mlflow=True\n",
        ")\n",
        "mlflow.set_experiment(\"ARIMA_Training\")\n"
      ],
      "metadata": {
        "id": "C2pqqJtx4j2r",
        "outputId": "ba853643-b53c-42bc-93a9-c05f1d54be3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "6e6ae7e670d64699a6291db5ad804ad7",
            "490637f63c0043b4bd90d210d77a2311"
          ]
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e6ae7e670d64699a6291db5ad804ad7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=d97ecdc7-59f0-4ba1-92d9-48b871aa37a1&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=a86998497631a0d85891aae820edd2f8a3535f6a5d13671b86bc05af844acbfc\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as abarb22\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as abarb22\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/6e01db02c8e240aebfa89d3184cdf829', creation_time=1751573162919, experiment_id='2', last_update_time=1751573162919, lifecycle_stage='active', name='ARIMA_Training', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"ARIMA_Data_Cleaning\"):\n",
        "    # Log data cleaning parameters\n",
        "    mlflow.log_param(\"missing_value_strategy\", \"MarkDowns->0, others->ffill/bfill/mean\")\n",
        "    mlflow.log_param(\"date_features_extracted\", True)\n",
        "\n",
        "\n",
        "    # Log metrics about data quality\n",
        "    mlflow.log_metric(\"cleaned_missing_values\", train_df.isna().sum().sum())\n"
      ],
      "metadata": {
        "id": "e1-FbzXy5O0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"ARIMA_Feature_Engineering\"):\n",
        "    # Log feature engineering parameters\n",
        "    mlflow.log_params({\n",
        "        \"temporal_features\": [\"Year\", \"Month\", \"Week\", \"DayOfWeek\"],\n",
        "        \"cyclical_features\": [\"Month_sin\", \"Month_cos\", \"Week_sin\", \"Week_cos\"],\n",
        "        \"economic_features\": [\"Fuel_CPI_Ratio\", \"Economic_Index\"],\n",
        "        \"markdown_features\": [\"Total_MarkDown\", \"MarkDown_Intensity\"]\n",
        "    })\n",
        "\n",
        "    # Your feature engineering\n",
        "    feature_pipeline = Pipeline([\n",
        "        ('date_extractor', DateFeatureExtractor())\n",
        "    ])\n",
        "\n",
        "    X_featured = feature_pipeline.fit_transform(X_train_split)\n",
        "\n",
        "    # Log results\n",
        "    mlflow.log_metric(\"total_features\", len(X_featured.columns))\n",
        "    mlflow.log_metric(\"time_span_days\", (X_featured['Date'].max() - X_featured['Date'].min()).days)"
      ],
      "metadata": {
        "id": "PjefJVnn5z1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with mlflow.start_run(run_name=\"ARIMA_Model_Training\"):\n",
        "    # Log model parameters\n",
        "    arima_params = {\n",
        "        'order': (1,0,1),\n",
        "        'seasonal_order': (0,0,0,0),\n",
        "        'trend': 'c'\n",
        "    }\n",
        "    mlflow.log_params(arima_params)\n",
        "\n",
        "    preprocessing = Pipeline([\n",
        "        ('missing_value_imputer', MissingValueImputer()),\n",
        "        ('date_feature_extractor', DateFeatureExtractor()),\n",
        "    ])\n",
        "\n",
        "    # Full pipeline with ARIMA\n",
        "    full_pipeline = Pipeline([\n",
        "        ('preprocessing', preprocessing),\n",
        "        ('arima_model', ARIMAModelWrapper(order=arima_params['order'], seasonal_order=arima_params['seasonal_order']))\n",
        "    ])\n",
        "\n",
        "\n",
        "    full_pipeline.fit(X_train_split, y_train_split)\n",
        "    val_preds = full_pipeline.predict(X_val_split)\n",
        "\n",
        "\n",
        "    val_wmae = weighted_mean_absolute_error(y_val_split, val_preds, val_weights)\n",
        "        # Log metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"train_samples\": len(X_train_split),\n",
        "        \"val_samples\": len(X_val_split),\n",
        "        \"val_wmae\": val_wmae,\n",
        "    })\n",
        "\n",
        "    # Log model (as artifact since statsmodels doesn't have native MLflow support)\n",
        "    import joblib\n",
        "    joblib.dump(full_pipeline, \"arima_pipeline.joblib\")\n",
        "    mlflow.log_artifact(\"arima_pipeline.joblib\")\n"
      ],
      "metadata": {
        "id": "MU-fxmC06L9i",
        "outputId": "555687ff-740c-438b-88dd-48ffef82618a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
            "/tmp/ipython-input-17-489002183.py:28: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  grouped = X.groupby(['Store', 'Dept'])\n",
            "/tmp/ipython-input-11-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"ARIMA_Model_Training\"):\n",
        "    # Log model parameters\n",
        "    arima_params = {\n",
        "        'order': (1, 0, 1),\n",
        "        'seasonal_order': (0, 0, 0, 0),\n",
        "        'trend': 'c'\n",
        "    }\n",
        "    mlflow.log_params(arima_params)\n",
        "\n",
        "    preprocessing = Pipeline([\n",
        "        ('missing_value_imputer', MissingValueImputer()),\n",
        "        ('date_feature_extractor', DateFeatureExtractor()),\n",
        "    ])\n",
        "\n",
        "    arima_model = ARIMAModelWrapper(order=arima_params['order'], seasonal_order=arima_params['seasonal_order'], verbose=True)\n",
        "\n",
        "    # Full pipeline with ARIMA\n",
        "    full_pipeline = Pipeline([\n",
        "        ('preprocessing', preprocessing),\n",
        "        ('arima_model', arima_model)\n",
        "    ])\n",
        "\n",
        "    full_pipeline.fit(X_train_split, y_train_split)\n",
        "    val_preds = full_pipeline.predict(X_val_split)\n",
        "\n",
        "    val_wmae = weighted_mean_absolute_error(y_val_split, val_preds, val_weights)\n",
        "\n",
        "    # Extra info from your model\n",
        "    trained_models_count = len(arima_model.models)\n",
        "    fallback_preds_count = np.sum(pd.Series(val_preds).isna() | pd.Series(val_preds) == arima_model.global_average)\n",
        "    skipped_count = (X_train_split.groupby(['Store', 'Dept']).ngroups) - trained_models_count\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"train_samples\": len(X_train_split),\n",
        "        \"val_samples\": len(X_val_split),\n",
        "        \"val_wmae\": val_wmae,\n",
        "        \"trained_groups\": trained_models_count,\n",
        "        \"skipped_groups\": skipped_count,\n",
        "        \"fallback_predictions\": int(fallback_preds_count),\n",
        "    })\n",
        "\n",
        "    if arima_model.global_average is not None:\n",
        "        mlflow.log_metric(\"global_average_fallback_value\", arima_model.global_average)\n",
        "\n",
        "    # Log model artifact\n",
        "    import joblib\n",
        "    joblib.dump(full_pipeline, \"arima_pipeline.joblib\")\n",
        "    mlflow.log_artifact(\"arima_pipeline.joblib\")\n"
      ],
      "metadata": {
        "id": "ZT2ZoadzNASy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e025581b-3a83-4a15-9905-d7feb083f3e3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
            "Training ARIMA models:   4%|▍         | 136/3326 [00:28<25:41,  2.07store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 2, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   6%|▋         | 213/3326 [00:49<04:42, 11.03store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 3, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   7%|▋         | 218/3326 [00:49<04:54, 10.56store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 3, Dept 83 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   8%|▊         | 266/3326 [00:53<03:10, 16.03store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 4, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  11%|█         | 366/3326 [01:05<03:11, 15.47store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 5, Dept 77 due to insufficient data\n",
            "Skipping Store 5, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  13%|█▎        | 437/3326 [01:17<05:36,  8.58store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 6, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  15%|█▌        | 515/3326 [01:24<02:55, 15.99store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 7, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  16%|█▌        | 532/3326 [01:25<02:38, 17.65store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 7, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  20%|█▉        | 662/3326 [01:39<04:03, 10.92store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 9, Dept 77 due to insufficient data\n",
            "Skipping Store 9, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  20%|██        | 675/3326 [01:41<06:59,  6.33store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 9, Dept 93 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  22%|██▏       | 740/3326 [01:48<03:05, 13.91store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 10, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  29%|██▊       | 951/3326 [02:05<02:12, 17.93store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 13, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  29%|██▉       | 968/3326 [02:07<06:05,  6.45store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 13, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  31%|███       | 1028/3326 [02:12<01:53, 20.24store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 14, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  33%|███▎      | 1102/3326 [02:17<01:47, 20.78store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 37 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  33%|███▎      | 1108/3326 [02:17<01:39, 22.19store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining ARIMA models:  33%|███▎      | 1111/3326 [02:17<01:49, 20.18store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 48 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  34%|███▍      | 1144/3326 [02:20<03:54,  9.30store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  36%|███▋      | 1206/3326 [02:25<01:41, 20.80store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 16, Dept 77 due to insufficient data\n",
            "Skipping Store 16, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  37%|███▋      | 1222/3326 [02:25<01:43, 20.33store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 16, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  40%|████      | 1332/3326 [02:31<01:37, 20.49store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  40%|████      | 1338/3326 [02:31<01:43, 19.29store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 48 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  41%|████▏     | 1376/3326 [02:36<01:35, 20.42store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  42%|████▏     | 1413/3326 [02:37<01:30, 21.18store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 19, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  47%|████▋     | 1576/3326 [02:49<01:18, 22.39store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 48 due to insufficient data\n",
            "Skipping Store 21, Dept 50 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1589/3326 [02:49<01:28, 19.60store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1604/3326 [02:50<01:34, 18.27store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 96 due to insufficient data\n",
            "Skipping Store 21, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  51%|█████     | 1685/3326 [02:54<01:16, 21.48store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 22, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  53%|█████▎    | 1762/3326 [03:00<02:29, 10.46store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 23, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  57%|█████▋    | 1898/3326 [03:08<01:17, 18.43store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 25, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  59%|█████▉    | 1973/3326 [03:14<01:18, 17.23store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 26, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  61%|██████    | 2027/3326 [03:16<00:56, 22.99store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 27, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  63%|██████▎   | 2110/3326 [03:21<01:03, 19.00store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 28, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  67%|██████▋   | 2223/3326 [03:30<01:01, 17.88store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 29, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  67%|██████▋   | 2240/3326 [03:31<01:15, 14.37store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 30, Dept 19 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  68%|██████▊   | 2254/3326 [03:32<01:00, 17.61store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 30, Dept 33 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  73%|███████▎  | 2420/3326 [03:43<00:42, 21.11store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 32, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  74%|███████▍  | 2464/3326 [03:47<00:51, 16.64store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 27 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  74%|███████▍  | 2474/3326 [03:48<01:54,  7.43store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 49 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  75%|███████▍  | 2484/3326 [03:50<02:10,  6.48store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  77%|███████▋  | 2562/3326 [03:54<00:35, 21.60store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 34, Dept 77 due to insufficient data\n",
            "Skipping Store 34, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2679/3326 [04:04<00:38, 16.91store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 29 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2684/3326 [04:04<00:33, 19.12store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 36 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2698/3326 [04:05<00:41, 15.07store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████▏ | 2704/3326 [04:06<00:49, 12.63store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 85 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  82%|████████▏ | 2716/3326 [04:07<00:37, 16.14store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  83%|████████▎ | 2758/3326 [04:10<00:34, 16.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 37, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  84%|████████▎ | 2779/3326 [04:11<00:31, 17.41store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 37, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  84%|████████▍ | 2809/3326 [04:15<01:15,  6.81store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 38, Dept 35 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  85%|████████▌ | 2841/3326 [04:18<00:34, 14.21store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 38, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  87%|████████▋ | 2898/3326 [04:21<00:26, 16.39store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 39, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  89%|████████▉ | 2975/3326 [04:28<00:39,  8.88store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 40, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  91%|█████████ | 3026/3326 [04:31<00:13, 21.47store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 41, Dept 37 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  93%|█████████▎| 3099/3326 [04:36<00:14, 15.15store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 42, Dept 34 due to insufficient data\n",
            "Skipping Store 42, Dept 41 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  95%|█████████▍| 3154/3326 [04:42<00:12, 13.90store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 43, Dept 24 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  95%|█████████▌| 3167/3326 [04:43<00:09, 16.06store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 43, Dept 55 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  97%|█████████▋| 3221/3326 [04:47<00:08, 13.01store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 44, Dept 34 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  98%|█████████▊| 3253/3326 [04:49<00:04, 16.38store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 44, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models: 100%|██████████| 3326/3326 [04:56<00:00, 11.23store-dept/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 45, Dept 96 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run ARIMA_Model_Training at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/89b439f8294e4e10a4ce360e057cd16d\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "import mlflow\n",
        "import joblib\n",
        "\n",
        "param_grid = list(product([0, 1, 2], [0, 1], [0, 1, 2]))  # (p, d, q)\n",
        "\n",
        "preprocessing = Pipeline([\n",
        "    ('missing_value_imputer', MissingValueImputer()),\n",
        "    ('date_feature_extractor', DateFeatureExtractor())\n",
        "])\n",
        "\n",
        "best_order = None\n",
        "best_wmae = float('inf')\n",
        "\n",
        "with mlflow.start_run(run_name=\"ARIMA_Model_GridSearch\"):\n",
        "    for order in param_grid:\n",
        "        print(f\"Trying ARIMA order: {order}\")\n",
        "        arima_model = ARIMAModelWrapper(order=order, seasonal_order=(0,0,0,0), verbose=False)\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessing', preprocessing),\n",
        "            ('arima_model', arima_model)\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            # Start a nested run for this specific ARIMA(p,d,q) config\n",
        "            with mlflow.start_run(run_name=f\"ARIMA_{order}\", nested=True):\n",
        "                pipeline.fit(X_train_split, y_train_split)\n",
        "                preds = pipeline.predict(X_val_split)\n",
        "                val_wmae = weighted_mean_absolute_error(y_val_split, preds, val_weights)\n",
        "\n",
        "                print(f\"WMAE for order {order}: {val_wmae:.4f}\")\n",
        "\n",
        "                # Log parameters and metrics for this sub-run\n",
        "                mlflow.log_params({\n",
        "                    'order_p': order[0],\n",
        "                    'order_d': order[1],\n",
        "                    'order_q': order[2],\n",
        "                })\n",
        "                mlflow.log_metric(\"val_wmae\", val_wmae)\n",
        "\n",
        "                if val_wmae < best_wmae:\n",
        "                    best_wmae = val_wmae\n",
        "                    best_order = order\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Order {order} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n✅ Best ARIMA order found: {best_order} with WMAE: {best_wmae:.4f}\")\n",
        "\n",
        "    # Optional: train and log best model\n",
        "    final_pipeline = Pipeline([\n",
        "        ('preprocessing', preprocessing),\n",
        "        ('arima_model', ARIMAModelWrapper(order=best_order, seasonal_order=(0,0,0,0)))\n",
        "    ])\n",
        "\n",
        "    final_pipeline.fit(X_train_split, y_train_split)\n",
        "    final_preds = final_pipeline.predict(X_val_split)\n",
        "    final_wmae = weighted_mean_absolute_error(y_val_split, final_preds, val_weights)\n",
        "\n",
        "    mlflow.log_params({\n",
        "        'best_order_p': best_order[0],\n",
        "        'best_order_d': best_order[1],\n",
        "        'best_order_q': best_order[2],\n",
        "        'seasonal_order': (0, 0, 0, 0)\n",
        "    })\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        \"train_samples\": len(X_train_split),\n",
        "        \"val_samples\": len(X_val_split),\n",
        "        \"best_val_wmae\": final_wmae\n",
        "    })\n",
        "\n",
        "    joblib.dump(final_pipeline, \"best_arima_pipeline.joblib\")\n",
        "    mlflow.log_artifact(\"best_arima_pipeline.joblib\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2Z-XtSpsQtX",
        "outputId": "370f22d6-5173-4c68-9af4-5d295dbb6831"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying ARIMA order: (0, 0, 0)\n",
            "WMAE for order (0, 0, 0): 2425.4089\n",
            "🏃 View run ARIMA_(0, 0, 0) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/81fa95ac04ad41ed8422f195a098c80c\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (0, 0, 1)\n",
            "WMAE for order (0, 0, 1): 2424.8848\n",
            "🏃 View run ARIMA_(0, 0, 1) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/2acc72532e8949c5abd697975e5e25b8\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (0, 0, 2)\n",
            "WMAE for order (0, 0, 2): 2298.0891\n",
            "🏃 View run ARIMA_(0, 0, 2) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/07caeed2bb5c4f988851f050229f421e\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (0, 1, 0)\n",
            "WMAE for order (0, 1, 0): 2372.3912\n",
            "🏃 View run ARIMA_(0, 1, 0) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/4c30fd89d11d4ef68d91802a44244786\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (0, 1, 1)\n",
            "WMAE for order (0, 1, 1): 2202.0273\n",
            "🏃 View run ARIMA_(0, 1, 1) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/91f8436f175442ab925986334cd462bb\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (0, 1, 2)\n",
            "WMAE for order (0, 1, 2): 2191.7010\n",
            "🏃 View run ARIMA_(0, 1, 2) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/97a5b68a25d34183a4954d38305283ce\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (1, 0, 0)\n",
            "WMAE for order (1, 0, 0): 2236.1301\n",
            "🏃 View run ARIMA_(1, 0, 0) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/0cdfb0e19d1947daad90d42bdbf9990e\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (1, 0, 1)\n",
            "WMAE for order (1, 0, 1): 1999.5102\n",
            "🏃 View run ARIMA_(1, 0, 1) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/3352713e85a04bbc9dbe2979a82b3c0d\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (1, 0, 2)\n",
            "WMAE for order (1, 0, 2): 1981.4517\n",
            "🏃 View run ARIMA_(1, 0, 2) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/8739eb946ebb40c6bd94295aa6d17952\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (1, 1, 0)\n",
            "WMAE for order (1, 1, 0): 2328.0746\n",
            "🏃 View run ARIMA_(1, 1, 0) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/53ffd27ad7e84185bd88e99979e94105\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (1, 1, 1)\n",
            "WMAE for order (1, 1, 1): 2159.3612\n",
            "🏃 View run ARIMA_(1, 1, 1) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/28b2d9c947e64ea2af1320e17255f0b7\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (1, 1, 2)\n",
            "WMAE for order (1, 1, 2): 2056.7429\n",
            "🏃 View run ARIMA_(1, 1, 2) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/a27a57cb1157438c80c095ddf255d714\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (2, 0, 0)\n",
            "WMAE for order (2, 0, 0): 2081.9697\n",
            "🏃 View run ARIMA_(2, 0, 0) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/87e5ecfd7fb84bc9b2b4d3e52bfa484f\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (2, 0, 1)\n",
            "WMAE for order (2, 0, 1): 2001.4835\n",
            "🏃 View run ARIMA_(2, 0, 1) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/448e38cfd7524efb998071efd5961cd2\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (2, 0, 2)\n",
            "WMAE for order (2, 0, 2): 1998.0549\n",
            "🏃 View run ARIMA_(2, 0, 2) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/879737a071e747cebf8381ad8ff274a1\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (2, 1, 0)\n",
            "WMAE for order (2, 1, 0): 2268.3878\n",
            "🏃 View run ARIMA_(2, 1, 0) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/d350ff1cd05b4e6b9dd9cb583395731f\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (2, 1, 1)\n",
            "WMAE for order (2, 1, 1): 2086.2076\n",
            "🏃 View run ARIMA_(2, 1, 1) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/d3e7e2fb93bf4737b4d29785ef7c38dc\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "Trying ARIMA order: (2, 1, 2)\n",
            "WMAE for order (2, 1, 2): 2437.7487\n",
            "🏃 View run ARIMA_(2, 1, 2) at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/0aedbea506ad406a921ef30bdb0b185d\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "\n",
            "✅ Best ARIMA order found: (1, 0, 2) with WMAE: 1981.4517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   4%|▍         | 138/3326 [00:12<04:01, 13.23store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 2, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   6%|▋         | 213/3326 [00:20<04:26, 11.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 3, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   7%|▋         | 219/3326 [00:21<04:12, 12.32store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 3, Dept 83 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:   8%|▊         | 266/3326 [00:24<04:05, 12.44store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 4, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  11%|█         | 361/3326 [00:35<04:53, 10.09store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 5, Dept 77 due to insufficient data\n",
            "Skipping Store 5, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  13%|█▎        | 437/3326 [00:41<03:33, 13.56store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 6, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  15%|█▌        | 515/3326 [00:49<03:18, 14.13store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 7, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  16%|█▌        | 532/3326 [00:50<03:10, 14.64store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 7, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  20%|█▉        | 665/3326 [01:03<02:58, 14.94store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 9, Dept 77 due to insufficient data\n",
            "Skipping Store 9, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  20%|██        | 676/3326 [01:04<03:09, 14.01store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 9, Dept 93 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  22%|██▏       | 740/3326 [01:10<02:42, 15.90store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 10, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  29%|██▊       | 951/3326 [01:30<02:50, 13.90store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 13, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  29%|██▉       | 969/3326 [01:31<02:56, 13.33store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 13, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  31%|███       | 1029/3326 [01:38<02:38, 14.47store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 14, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  33%|███▎      | 1102/3326 [01:44<02:31, 14.72store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 37 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  33%|███▎      | 1106/3326 [01:44<02:57, 12.53store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  33%|███▎      | 1110/3326 [01:45<07:14,  5.10store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 48 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  34%|███▍      | 1146/3326 [01:49<02:36, 13.91store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 15, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  36%|███▌      | 1203/3326 [01:54<02:24, 14.66store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 16, Dept 77 due to insufficient data\n",
            "Skipping Store 16, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  37%|███▋      | 1221/3326 [01:55<02:35, 13.56store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 16, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  40%|████      | 1332/3326 [02:06<02:03, 16.08store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  40%|████      | 1338/3326 [02:09<10:44,  3.08store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 48 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  41%|████▏     | 1373/3326 [02:13<02:43, 11.95store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 18, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  42%|████▏     | 1412/3326 [02:16<02:25, 13.14store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 19, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  47%|████▋     | 1572/3326 [02:31<02:36, 11.20store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 48 due to insufficient data\n",
            "Skipping Store 21, Dept 50 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1590/3326 [02:33<02:41, 10.72store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1602/3326 [02:34<02:15, 12.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 96 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  48%|████▊     | 1607/3326 [02:35<04:43,  6.06store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 21, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  51%|█████     | 1684/3326 [02:43<02:09, 12.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 22, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  53%|█████▎    | 1760/3326 [02:50<02:08, 12.18store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 23, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  57%|█████▋    | 1898/3326 [03:03<01:31, 15.64store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 25, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  59%|█████▉    | 1973/3326 [03:09<01:39, 13.66store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 26, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  61%|██████    | 2028/3326 [03:15<01:19, 16.43store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 27, Dept 39 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  63%|██████▎   | 2106/3326 [03:22<01:34, 12.93store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 28, Dept 43 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  67%|██████▋   | 2222/3326 [03:35<01:38, 11.17store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 29, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  67%|██████▋   | 2237/3326 [03:37<02:19,  7.81store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 30, Dept 19 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  68%|██████▊   | 2252/3326 [03:40<02:16,  7.84store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 30, Dept 33 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  73%|███████▎  | 2420/3326 [03:57<01:01, 14.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 32, Dept 77 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  74%|███████▍  | 2463/3326 [04:02<01:27,  9.85store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 27 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  74%|███████▍  | 2477/3326 [04:05<01:45,  8.06store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 49 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  75%|███████▍  | 2483/3326 [04:05<01:31,  9.19store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 33, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  77%|███████▋  | 2562/3326 [04:13<00:47, 16.09store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 34, Dept 77 due to insufficient data\n",
            "Skipping Store 34, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2678/3326 [04:27<01:04,  9.97store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 29 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining ARIMA models:  81%|████████  | 2681/3326 [04:27<00:48, 13.20store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 36 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████  | 2696/3326 [04:30<01:28,  7.15store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  81%|████████▏ | 2704/3326 [04:31<01:14,  8.33store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 85 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  82%|████████▏ | 2716/3326 [04:33<00:52, 11.58store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 36, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  83%|████████▎ | 2758/3326 [04:37<00:54, 10.52store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 37, Dept 71 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  83%|████████▎ | 2777/3326 [04:39<00:33, 16.44store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 37, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  84%|████████▍ | 2809/3326 [04:44<00:51,  9.99store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 38, Dept 35 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  85%|████████▌ | 2841/3326 [04:48<00:44, 11.00store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 38, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  87%|████████▋ | 2896/3326 [04:55<01:25,  5.04store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 39, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  89%|████████▉ | 2976/3326 [05:02<00:26, 13.43store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 40, Dept 78 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  91%|█████████ | 3026/3326 [05:08<00:25, 11.91store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 41, Dept 37 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  93%|█████████▎| 3098/3326 [05:16<00:23,  9.86store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 42, Dept 34 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining ARIMA models:  93%|█████████▎| 3100/3326 [05:16<00:19, 11.61store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 42, Dept 41 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  95%|█████████▍| 3151/3326 [05:26<00:16, 10.67store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 43, Dept 24 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  95%|█████████▌| 3167/3326 [05:28<00:15, 10.55store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 43, Dept 55 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  97%|█████████▋| 3219/3326 [05:38<00:18,  5.70store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 44, Dept 34 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models:  98%|█████████▊| 3254/3326 [05:45<00:17,  4.18store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 44, Dept 99 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ARIMA models: 100%|█████████▉| 3325/3326 [05:53<00:00,  9.75store-dept/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Store 45, Dept 96 due to insufficient data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining ARIMA models: 100%|██████████| 3326/3326 [05:53<00:00,  9.41store-dept/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run ARIMA_Model_GridSearch at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/1e212ae6ec984521b6dbb3e28069426f\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lc0YZcP19bj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}