{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_experiment_lightGBM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abarb2022/Walmart-Recruiting---Store-Sales-Forecasting/blob/main/model_experiment_lightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Kaggle data sets directly into Colab**"
      ],
      "metadata": {
        "id": "SyZxTF7lf7jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the kaggle python library"
      ],
      "metadata": {
        "id": "7lvdgeEMgCoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwSaX9akGfG",
        "outputId": "1af396f2-c0eb-4cf4-bfc2-a4a21cbbc033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google drive so you can store your kaggle API credentials for future use"
      ],
      "metadata": {
        "id": "rw0DfSAggHED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGineQt7dErh",
        "outputId": "df043a95-c631-4805-d212-7beed5d2078f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a directory for kaggle at the temporary instance location on Colab drive.\n",
        "\n",
        "Download your kaggle API key (.json file). You can do this by going to your kaggle account page and clicking 'Create new API token' under the API section."
      ],
      "metadata": {
        "id": "Rvmi3WbigOmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vhywUxLXgjBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ZTkKggcylXfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file to Google Drive and then copy to the temporary location."
      ],
      "metadata": {
        "id": "p3N4it0xrFmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IQq6ZMyTrEfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the file permissions to read/write to the owner only"
      ],
      "metadata": {
        "id": "p3dHJgtLehrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7ncAtrq2lg5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Competitions and Datasets are the two types of Kaggle data**"
      ],
      "metadata": {
        "id": "Rb3Zm9VMlu3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Download competition data**\n",
        "\n",
        "If you get 403 Forbidden error, you need to click 'Late Submission' on the Kaggle page for that competition."
      ],
      "metadata": {
        "id": "OrdSFfGjl3Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0yNdtoRln8A",
        "outputId": "e3e229e4-bf3e-4bf6-bd45-b48124d6f222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 537MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip, in case the downloaded file is zipped. Refresh the files on the left hand side to update the view."
      ],
      "metadata": {
        "id": "fRmXZnHghNAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAs9oVnNoziL",
        "outputId": "6b87899d-07e2-4abc-f5e8-5a360e589862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder # For Type encoding if not using category dtype directly\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc # For garbage collection\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "RAb9vK9B7YFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stores = pd.read_csv('stores.csv')\n",
        "train = pd.read_csv(\"train.csv.zip\")\n",
        "features = pd.read_csv('features.csv.zip')\n",
        "sample = pd.read_csv('sampleSubmission.csv.zip')\n",
        "test = pd.read_csv('test.csv.zip')"
      ],
      "metadata": {
        "id": "255em5G65SWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' columns to datetime objects for easier manipulation\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "features['Date'] = pd.to_datetime(features['Date'])\n",
        "\n",
        "# Merge features with train and test data.\n",
        "# Note: 'IsHoliday' is present in both train/test and features.csv.\n",
        "# We'll merge on it to ensure consistency, but if there were discrepancies,\n",
        "# we'd need a more careful merge strategy.\n",
        "train_df = pd.merge(train, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "test_df = pd.merge(test, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "# Merge store information\n",
        "train_df = pd.merge(train_df, stores, on='Store', how='left')\n",
        "test_df = pd.merge(test_df, stores, on='Store', how='left')\n",
        "\n",
        "print(\"\\n--- Merged Train Data Head ---\")\n",
        "print(train_df.head())\n",
        "print(\"\\n--- Merged Test Data Head ---\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\n--- Merged Train Data Info ---\")\n",
        "print(train_df.info())\n",
        "print(\"\\n--- Merged Test Data Info ---\")\n",
        "print(test_df.info())\n",
        "\n",
        "# Free up memory\n",
        "del train, test, features, stores\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "OFPJWG6V5nZ3",
        "outputId": "09d87e7f-4aa9-4e90-a7fd-12b86da29b00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merged Train Data Head ---\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n",
            "0      1     1 2010-02-05      24924.50      False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106    A  151315\n",
            "1      1     1 2010-02-12      46039.49       True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106    A  151315\n",
            "2      1     1 2010-02-19      41595.55      False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106    A  151315\n",
            "3      1     1 2010-02-26      19403.54      False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106    A  151315\n",
            "4      1     1 2010-03-05      21827.90      False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106    A  151315\n",
            "\n",
            "--- Merged Test Data Head ---\n",
            "   Store  Dept       Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n",
            "0      1     1 2012-11-02      False        55.32       3.386    6766.44    5147.70      50.82    3639.90    2737.42  223.462779         6.573    A  151315\n",
            "1      1     1 2012-11-09      False        61.24       3.314   11421.32    3370.89      40.28    4646.79    6154.16  223.481307         6.573    A  151315\n",
            "2      1     1 2012-11-16      False        52.92       3.252    9696.28     292.10     103.78    1133.15    6612.69  223.512911         6.573    A  151315\n",
            "3      1     1 2012-11-23       True        56.23       3.211     883.59       4.17   74910.32     209.91     303.32  223.561947         6.573    A  151315\n",
            "4      1     1 2012-11-30      False        52.34       3.207    2460.03        NaN    3838.35     150.57    6966.34  223.610984         6.573    A  151315\n",
            "\n",
            "--- Merged Train Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 16 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         421570 non-null  int64         \n",
            " 1   Dept          421570 non-null  int64         \n",
            " 2   Date          421570 non-null  datetime64[ns]\n",
            " 3   Weekly_Sales  421570 non-null  float64       \n",
            " 4   IsHoliday     421570 non-null  bool          \n",
            " 5   Temperature   421570 non-null  float64       \n",
            " 6   Fuel_Price    421570 non-null  float64       \n",
            " 7   MarkDown1     150681 non-null  float64       \n",
            " 8   MarkDown2     111248 non-null  float64       \n",
            " 9   MarkDown3     137091 non-null  float64       \n",
            " 10  MarkDown4     134967 non-null  float64       \n",
            " 11  MarkDown5     151432 non-null  float64       \n",
            " 12  CPI           421570 non-null  float64       \n",
            " 13  Unemployment  421570 non-null  float64       \n",
            " 14  Type          421570 non-null  object        \n",
            " 15  Size          421570 non-null  int64         \n",
            "dtypes: bool(1), datetime64[ns](1), float64(10), int64(3), object(1)\n",
            "memory usage: 48.6+ MB\n",
            "None\n",
            "\n",
            "--- Merged Test Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 15 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   Store         115064 non-null  int64         \n",
            " 1   Dept          115064 non-null  int64         \n",
            " 2   Date          115064 non-null  datetime64[ns]\n",
            " 3   IsHoliday     115064 non-null  bool          \n",
            " 4   Temperature   115064 non-null  float64       \n",
            " 5   Fuel_Price    115064 non-null  float64       \n",
            " 6   MarkDown1     114915 non-null  float64       \n",
            " 7   MarkDown2     86437 non-null   float64       \n",
            " 8   MarkDown3     105235 non-null  float64       \n",
            " 9   MarkDown4     102176 non-null  float64       \n",
            " 10  MarkDown5     115064 non-null  float64       \n",
            " 11  CPI           76902 non-null   float64       \n",
            " 12  Unemployment  76902 non-null   float64       \n",
            " 13  Type          115064 non-null  object        \n",
            " 14  Size          115064 non-null  int64         \n",
            "dtypes: bool(1), datetime64[ns](1), float64(9), int64(3), object(1)\n",
            "memory usage: 12.4+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DATA CLEANING**\n"
      ],
      "metadata": {
        "id": "XSajWBEMo4CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to handle missing values for specific columns.\n",
        "    - MarkDown columns: fill with 0.\n",
        "    - Other specified numerical columns: fill with ffill then bfill, fallback to mean.\n",
        "    \"\"\"\n",
        "    def __init__(self, markdown_cols=None, numerical_cols_to_impute=None):\n",
        "        self.markdown_cols = markdown_cols if markdown_cols is not None else [f'MarkDown{i}' for i in range(1, 6)]\n",
        "        self.numerical_cols_to_impute = numerical_cols_to_impute if numerical_cols_to_impute is not None else ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "        self.means = {} # To store means for fallback imputation during transform\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate means for fallback imputation from the training data\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X.columns:\n",
        "                self.means[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "\n",
        "\n",
        "        for col in self.markdown_cols:\n",
        "          if col in X_copy.columns:\n",
        "            X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n",
        "            X_copy[col] = X_copy[col].fillna(0)\n",
        "\n",
        "\n",
        "        # Impute other numerical columns with ffill then bfill, fallback to mean\n",
        "        for col in self.numerical_cols_to_impute:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
        "                # Fallback to mean if NaNs still exist (e.g., if all values were NaN in a column)\n",
        "                if X_copy[col].isnull().any() and col in self.means:\n",
        "                    X_copy[col] = X_copy[col].fillna(self.means[col])\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "CFCOb354o73R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to extract temporal features from the 'Date' column.\n",
        "    \"\"\"\n",
        "    def __init__(self, date_column='Date'):\n",
        "        self.date_column = date_column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        if self.date_column not in X_copy.columns:\n",
        "            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n",
        "\n",
        "        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n",
        "\n",
        "        X_copy['Year'] = X_copy[self.date_column].dt.year\n",
        "        X_copy['Month'] = X_copy[self.date_column].dt.month\n",
        "        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n",
        "        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n",
        "\n",
        "        # Using .dt.isocalendar().week for consistent week numbering across years\n",
        "        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n",
        "        X_copy['Day'] = X_copy[self.date_column].dt.day\n",
        "        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n",
        "\n",
        "        # Convert IsHoliday to integer if it exists and is boolean\n",
        "        if 'IsHoliday' in X_copy.columns and X_copy['IsHoliday'].dtype == bool:\n",
        "            X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n",
        "\n",
        "        return X_copy.drop(columns=[self.date_column, \"Month\"]) # Drop the original Date column\n"
      ],
      "metadata": {
        "id": "yzQNQ5a8tRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalFeatureConverter(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom Transformer to convert specified columns to 'category' dtype\n",
        "    for LightGBM to handle them efficiently.\n",
        "    \"\"\"\n",
        "    def __init__(self, categorical_cols=None):\n",
        "        self.categorical_cols = categorical_cols if categorical_cols is not None else ['Store', 'Dept', 'Type']\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        for col in self.categorical_cols:\n",
        "            if col in X_copy.columns:\n",
        "                X_copy[col] = X_copy[col].astype('category')\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "cUuRfpD0toh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('date_features', DateFeatureExtractor(date_column='Date')),\n",
        "    ('imputer', MissingValueImputer(\n",
        "        markdown_cols=[f'MarkDown{i}' for i in range(1, 6)],\n",
        "        numerical_cols_to_impute=['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "    )),\n",
        "    ('categorical_converter', CategoricalFeatureConverter(categorical_cols=['Store', 'Dept', 'Type']))\n",
        "])\n",
        "\n",
        "print(\"\\n--- Applying Preprocessing Pipeline to Train Data ---\")\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(train_df.drop(columns=['Weekly_Sales', 'Id'], errors='ignore'))\n",
        "y_train = train_df['Weekly_Sales']\n",
        "# The 'Date' column is dropped by DateFeatureExtractor, so it won't be in X_train_processed.\n",
        "\n",
        "print(\"\\n--- Applying Preprocessing Pipeline to Test Data ---\")\n",
        "# For the test set, we only call transform, as fit was done on the training data.\n",
        "X_test_processed = preprocessing_pipeline.transform(test_df.drop(columns=['Id'], errors='ignore'))\n",
        "\n",
        "print(\"\\nProcessed X_train_processed info:\")\n",
        "print(X_train_processed.info())\n",
        "print(\"\\nProcessed X_test_processed info:\")\n",
        "print(X_test_processed.info())\n",
        "\n",
        "# Verify no missing values in processed data\n",
        "print(\"\\nMissing values in processed X_train_processed:\\n\", X_train_processed.isnull().sum().sum())\n",
        "print(\"Missing values in processed X_test_processed:\\n\", X_test_processed.isnull().sum().sum())\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "features_after_pipeline = X_train_processed.columns.tolist()\n",
        "categorical_features_after_pipeline = [col for col in ['Store', 'Dept', 'Type'] if col in features_after_pipeline]\n",
        "\n",
        "# Prepare weights for training (this is typically outside the main feature pipeline)\n",
        "# We need the 'IsHoliday' column which is now an integer from DateFeatureExtractor\n",
        "train_weights = np.where(X_train_processed['IsHoliday'] == 1, 5, 1)\n",
        "\n",
        "# Store test IDs for submission\n",
        "test_ids = test_df['Store'].astype(str) + '_' + test_df['Dept'].astype(str) + '_' + test_df['Date'].dt.strftime('%Y-%m-%d')\n",
        "\n"
      ],
      "metadata": {
        "id": "sq1ggoAwuQ8g",
        "outputId": "1b8dea60-faac-4291-810d-40299db128a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying Preprocessing Pipeline to Train Data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying Preprocessing Pipeline to Test Data ---\n",
            "\n",
            "Processed X_train_processed info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 25 columns):\n",
            " #   Column                 Non-Null Count   Dtype   \n",
            "---  ------                 --------------   -----   \n",
            " 0   Store                  421570 non-null  category\n",
            " 1   Dept                   421570 non-null  category\n",
            " 2   IsHoliday              421570 non-null  int64   \n",
            " 3   Temperature            421570 non-null  float64 \n",
            " 4   Fuel_Price             421570 non-null  float64 \n",
            " 5   MarkDown1              421570 non-null  float64 \n",
            " 6   MarkDown2              421570 non-null  float64 \n",
            " 7   MarkDown3              421570 non-null  float64 \n",
            " 8   MarkDown4              421570 non-null  float64 \n",
            " 9   MarkDown5              421570 non-null  float64 \n",
            " 10  CPI                    421570 non-null  float64 \n",
            " 11  Unemployment           421570 non-null  float64 \n",
            " 12  Type                   421570 non-null  category\n",
            " 13  Size                   421570 non-null  int64   \n",
            " 14  Year                   421570 non-null  int32   \n",
            " 15  Month_sin              421570 non-null  float64 \n",
            " 16  Month_cos              421570 non-null  float64 \n",
            " 17  Week                   421570 non-null  int64   \n",
            " 18  Day                    421570 non-null  int32   \n",
            " 19  DayOfWeek              421570 non-null  int32   \n",
            " 20  MarkDown1_was_missing  421570 non-null  int64   \n",
            " 21  MarkDown2_was_missing  421570 non-null  int64   \n",
            " 22  MarkDown3_was_missing  421570 non-null  int64   \n",
            " 23  MarkDown4_was_missing  421570 non-null  int64   \n",
            " 24  MarkDown5_was_missing  421570 non-null  int64   \n",
            "dtypes: category(3), float64(11), int32(3), int64(8)\n",
            "memory usage: 67.1 MB\n",
            "None\n",
            "\n",
            "Processed X_test_processed info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 25 columns):\n",
            " #   Column                 Non-Null Count   Dtype   \n",
            "---  ------                 --------------   -----   \n",
            " 0   Store                  115064 non-null  category\n",
            " 1   Dept                   115064 non-null  category\n",
            " 2   IsHoliday              115064 non-null  int64   \n",
            " 3   Temperature            115064 non-null  float64 \n",
            " 4   Fuel_Price             115064 non-null  float64 \n",
            " 5   MarkDown1              115064 non-null  float64 \n",
            " 6   MarkDown2              115064 non-null  float64 \n",
            " 7   MarkDown3              115064 non-null  float64 \n",
            " 8   MarkDown4              115064 non-null  float64 \n",
            " 9   MarkDown5              115064 non-null  float64 \n",
            " 10  CPI                    115064 non-null  float64 \n",
            " 11  Unemployment           115064 non-null  float64 \n",
            " 12  Type                   115064 non-null  category\n",
            " 13  Size                   115064 non-null  int64   \n",
            " 14  Year                   115064 non-null  int32   \n",
            " 15  Month_sin              115064 non-null  float64 \n",
            " 16  Month_cos              115064 non-null  float64 \n",
            " 17  Week                   115064 non-null  int64   \n",
            " 18  Day                    115064 non-null  int32   \n",
            " 19  DayOfWeek              115064 non-null  int32   \n",
            " 20  MarkDown1_was_missing  115064 non-null  int64   \n",
            " 21  MarkDown2_was_missing  115064 non-null  int64   \n",
            " 22  MarkDown3_was_missing  115064 non-null  int64   \n",
            " 23  MarkDown4_was_missing  115064 non-null  int64   \n",
            " 24  MarkDown5_was_missing  115064 non-null  int64   \n",
            "dtypes: category(3), float64(11), int32(3), int64(8)\n",
            "memory usage: 18.3 MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values in processed X_train_processed:\n",
            " 0\n",
            "Missing values in processed X_test_processed:\n",
            " 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort processed data by date for proper time-series splitting\n",
        "# We need to re-attach Date for splitting\n",
        "temp_train_df = X_train_processed.copy()\n",
        "temp_train_df['Date'] = pd.to_datetime(train_df['Date']) # Get original dates back for sorting\n",
        "temp_train_df['Weekly_Sales'] = y_train\n",
        "\n",
        "temp_train_df = temp_train_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Define a cutoff date for validation\n",
        "validation_cutoff_date = pd.to_datetime('2012-09-01')\n",
        "\n",
        "X_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date][features_after_pipeline]\n",
        "y_train_split = temp_train_df[temp_train_df['Date'] < validation_cutoff_date]['Weekly_Sales']\n",
        "\n",
        "X_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date][features_after_pipeline]\n",
        "y_val_split = temp_train_df[temp_train_df['Date'] >= validation_cutoff_date]['Weekly_Sales']\n",
        "\n"
      ],
      "metadata": {
        "id": "2tQfTxyeu5En"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Try different num_leaves and learning_rate values\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "lgb_params = {\n",
        "    'objective': 'regression_l1',\n",
        "    'metric': 'mae',\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.02,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 0.1,\n",
        "    'verbose': -1,\n",
        "    'n_jobs': -1,\n",
        "    'seed': 42,\n",
        "    'boosting_type': 'gbdt',\n",
        "    'max_depth': -1,\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'num_leaves': [32, 64, 128]\n",
        "}\n",
        "\n",
        "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "best_score = float('inf')\n",
        "best_params = None\n",
        "\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    model = lgb.LGBMRegressor(**params, **lgb_params)\n",
        "    model.fit(X_train_split, y_train_split,\n",
        "              eval_set=[(X_val_split, y_val_split)],\n",
        "              callbacks=[lgb.early_stopping(50)],\n",
        "              categorical_feature=categorical_features_after_pipeline)\n",
        "\n",
        "    val_preds = model.predict(X_val_split)\n",
        "    val_weights = np.where(X_val_split['IsHoliday'] == 1, 5, 1)\n",
        "    val_wmae = weighted_mean_absolute_error(y_val_split, val_preds, val_weights)\n",
        "    print(f\"WMAE on val set: {val_wmae:.4f}\")\n",
        "\n",
        "    train_predictions = model.predict(X_train_split)\n",
        "    train_weights = np.where(X_train_split['IsHoliday'] == 1, 5, 1)\n",
        "    train_wmae = weighted_mean_absolute_error(y_train_split, train_predictions, train_weights)\n",
        "    print(f\"WMAE on training set: {train_wmae:.4f}\")\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "    if val_wmae < best_score:\n",
        "        best_score = val_wmae\n",
        "        best_params = params\n",
        "        best_model = model\n",
        "\n",
        "print(\"Best WMAE:\", best_score)\n",
        "print(\"Best hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "id": "qeVfqK0C17OG",
        "outputId": "7d140bbc-bc81-488e-d520-75b35ecf39b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's l1: 1862.37\n",
            "WMAE on val set: 1997.3770\n",
            "WMAE on training set: 2338.9127\n",
            "--------------------------------------------------\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's l1: 1579.61\n",
            "WMAE on val set: 1692.7182\n",
            "WMAE on training set: 1943.3878\n",
            "--------------------------------------------------\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's l1: 1428.25\n",
            "WMAE on val set: 1526.0126\n",
            "WMAE on training set: 1633.4905\n",
            "--------------------------------------------------\n",
            "Best WMAE: 1526.0126321230625\n",
            "Best hyperparameters: {'num_leaves': 128}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predictions = model.predict(X_test_processed)\n",
        "\n",
        "# test_ids = test_df['Store'].astype(str) + '_' + test_df['Dept'].astype(str) + '_' + test_df['Date'].dt.strftime('%Y-%m-%d')\n",
        "# submission = pd.DataFrame({\n",
        "#     'Id': test_ids,\n",
        "#     'Weekly_Sales': test_predictions\n",
        "# })\n",
        "\n",
        "# submission.to_csv('submission.csv', index=False)\n",
        "# print(\"Submission file saved.\")\n"
      ],
      "metadata": {
        "id": "H4NVuVpDJGeD",
        "outputId": "8e1602c4-0caa-4c61-a572-6311328adca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q dagshub mlflow\n"
      ],
      "metadata": {
        "id": "nRj0MDGdJcdV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import dagshub\n",
        "# Try to get credentials from environment first\n",
        "dagshub.init(\n",
        "    repo_owner='abarb22',\n",
        "    repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "    mlflow=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "26gLz9xCSEt7",
        "outputId": "df9ab481-48f2-49e1-f22f-d8fab00f5d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from datetime import datetime\n",
        "\n",
        "mlflow.set_experiment(\"LightGBM_Training\")"
      ],
      "metadata": {
        "id": "cJm0CzS_SUw3",
        "outputId": "9e701ee6-f1fe-4458-a697-2a60445b4f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///content/mlruns/233425010286913265', creation_time=1751546494199, experiment_id='233425010286913265', last_update_time=1751546494199, lifecycle_stage='active', name='LightGBM_Training', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"LightGBM_Data_Cleaning\"):\n",
        "    # Log data cleaning parameters\n",
        "    mlflow.log_param(\"missing_value_strategy\", \"MarkDowns->0, others->ffill/bfill/mean\")\n",
        "    mlflow.log_param(\"date_features_extracted\", True)\n",
        "\n",
        "\n",
        "    # After cleaning, log metrics about data quality\n",
        "    mlflow.log_metric(\"train_samples\", len(train_df))\n",
        "    mlflow.log_metric(\"missing_values_filled\", train_df.isna().sum().sum())"
      ],
      "metadata": {
        "id": "8Y6AQ1I6SeXt",
        "outputId": "5776ca7b-07ed-4ccd-9dfd-a0848404bd06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run LightGBM_Data_Cleaning at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/0/runs/00101f5024ba48a6babbe257b51d7453\n",
            "🧪 View experiment at: https://dagshub.com/abarb22/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"LightGBM_Feature_Engineering\"):\n",
        "    # Log feature engineering parameters\n",
        "    mlflow.log_param(\"temporal_features\", [\"Year\", \"Month\", \"Week\", \"Day\", \"DayOfWeek\"])\n",
        "    mlflow.log_param(\"cyclical_features\", [\"Month_sin\", \"Month_cos\"])\n",
        "\n",
        "    # After feature engineering, log the new feature count\n",
        "    mlflow.log_metric(\"total_features\", len(X_train_processed.columns))"
      ],
      "metadata": {
        "id": "tbUC0VuaSlAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow.lightgbm\n",
        "\n",
        "\n",
        "with mlflow.start_run(run_name=\"lightGBM_Base_Model\"):\n",
        "    # Log model parameters\n",
        "    base_params = {\n",
        "        'objective': 'regression_l1',\n",
        "        'metric': 'mae',\n",
        "        'n_estimators': 100,\n",
        "        'learning_rate': 0.1\n",
        "    }\n",
        "    mlflow.log_params(base_params)\n",
        "\n",
        "    # Train and evaluate base model\n",
        "    model = lgb.LGBMRegressor(**base_params)\n",
        "    model.fit(X_train_split, y_train_split)\n",
        "\n",
        "    # Evaluate\n",
        "    val_preds = model.predict(X_val_split)\n",
        "    val_wmae = weighted_mean_absolute_error(y_val_split, val_preds, val_weights)\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"val_wmae\", val_wmae)\n",
        "    mlflow.lightgbm.log_model(model, \"base_model\")"
      ],
      "metadata": {
        "id": "fQZO4l56Srn6",
        "outputId": "bfd60f87-7388-4e54-c262-f7f5434c445a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050826 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2580\n",
            "[LightGBM] [Info] Number of data points in the train set: 397841, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 7630.430176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/03 12:57:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "\u001b[31m2025/07/03 12:58:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the run finished successfully\n",
        "import mlflow\n",
        "print(\"Current experiment:\", mlflow.get_experiment_by_name(\"LightGBM_Training\"))\n",
        "print(\"Active run:\", mlflow.active_run())"
      ],
      "metadata": {
        "id": "6QB0OSVhXEkQ",
        "outputId": "f3877654-ca45-4803-d694-362eb6590485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current experiment: <Experiment: artifact_location='file:///content/mlruns/233425010286913265', creation_time=1751546494199, experiment_id='233425010286913265', last_update_time=1751546494199, lifecycle_stage='active', name='LightGBM_Training', tags={}>\n",
            "Active run: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the MLflow tracking URI\n",
        "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
        "\n",
        "# List all runs in the experiment\n",
        "experiment = mlflow.get_experiment_by_name(\"LightGBM_Training\")\n",
        "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "print(f\"Number of runs: {len(runs)}\")\n",
        "print(runs[['run_id', 'status', 'start_time']] if len(runs) > 0 else \"No runs found\")"
      ],
      "metadata": {
        "id": "v7T91-2CXE81",
        "outputId": "d49d4c36-3111-48ac-9efe-b52482c1f5d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow tracking URI: file:///content/mlruns\n",
            "Number of runs: 7\n",
            "                             run_id    status                       start_time\n",
            "0  9d39bc6aab48426e828bd7312330f4c9  FINISHED 2025-07-03 12:53:30.649000+00:00\n",
            "1  95d2affee981402dbcd113dfe8907e67  FINISHED 2025-07-03 12:51:35.450000+00:00\n",
            "2  f483936bb5ff4449942b4f93456e7359  FINISHED 2025-07-03 12:49:38.114000+00:00\n",
            "3  c01cd738f4d643ffafe2776570b772b5    FAILED 2025-07-03 12:45:17.696000+00:00\n",
            "4  159e0d5862ec4006b8f57be1ee74a915    FAILED 2025-07-03 12:45:05.632000+00:00\n",
            "5  5258930e0ce3451c8435d7d016e4ab7a    FAILED 2025-07-03 12:44:33.020000+00:00\n",
            "6  ce5c9cc6f0b547868331d759fedb9595  FINISHED 2025-07-03 12:41:36.851000+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BO_8WkbqXJma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}