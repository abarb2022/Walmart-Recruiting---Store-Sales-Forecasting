{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.11",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "colab": {
            "name": "model_experiment_.ipynb",
            "provenance": [],
            "gpuType": "T4"
        },
        "accelerator": "GPU",

"kaggle": {
"accelerator": "none",
"dataSources": [
{
"sourceId": 3816,
"databundleVersionId": 32105,
"sourceType": "competition"
}
],
"dockerImageVersionId": 31040,
"isInternetEnabled": true,
"language": "python",
"sourceType": "notebook",
"isGpuEnabled": false
}
},
"nbformat_minor": 4,
"nbformat": 4,
"cells": [
{
"cell_type": "markdown",
"source": "**Downloading Kaggle data sets directly into Colab**",
"metadata": {
"id": "SyZxTF7lf7jk"
}
},
{
"cell_type": "markdown",
"source": "Install the kaggle python library",
"metadata": {
"id": "7lvdgeEMgCoy"
}
},
{
"cell_type": "code",
"source": "#! pip install kaggle",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "KlwSaX9akGfG",
"outputId": "c7807e21-3490-4f92-92c8-80179b4e65c4"
},
"outputs": [
{
"output_type": "stream",
"name": "stdout",
"text": [
    "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
    "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
    "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
    "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
    "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
    "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
    "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
    "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
    "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
    "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
    "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
    "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
    "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
    "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
    "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
]
}
],
"execution_count": 3
},
{
"cell_type": "markdown",
"source": "Mount the Google drive so you can store your kaggle API credentials for future use",
"metadata": {
"id": "rw0DfSAggHED"
}
},
{
"cell_type": "code",
"source": "# from google.colab import drive\n# drive.mount('/content/drive')",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "NGineQt7dErh",
"outputId": "7238e4ef-be86-4589-ed38-5c5c1dfffec3"
},
"outputs": [
{
"output_type": "stream",
"name": "stdout",
"text": [
    "Mounted at /content/drive\n"
]
}
],
"execution_count": 5
},
{
"cell_type": "markdown",
"source": "Make a directory for kaggle at the temporary instance location on Colab drive.\n\nDownload your kaggle API key (.json file). You can do this by going to your kaggle account page and clicking 'Create new API token' under the API section.",
"metadata": {
"id": "Rvmi3WbigOmT"
}
},
{
"cell_type": "code",
"source": "! mkdir ~/.kaggle",
"metadata": {
"id": "ZTkKggcylXfa"
},
"outputs": [],
"execution_count": 6
},
{
"cell_type": "markdown",
"source": "Upload the json file to Google Drive and then copy to the temporary location.",
"metadata": {
"id": "p3N4it0xrFmU"
}
},
{
"cell_type": "code",
"source": "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json",
"metadata": {
"id": "IQq6ZMyTrEfO"
},
"outputs": [],
"execution_count": 7
},
{
"cell_type": "markdown",
"source": "Change the file permissions to read/write to the owner only",
"metadata": {
"id": "p3dHJgtLehrM"
}
},
{
"cell_type": "code",
"source": "! chmod 600 ~/.kaggle/kaggle.json",
"metadata": {
"id": "7ncAtrq2lg5F"
},
"outputs": [],
"execution_count": 8
},
{
"cell_type": "markdown",
"source": "**Competitions and Datasets are the two types of Kaggle data**",
"metadata": {
"id": "Rb3Zm9VMlu3t"
}
},
{
"cell_type": "markdown",
"source": "**1. Download competition data**\n\nIf you get 403 Forbidden error, you need to click 'Late Submission' on the Kaggle page for that competition.",
"metadata": {
"id": "OrdSFfGjl3Ud"
}
},
{
"cell_type": "code",
"source": "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "C0yNdtoRln8A",
"outputId": "a4cab2d7-2d49-49cb-e800-20272d45fa7e"
},
"outputs": [
{
"output_type": "stream",
"name": "stdout",
"text": [
    "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
    "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
    "\r100% 2.70M/2.70M [00:00<00:00, 932MB/s]\n"
]
}
],
"execution_count": 9
},
{
"cell_type": "markdown",
"source": "Unzip, in case the downloaded file is zipped. Refresh the files on the left hand side to update the view.",
"metadata": {
"id": "fRmXZnHghNAz"
}
},
{
"cell_type": "code",
"source": "! unzip walmart-recruiting-store-sales-forecasting",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "dAs9oVnNoziL",
"outputId": "391981c0-ac36-416a-909f-9c268afe49ee"
},
"outputs": [
{
"output_type": "stream",
"name": "stdout",
"text": [
    "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
    "  inflating: features.csv.zip        \n",
    "  inflating: sampleSubmission.csv.zip  \n",
    "  inflating: stores.csv              \n",
    "  inflating: test.csv.zip            \n",
    "  inflating: train.csv.zip           \n"
]
}
],
"execution_count": 10
},
{
"cell_type": "code",
"source": "!pip install torch torchvision torchaudio\n!pip install pytorch-lightning\n!pip install optuna\n!pip install mlflow==2.7.1\n!pip install -q dagshub",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "J_V9Z33zhGHN",
"outputId": "bc05904a-139b-4c0a-9d31-394f446078ee",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T17:11:10.075750Z",
"iopub.execute_input": "2025-07-07T17:11:10.076087Z",
"iopub.status.idle": "2025-07-07T17:13:11.246268Z",
"shell.execute_reply.started": "2025-07-07T17:11:10.076060Z",
"shell.execute_reply": "2025-07-07T17:13:11.245237Z"
},
"collapsed": true,
"jupyter": {
"outputs_hidden": true
}
},
"outputs": [
{
"name": "stdout",
"text": "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1.post0)\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\nRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (25.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.18)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\nCollecting mlflow==2.7.1\n  Downloading mlflow-2.7.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (8.1.8)\nCollecting cloudpickle<3 (from mlflow==2.7.1)\n  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\nCollecting databricks-cli<1,>=0.8.7 (from mlflow==2.7.1)\n  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.4)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.44)\nRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.0.2)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.20.3)\nCollecting pytz<2024 (from mlflow==2.7.1)\n  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.32.3)\nCollecting packaging<24 (from mlflow==2.7.1)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow==2.7.1)\n  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.5.3)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.15.2)\nCollecting docker<7,>=4.0.0 (from mlflow==2.7.1)\n  Downloading docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\nCollecting Flask<3 (from mlflow==2.7.1)\n  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.26.4)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.15.2)\nRequirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.2.3)\nCollecting querystring-parser<2 (from mlflow==2.7.1)\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.0.40)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.2.2)\nCollecting pyarrow<14,>=4.0.0 (from mlflow==2.7.1)\n  Downloading pyarrow-13.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.7)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.7.2)\nCollecting gunicorn<22 (from mlflow==2.7.1)\n  Downloading gunicorn-21.2.0-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.6)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (4.13.2)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.10.1)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (3.2.2)\nRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (0.9.0)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (1.17.0)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.4.0)\nRequirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==2.7.1) (1.8.0)\nRequirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (3.1.3)\nRequirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (1.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==2.7.1) (4.0.12)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.7.1) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow==2.7.1) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (2.4.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow==2.7.1) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (2025.4.26)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (3.6.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.7.1) (3.1.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.7.1) (5.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mlflow==2.7.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mlflow==2.7.1) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->mlflow==2.7.1) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->mlflow==2.7.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->mlflow==2.7.1) (2024.2.0)\nDownloading mlflow-2.7.1-py3-none-any.whl (18.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nDownloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docker-6.1.3-py3-none-any.whl (148 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading flask-2.3.3-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\nDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-13.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pytz-2023.4-py2.py3-none-any.whl (506 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nInstalling collected packages: pytz, querystring-parser, packaging, importlib-metadata, cloudpickle, gunicorn, Flask, docker, databricks-cli, pyarrow, mlflow\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib_metadata 8.7.0\n    Uninstalling importlib_metadata-8.7.0:\n      Successfully uninstalled importlib_metadata-8.7.0\n  Attempting uninstall: cloudpickle\n    Found existing installation: cloudpickle 3.1.1\n    Uninstalling cloudpickle-3.1.1:\n      Successfully uninstalled cloudpickle-3.1.1\n  Attempting uninstall: Flask\n    Found existing installation: Flask 3.1.0\n    Uninstalling Flask-3.1.0:\n      Successfully uninstalled Flask-3.1.0\n  Attempting uninstall: docker\n    Found existing installation: docker 7.1.0\n    Uninstalling docker-7.1.0:\n      Successfully uninstalled docker-7.1.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\ndistributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\ndask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 13.0.0 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 13.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 13.0.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.6.0 requires pyarrow>=15.0.0, but you have pyarrow 13.0.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nbigframes 1.42.0 requires pyarrow>=15.0.2, but you have pyarrow 13.0.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Flask-2.3.3 cloudpickle-2.2.1 databricks-cli-0.18.0 docker-6.1.3 gunicorn-21.2.0 importlib-metadata-6.11.0 mlflow-2.7.1 packaging-23.2 pyarrow-13.0.0 pytz-2023.4 querystring-parser-1.2.4\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
"output_type": "stream"
}
],
"execution_count": 1
},
{
"cell_type": "code",
"source": "!pip uninstall scipy numpy -y\n!pip install numpy scipy",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "cXG_VgT4kx1I",
"outputId": "5e8a6ec0-6e7b-4c6a-9360-1e00b8143149",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T17:09:29.770113Z",
"iopub.execute_input": "2025-07-07T17:09:29.771237Z",
"iopub.status.idle": "2025-07-07T17:09:49.003004Z",
"shell.execute_reply.started": "2025-07-07T17:09:29.771199Z",
"shell.execute_reply": "2025-07-07T17:09:49.001958Z"
},
"collapsed": true,
"jupyter": {
"outputs_hidden": true
}
},
"outputs": [
{
"name": "stdout",
"text": "Found existing installation: scipy 1.15.2\nUninstalling scipy-1.15.2:\n  Successfully uninstalled scipy-1.15.2\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\nCollecting numpy\n  Using cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\nCollecting scipy\n  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\nDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, scipy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlflow 2.7.1 requires numpy<2, but you have numpy 2.3.1 which is incompatible.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.0 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\ncupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ndask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 13.0.0 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 13.0.0 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 13.0.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.6.0 requires pyarrow>=15.0.0, but you have pyarrow 13.0.0 which is incompatible.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.1 which is incompatible.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.0 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires pyarrow>=15.0.2, but you have pyarrow 13.0.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.1 scipy-1.16.0\n",
"output_type": "stream"
}
],
"execution_count": 7
},
{
"cell_type": "code",
"source": "import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport optuna\nimport mlflow\nimport dagshub\nfrom typing import Optional, List, Tuple, Dict, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.expand_frame_repr', False)",
"metadata": {
"id": "RAb9vK9B7YFb",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T17:13:52.982383Z",
"iopub.execute_input": "2025-07-07T17:13:52.983548Z",
"iopub.status.idle": "2025-07-07T17:14:13.531685Z",
"shell.execute_reply.started": "2025-07-07T17:13:52.983468Z",
"shell.execute_reply": "2025-07-07T17:14:13.530762Z"
}
},
"outputs": [
{
"name": "stderr",
"text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n* 'schema_extra' has been renamed to 'json_schema_extra'\n  warnings.warn(message, UserWarning)\n",
"output_type": "stream"
}
],
"execution_count": 2
},
{
"cell_type": "code",
"source": "# stores = pd.read_csv('stores.csv')\n# train = pd.read_csv(\"train.csv.zip\")\n# features = pd.read_csv('features.csv.zip')\n# sample = pd.read_csv('sampleSubmission.csv.zip')\n# test = pd.read_csv('test.csv.zip')\n\ndata_path = \"/kaggle/input/walmart-recruiting-store-sales-forecasting/\"\n\n# Read the datasets using the correct paths\nstores = pd.read_csv(data_path + 'stores.csv')\ntrain = pd.read_csv(data_path + 'train.csv.zip')\nfeatures = pd.read_csv(data_path + 'features.csv.zip')\nsample = pd.read_csv(data_path + 'sampleSubmission.csv.zip')\ntest = pd.read_csv(data_path + 'test.csv.zip')",
"metadata": {
"id": "255em5G65SWD",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:16:53.591789Z",
"iopub.execute_input": "2025-07-07T18:16:53.592222Z",
"iopub.status.idle": "2025-07-07T18:16:54.034639Z",
"shell.execute_reply.started": "2025-07-07T18:16:53.592195Z",
"shell.execute_reply": "2025-07-07T18:16:54.033525Z"
}
},
"outputs": [],
"execution_count": 30
},
{
"cell_type": "code",
"source": "# Convert 'Date' columns to datetime objects for easier manipulation\ntrain['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])\nfeatures['Date'] = pd.to_datetime(features['Date'])\n\n# Merge features with train and test data.\n# Note: 'IsHoliday' is present in both train/test and features.csv.\n# We'll merge on it to ensure consistency, but if there were discrepancies,\n# we'd need a more careful merge strategy.\ntrain_df = pd.merge(train, features, on=['Store', 'Date', 'IsHoliday'], how='left')\ntest_df = pd.merge(test, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n\n# Merge store information\ntrain_df = pd.merge(train_df, stores, on='Store', how='left')\ntest_df = pd.merge(test_df, stores, on='Store', how='left')\n\nprint(\"\\n--- Merged Train Data Head ---\")\nprint(train_df.head())\nprint(\"\\n--- Merged Test Data Head ---\")\nprint(test_df.head())\n\nprint(\"\\n--- Merged Train Data Info ---\")\nprint(train_df.info())\nprint(\"\\n--- Merged Test Data Info ---\")\nprint(test_df.info())\n\n# Free up memory\ndel train, test, features, stores\ngc.collect()",
"metadata": {
"id": "OFPJWG6V5nZ3",
"outputId": "e54b028d-bc15-4f62-c503-d65b5215790d",
"colab": {
"base_uri": "https://localhost:8080/"
},
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:16:56.656326Z",
"iopub.execute_input": "2025-07-07T18:16:56.657156Z",
"iopub.status.idle": "2025-07-07T18:16:58.345921Z",
"shell.execute_reply.started": "2025-07-07T18:16:56.657120Z",
"shell.execute_reply": "2025-07-07T18:16:58.344876Z"
}
},
"outputs": [
{
"name": "stdout",
"text": "\n--- Merged Train Data Head ---\n   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n0      1     1 2010-02-05      24924.50      False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106    A  151315\n1      1     1 2010-02-12      46039.49       True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106    A  151315\n2      1     1 2010-02-19      41595.55      False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106    A  151315\n3      1     1 2010-02-26      19403.54      False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106    A  151315\n4      1     1 2010-03-05      21827.90      False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106    A  151315\n\n--- Merged Test Data Head ---\n   Store  Dept       Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n0      1     1 2012-11-02      False        55.32       3.386    6766.44    5147.70      50.82    3639.90    2737.42  223.462779         6.573    A  151315\n1      1     1 2012-11-09      False        61.24       3.314   11421.32    3370.89      40.28    4646.79    6154.16  223.481307         6.573    A  151315\n2      1     1 2012-11-16      False        52.92       3.252    9696.28     292.10     103.78    1133.15    6612.69  223.512911         6.573    A  151315\n3      1     1 2012-11-23       True        56.23       3.211     883.59       4.17   74910.32     209.91     303.32  223.561947         6.573    A  151315\n4      1     1 2012-11-30      False        52.34       3.207    2460.03        NaN    3838.35     150.57    6966.34  223.610984         6.573    A  151315\n\n--- Merged Train Data Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 421570 entries, 0 to 421569\nData columns (total 16 columns):\n #   Column        Non-Null Count   Dtype         \n---  ------        --------------   -----         \n 0   Store         421570 non-null  int64         \n 1   Dept          421570 non-null  int64         \n 2   Date          421570 non-null  datetime64[ns]\n 3   Weekly_Sales  421570 non-null  float64       \n 4   IsHoliday     421570 non-null  bool          \n 5   Temperature   421570 non-null  float64       \n 6   Fuel_Price    421570 non-null  float64       \n 7   MarkDown1     150681 non-null  float64       \n 8   MarkDown2     111248 non-null  float64       \n 9   MarkDown3     137091 non-null  float64       \n 10  MarkDown4     134967 non-null  float64       \n 11  MarkDown5     151432 non-null  float64       \n 12  CPI           421570 non-null  float64       \n 13  Unemployment  421570 non-null  float64       \n 14  Type          421570 non-null  object        \n 15  Size          421570 non-null  int64         \ndtypes: bool(1), datetime64[ns](1), float64(10), int64(3), object(1)\nmemory usage: 48.6+ MB\nNone\n\n--- Merged Test Data Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 115064 entries, 0 to 115063\nData columns (total 15 columns):\n #   Column        Non-Null Count   Dtype         \n---  ------        --------------   -----         \n 0   Store         115064 non-null  int64         \n 1   Dept          115064 non-null  int64         \n 2   Date          115064 non-null  datetime64[ns]\n 3   IsHoliday     115064 non-null  bool          \n 4   Temperature   115064 non-null  float64       \n 5   Fuel_Price    115064 non-null  float64       \n 6   MarkDown1     114915 non-null  float64       \n 7   MarkDown2     86437 non-null   float64       \n 8   MarkDown3     105235 non-null  float64       \n 9   MarkDown4     102176 non-null  float64       \n 10  MarkDown5     115064 non-null  float64       \n 11  CPI           76902 non-null   float64       \n 12  Unemployment  76902 non-null   float64       \n 13  Type          115064 non-null  object        \n 14  Size          115064 non-null  int64         \ndtypes: bool(1), datetime64[ns](1), float64(9), int64(3), object(1)\nmemory usage: 12.4+ MB\nNone\n",
"output_type": "stream"
},
{
"execution_count": 31,
"output_type": "execute_result",
"data": {
    "text/plain": "1319"
},
"metadata": {}
}
],
"execution_count": 31
},
{
"cell_type": "markdown",
"source": "## **DATA CLEANING**\n",
"metadata": {
"id": "XSajWBEMo4CA"
}
},
{
"cell_type": "code",
"source": "class ImprovedMissingValueImputer(BaseEstimator, TransformerMixin):\n    \"\"\"Enhanced imputer that handles ALL missing values including Weekly_Sales\"\"\"\n    \n    def __init__(self, markdown_cols=None, numerical_cols=None, target_col='Weekly_Sales'):\n        self.markdown_cols = markdown_cols if markdown_cols is not None else [f'MarkDown{i}' for i in range(1, 6)]\n        self.numerical_cols = numerical_cols if numerical_cols is not None else [\n            'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size', target_col\n        ]\n        self.target_col = target_col\n        self.group_means = {}\n        self.global_means = {}\n        \n    def fit(self, X, y=None):\n        \"\"\"Fit imputer with group-wise and global statistics\"\"\"\n        # Calculate group means for target variable\n        if 'group_id' in X.columns and self.target_col in X.columns:\n            group_stats = X.groupby('group_id')[self.target_col].agg(['mean', 'median']).reset_index()\n            self.group_means = group_stats.set_index('group_id').to_dict()\n        \n        # Calculate global means for all numerical columns\n        for col in self.numerical_cols:\n            if col in X.columns:\n                self.global_means[col] = X[col].mean()\n                \n        return self\n    \n    def transform(self, X):\n        \"\"\"Transform with comprehensive missing value handling\"\"\"\n        X_copy = X.copy()\n        \n        # Handle MarkDown columns (fill with 0 and create indicators)\n        for col in self.markdown_cols:\n            if col in X_copy.columns:\n                X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n                X_copy[col] = X_copy[col].fillna(0)\n        \n        # Handle numerical columns with forward/backward fill first\n        for col in self.numerical_cols:\n            if col in X_copy.columns:\n                if 'group_id' in X_copy.columns:\n                    # Group-wise forward/backward fill\n                    X_copy[col] = X_copy.groupby('group_id')[col].transform(\n                        lambda x: x.fillna(method='ffill').fillna(method='bfill')\n                    )\n                else:\n                    # Global forward/backward fill\n                    X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n        \n        # Handle remaining missing values with group means (for target variable)\n        if self.target_col in X_copy.columns and 'group_id' in X_copy.columns:\n            def fill_group_mean(group):\n                group_id = group.name\n                if group_id in self.group_means.get('mean', {}):\n                    return group[self.target_col].fillna(self.group_means['mean'][group_id])\n                else:\n                    return group[self.target_col].fillna(self.global_means.get(self.target_col, 0))\n            \n            X_copy[self.target_col] = X_copy.groupby('group_id').apply(fill_group_mean).reset_index(level=0, drop=True)\n        \n        # Final cleanup: fill any remaining missing values with global means\n        for col in self.numerical_cols:\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].fillna(self.global_means.get(col, 0))\n        \n        return X_copy\n",
"metadata": {
"id": "CFCOb354o73R",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:17:06.679645Z",
"iopub.execute_input": "2025-07-07T18:17:06.680005Z",
"iopub.status.idle": "2025-07-07T18:17:06.696239Z",
"shell.execute_reply.started": "2025-07-07T18:17:06.679979Z",
"shell.execute_reply": "2025-07-07T18:17:06.694778Z"
}
},
"outputs": [],
"execution_count": 32
},
{
"cell_type": "code",
"source": "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, date_column='Date', keep_date=True):\n        self.date_column = date_column\n        self.keep_date = keep_date\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        if self.date_column not in X_copy.columns:\n            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n\n        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n\n        # Create time features\n        X_copy['Year'] = X_copy[self.date_column].dt.year\n        X_copy['Month'] = X_copy[self.date_column].dt.month\n        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n        X_copy['Day'] = X_copy[self.date_column].dt.day\n        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n\n        # Convert boolean to int\n        if 'IsHoliday' in X_copy.columns and X_copy['IsHoliday'].dtype == bool:\n            X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n\n        # Drop Month, optionally keep Date\n        columns_to_drop = [\"Month\"]\n        if not self.keep_date:\n            columns_to_drop.append(self.date_column)\n\n        return X_copy.drop(columns=columns_to_drop)",
"metadata": {
"id": "yzQNQ5a8tRsi",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:17:10.411870Z",
"iopub.execute_input": "2025-07-07T18:17:10.412371Z",
"iopub.status.idle": "2025-07-07T18:17:10.430742Z",
"shell.execute_reply.started": "2025-07-07T18:17:10.412331Z",
"shell.execute_reply": "2025-07-07T18:17:10.429586Z"
}
},
"outputs": [],
"execution_count": 33
},
{
"cell_type": "code",
"source": "class PatchTSTPreprocessor(BaseEstimator, TransformerMixin):\n    \"\"\"Preprocessor for PatchTST model that handles encoding and scaling\"\"\"\n    def __init__(self):\n        self.scalers = {}\n        self.categorical_encoders = {}\n\n    def fit(self, X, y=None):\n        # Fit scalers for numerical columns\n        numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n        # Remove target column if present\n        if 'Weekly_Sales' in numerical_cols:\n            numerical_cols.remove('Weekly_Sales')\n\n        for col in numerical_cols:\n            if col in X.columns:\n                self.scalers[col] = StandardScaler()\n                self.scalers[col].fit(X[[col]])\n\n        # Fit encoders for categorical columns\n        categorical_cols = ['Type']  # Store and Dept will be handled as group identifiers\n        for col in categorical_cols:\n            if col in X.columns:\n                unique_values = X[col].unique()\n                self.categorical_encoders[col] = {val: idx for idx, val in enumerate(unique_values)}\n\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n\n        # Scale numerical features\n        for col, scaler in self.scalers.items():\n            if col in X_copy.columns:\n                X_copy[col] = scaler.transform(X_copy[[col]]).flatten()\n\n        # Encode categorical features\n        for col, encoder in self.categorical_encoders.items():\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].map(encoder).fillna(-1)\n\n        # Create group identifier\n        X_copy['group_id'] = X_copy['Store'].astype(str) + '_' + X_copy['Dept'].astype(str)\n\n        return X_copy",
"metadata": {
"id": "cUuRfpD0toh1",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:17:14.947016Z",
"iopub.execute_input": "2025-07-07T18:17:14.947802Z",
"iopub.status.idle": "2025-07-07T18:17:14.957776Z",
"shell.execute_reply.started": "2025-07-07T18:17:14.947770Z",
"shell.execute_reply": "2025-07-07T18:17:14.956385Z"
}
},
"outputs": [],
"execution_count": 34
},
{
"cell_type": "code",
"source": "class PatchTSTModel(nn.Module):\n    def __init__(self, seq_len, pred_len, patch_len=16, stride=8, d_model=128,\n                 n_heads=8, n_layers=3, d_ff=256, dropout=0.1, num_features=1):\n        super().__init__()\n        \n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.patch_len = min(patch_len, seq_len)  # FIX: Ensure patch_len <= seq_len\n        self.stride = min(stride, self.patch_len)  # FIX: Ensure stride <= patch_len\n        self.d_model = d_model\n        self.num_features = num_features\n        \n        # FIX: Calculate number of patches correctly\n        self.n_patches = max(1, (seq_len - self.patch_len) // self.stride + 1)\n        \n        # FIX: Ensure d_model is divisible by n_heads\n        if d_model % n_heads != 0:\n            d_model = (d_model // n_heads) * n_heads\n            self.d_model = d_model\n        \n        # Input projection\n        self.patch_embedding = nn.Linear(self.patch_len * num_features, d_model)\n        \n        # Positional encoding\n        self.pos_encoding = nn.Parameter(torch.randn(1, self.n_patches, d_model) * 0.02)\n        \n        # FIX: Use standard TransformerEncoder with proper configuration\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            dim_feedforward=d_ff,\n            dropout=dropout,\n            batch_first=True,\n            activation='relu'\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        \n        # FIX: Better output projection\n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, pred_len)\n        )\n        \n        # FIX: Proper weight initialization\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.TransformerEncoderLayer):\n            for param in module.parameters():\n                if param.dim() > 1:\n                    torch.nn.init.xavier_uniform_(param)\n    \n    def create_patches(self, x):\n        \"\"\"Create patches from time series data\"\"\"\n        batch_size, seq_len, num_features = x.shape\n        \n        # FIX: Handle edge cases\n        if seq_len < self.patch_len:\n            # Pad sequence if too short\n            padding = self.patch_len - seq_len\n            x = F.pad(x, (0, 0, 0, padding), mode='constant', value=0)\n            seq_len = self.patch_len\n        \n        patches = []\n        for i in range(0, seq_len - self.patch_len + 1, self.stride):\n            patch = x[:, i:i+self.patch_len, :]\n            patches.append(patch)\n        \n        if len(patches) == 0:\n            patches = [x[:, :self.patch_len, :]]\n        \n        # Stack patches: [batch_size, n_patches, patch_len, num_features]\n        patches = torch.stack(patches, dim=1)\n        \n        # Reshape for embedding: [batch_size, n_patches, patch_len * num_features]\n        patches = patches.reshape(batch_size, patches.size(1), -1)\n        \n        return patches\n    \n    def forward(self, x):\n        # FIX: Add input validation\n        if torch.isnan(x).any() or torch.isinf(x).any():\n            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n        \n        # Create patches\n        patches = self.create_patches(x)\n        \n        # Embed patches\n        embedded = self.patch_embedding(patches)\n        \n        # Add positional encoding\n        pos_enc = self.pos_encoding[:, :embedded.size(1), :]\n        embedded = embedded + pos_enc\n        \n        # Apply transformer\n        encoded = self.transformer(embedded)\n        \n        # FIX: Use mean pooling instead of flattening\n        pooled = encoded.mean(dim=1)  # [batch_size, d_model]\n        \n        # Project to output\n        output = self.head(pooled)\n        \n        return output",
"metadata": {
"id": "rzlLnI_Ihf9p",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:17:19.140517Z",
"iopub.execute_input": "2025-07-07T18:17:19.140861Z",
"iopub.status.idle": "2025-07-07T18:17:19.158133Z",
"shell.execute_reply.started": "2025-07-07T18:17:19.140837Z",
"shell.execute_reply": "2025-07-07T18:17:19.156939Z"
}
},
"outputs": [],
"execution_count": 35
},
{
"cell_type": "code",
"source": "class PatchTSTLightningModule(pl.LightningModule):\n    def __init__(self, seq_len, pred_len, patch_len=16, stride=8, d_model=128,\n                 n_heads=8, n_layers=3, d_ff=256, dropout=0.1, num_features=1,\n                 learning_rate=1e-3, weight_decay=1e-4):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # FIX: Validate parameters\n        patch_len = min(patch_len, seq_len)\n        stride = min(stride, patch_len)\n        if d_model % n_heads != 0:\n            d_model = (d_model // n_heads) * n_heads\n        \n        self.model = PatchTSTModel(\n            seq_len=seq_len,\n            pred_len=pred_len,\n            patch_len=patch_len,\n            stride=stride,\n            d_model=d_model,\n            n_heads=n_heads,\n            n_layers=n_layers,\n            d_ff=d_ff,\n            dropout=dropout,\n            num_features=num_features\n        )\n        \n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        \n        # FIX: Add validation tracking\n        self.validation_step_outputs = []\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def compute_wmae(self, y_true, y_pred):\n        \"\"\"Compute Weighted Mean Absolute Error\"\"\"\n        # Handle shape mismatch\n        if y_pred.shape != y_true.shape:\n            min_len = min(y_pred.shape[-1], y_true.shape[-1])\n            y_pred = y_pred[:, :min_len]\n            y_true = y_true[:, :min_len]\n        \n        # Compute weights (based on magnitude of true values)\n        weights = torch.abs(y_true) + 1  # Add 1 to avoid zero weights\n        \n        # Compute weighted MAE\n        mae = torch.abs(y_pred - y_true)\n        wmae = torch.sum(weights * mae) / torch.sum(weights)\n        \n        return wmae\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        \n        # FIX: Robust error handling\n        try:\n            y_hat = self(x)\n            \n            # Compute WMAE (this will also be our loss)\n            wmae = self.compute_wmae(y, y_hat)\n            \n            # FIX: Check for invalid loss\n            if torch.isnan(wmae) or torch.isinf(wmae):\n                return None\n                \n            # Log both loss and WMAE (they're the same in this case)\n            self.log('train_loss', wmae, on_step=True, on_epoch=True, prog_bar=True)\n            self.log('train_wmae', wmae, on_step=True, on_epoch=True, prog_bar=True)\n            \n            return wmae\n            \n        except Exception as e:\n            print(f\"Training step failed: {e}\")\n            return None\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        \n        try:\n            y_hat = self(x)\n            \n            # Compute WMAE\n            wmae = self.compute_wmae(y, y_hat)\n            \n            if torch.isnan(wmae) or torch.isinf(wmae):\n                return None\n                \n            # Log both loss and WMAE (they're the same in this case)\n            self.log('val_loss', wmae, on_step=False, on_epoch=True, prog_bar=True)\n            self.log('val_wmae', wmae, on_step=False, on_epoch=True, prog_bar=True)\n            \n            self.validation_step_outputs.append(wmae)\n            return wmae\n            \n        except Exception as e:\n            print(f\"Validation step failed: {e}\")\n            return None\n    \n    def on_validation_epoch_end(self):\n        # FIX: Clear validation outputs\n        self.validation_step_outputs.clear()\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(),\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay\n        )\n        \n        # FIX: Add learning rate scheduler\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            mode='min',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6\n        )\n        \n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_wmae',  # CHANGE: Monitor WMAE instead of val_loss\n                'frequency': 1\n            }\n        }",
"metadata": {
"id": "T9oANChxh5vM",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:37:56.067086Z",
"iopub.execute_input": "2025-07-07T18:37:56.067524Z",
"iopub.status.idle": "2025-07-07T18:37:56.088308Z",
"shell.execute_reply.started": "2025-07-07T18:37:56.067480Z",
"shell.execute_reply": "2025-07-07T18:37:56.087179Z"
}
},
"outputs": [],
"execution_count": 42
},
{
"cell_type": "code",
"source": "class WalmartTSDataset(Dataset):\n    def __init__(self, df, seq_len, pred_len, feature_cols, target_col='Weekly_Sales'):\n        self.df = df.copy()\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.feature_cols = feature_cols\n        self.target_col = target_col\n        \n        # Ensure data is sorted\n        self.df = self.df.sort_values(['group_id', 'Date']).reset_index(drop=True)\n        \n        # Create samples\n        self.samples = self._create_samples()\n        \n        if len(self.samples) == 0:\n            raise ValueError(\"No valid samples created. Check your data and parameters.\")\n        \n        # FIX: Better scaling approach\n        self._prepare_scalers()\n        \n        print(f\"Dataset created with {len(self.samples)} samples\")\n    \n    def _create_samples(self):\n        \"\"\"Create valid samples from the dataset\"\"\"\n        samples = []\n        \n        for group_id in self.df['group_id'].unique():\n            group_data = self.df[self.df['group_id'] == group_id].copy()\n            group_data = group_data.reset_index(drop=True)\n            \n            # FIX: Check for sufficient data\n            if len(group_data) < self.seq_len + self.pred_len:\n                continue\n            \n            # FIX: Skip groups with too many missing values\n            if group_data[self.target_col].isna().sum() > len(group_data) * 0.5:\n                continue\n            \n            # Create sliding window samples\n            for i in range(len(group_data) - self.seq_len - self.pred_len + 1):\n                sample = {\n                    'group_id': group_id,\n                    'start_idx': i,\n                    'seq_end_idx': i + self.seq_len,\n                    'pred_end_idx': i + self.seq_len + self.pred_len,\n                    'group_data': group_data\n                }\n                samples.append(sample)\n        \n        return samples\n    \n    def _prepare_scalers(self):\n        \"\"\"Prepare scalers for features and target\"\"\"\n        # FIX: Collect all data for scaling\n        all_features = []\n        all_targets = []\n        \n        for sample in self.samples:\n            group_data = sample['group_data']\n            \n            # Get feature data\n            seq_data = group_data.iloc[sample['start_idx']:sample['seq_end_idx']]\n            features = seq_data[self.feature_cols].values\n            \n            # Get target data\n            pred_data = group_data.iloc[sample['seq_end_idx']:sample['pred_end_idx']]\n            target = pred_data[self.target_col].values\n            \n            # FIX: Handle missing values\n            features = np.nan_to_num(features, nan=0.0)\n            target = np.nan_to_num(target, nan=0.0)\n            \n            all_features.append(features)\n            all_targets.append(target)\n        \n        # Combine all data\n        all_features = np.concatenate(all_features, axis=0)\n        all_targets = np.concatenate(all_targets, axis=0).reshape(-1, 1)\n        \n        # FIX: Robust scaling\n        self.feature_scaler = StandardScaler()\n        self.target_scaler = StandardScaler()\n        \n        # Handle edge cases\n        if np.std(all_features, axis=0).min() < 1e-8:\n            # Add small noise to prevent division by zero\n            all_features += np.random.normal(0, 1e-8, all_features.shape)\n        \n        if np.std(all_targets) < 1e-8:\n            all_targets += np.random.normal(0, 1e-8, all_targets.shape)\n        \n        self.feature_scaler.fit(all_features)\n        self.target_scaler.fit(all_targets)\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        group_data = sample['group_data']\n        \n        # Get sequence data\n        seq_data = group_data.iloc[sample['start_idx']:sample['seq_end_idx']]\n        pred_data = group_data.iloc[sample['seq_end_idx']:sample['pred_end_idx']]\n        \n        # Extract features and target\n        features = seq_data[self.feature_cols].values\n        target = pred_data[self.target_col].values\n        \n        # FIX: Handle missing values\n        features = np.nan_to_num(features, nan=0.0)\n        target = np.nan_to_num(target, nan=0.0)\n        \n        # Scale data\n        features_scaled = self.feature_scaler.transform(features)\n        target_scaled = self.target_scaler.transform(target.reshape(-1, 1)).flatten()\n        \n        # Convert to tensors\n        X = torch.FloatTensor(features_scaled)\n        y = torch.FloatTensor(target_scaled)\n        \n        return X, y",
"metadata": {
"id": "7H1feePDiR9C",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:17:29.522641Z",
"iopub.execute_input": "2025-07-07T18:17:29.523952Z",
"iopub.status.idle": "2025-07-07T18:17:29.541605Z",
"shell.execute_reply.started": "2025-07-07T18:17:29.523906Z",
"shell.execute_reply": "2025-07-07T18:17:29.540451Z"
}
},
"outputs": [],
"execution_count": 37
},
{
"cell_type": "code",
"source": "improved_preprocessing_pipeline = Pipeline([\n    ('date_extractor', DateFeatureExtractor(keep_date=True)),  # Do this first\n    ('patchtst_preprocessor', PatchTSTPreprocessor()),  # This creates group_id\n    ('imputer', ImprovedMissingValueImputer(target_col='Weekly_Sales')),  # This uses group_id\n])\n\n# Apply preprocessing\nprint(\"Applying improved preprocessing...\")\ntrain_processed = improved_preprocessing_pipeline.fit_transform(train_df)\ntest_processed = improved_preprocessing_pipeline.transform(test_df)\n\nprint(f\"Train processed shape: {train_processed.shape}\")\nprint(f\"Test processed shape: {test_processed.shape}\")\n\n# Check for missing values after preprocessing\nprint(\"\\nMissing values after preprocessing:\")\nprint(\"Train missing values:\", train_processed.isnull().sum().sum())\nprint(\"Test missing values:\", test_processed.isnull().sum().sum())\n\n# Define parameters\nseq_len = 8\npred_len = 4\nvalidation_cutoff_date = pd.to_datetime('2012-02-01')\n\n# Split data temporally\ntrain_temporal = train_processed[train_processed['Date'] < validation_cutoff_date].copy()\nval_temporal = train_processed[train_processed['Date'] >= validation_cutoff_date].copy()\n\nprint(f\"\\nTemporal split:\")\nprint(f\"Training: {train_temporal['Date'].min()} to {train_temporal['Date'].max()}\")\nprint(f\"Validation: {val_temporal['Date'].min()} to {val_temporal['Date'].max()}\")\n\n# Filter groups with sufficient data\nmin_samples_per_group = seq_len + pred_len\ntrain_group_sizes = train_temporal.groupby('group_id').size()\nval_group_sizes = val_temporal.groupby('group_id').size()\n\nprint(f\"\\nGroup filtering:\")\nprint(f\"Total groups in train: {len(train_group_sizes)}\")\nprint(f\"Total groups in val: {len(val_group_sizes)}\")\n\n# Find groups with sufficient data\nvalid_train_groups = train_group_sizes[train_group_sizes >= min_samples_per_group].index\nvalid_val_groups = val_group_sizes[val_group_sizes >= min_samples_per_group].index\nvalid_groups = valid_train_groups.intersection(valid_val_groups)\n\nprint(f\"Groups with sufficient data: {len(valid_groups)}\")\n\n# Apply filtering\ntrain_filtered = train_temporal[train_temporal['group_id'].isin(valid_groups)].copy()\nval_filtered = val_temporal[val_temporal['group_id'].isin(valid_groups)].copy()\n\nprint(f\"Training samples after filtering: {len(train_filtered)}\")\nprint(f\"Validation samples after filtering: {len(val_filtered)}\")\n\n# Check for missing values in target after filtering\nprint(f\"\\nTarget missing values after filtering:\")\nprint(f\"Train Weekly_Sales missing: {train_filtered['Weekly_Sales'].isnull().sum()}\")\nprint(f\"Val Weekly_Sales missing: {val_filtered['Weekly_Sales'].isnull().sum()}\")\n\n# Define feature columns\nfeature_cols = [\n    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n    'Month_sin', 'Month_cos', 'Week', 'Day', 'Year', 'IsHoliday', 'DayOfWeek',\n    'Size', 'Type'\n] + [f'MarkDown{i}' for i in range(1, 6)] + [f'MarkDown{i}_was_missing' for i in range(1, 6)]\n\n# Filter existing columns\nfeature_cols = [col for col in feature_cols if col in train_filtered.columns]\nprint(f\"\\nFeature columns: {len(feature_cols)}\")\nprint(f\"Features: {feature_cols}\")\n\n# Create datasets\nprint(\"\\nCreating datasets...\")\ntry:\n    train_dataset = WalmartTSDataset(train_filtered, seq_len, pred_len, feature_cols)\n    val_dataset = WalmartTSDataset(val_filtered, seq_len, pred_len, feature_cols)\n    \n    print(f\"Training dataset size: {len(train_dataset)}\")\n    print(f\"Validation dataset size: {len(val_dataset)}\")\n    \n    # Test dataset creation\n    if len(train_dataset) > 0 and len(val_dataset) > 0:\n        test_batch = train_dataset[0]\n        print(f\"Sample batch shapes: X={test_batch[0].shape}, y={test_batch[1].shape}\")\n        print(\"Dataset creation successful!\")\n    else:\n        print(\"ERROR: Empty datasets created!\")\n        \nexcept Exception as e:\n    print(f\"ERROR creating datasets: {e}\")\n    print(\"This suggests there are still missing value issues or insufficient data.\")",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "WCTQe704iVvd",
"outputId": "5934635f-6d6e-4ca0-cf9b-bb18ed0dede6",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:38:08.996736Z",
"iopub.execute_input": "2025-07-07T18:38:08.997081Z",
"iopub.status.idle": "2025-07-07T18:44:13.807852Z",
"shell.execute_reply.started": "2025-07-07T18:38:08.997062Z",
"shell.execute_reply": "2025-07-07T18:44:13.806892Z"
}
},
"outputs": [
{
"name": "stdout",
"text": "Applying improved preprocessing...\nTrain processed shape: (421570, 28)\nTest processed shape: (115064, 27)\n\nMissing values after preprocessing:\nTrain missing values: 0\nTest missing values: 0\n\nTemporal split:\nTraining: 2010-02-05 00:00:00 to 2012-01-27 00:00:00\nValidation: 2012-02-03 00:00:00 to 2012-10-26 00:00:00\n\nGroup filtering:\nTotal groups in train: 3306\nTotal groups in val: 3204\nGroups with sufficient data: 2993\nTraining samples after filtering: 301424\nValidation samples after filtering: 114400\n\nTarget missing values after filtering:\nTrain Weekly_Sales missing: 0\nVal Weekly_Sales missing: 0\n\nFeature columns: 23\nFeatures: ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Month_sin', 'Month_cos', 'Week', 'Day', 'Year', 'IsHoliday', 'DayOfWeek', 'Size', 'Type', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'MarkDown1_was_missing', 'MarkDown2_was_missing', 'MarkDown3_was_missing', 'MarkDown4_was_missing', 'MarkDown5_was_missing']\n\nCreating datasets...\nDataset created with 268501 samples\nDataset created with 81477 samples\nTraining dataset size: 268501\nValidation dataset size: 81477\nSample batch shapes: X=torch.Size([8, 23]), y=torch.Size([4])\nDataset creation successful!\n",
"output_type": "stream"
}
],
"execution_count": 43
},
{
"cell_type": "code",
"source": "!pip install optuna-integration[pytorch_lightning]",
"metadata": {
"colab": {
"base_uri": "https://localhost:8080/"
},
"id": "avBLhwS855ku",
"outputId": "06978bba-051c-4c75-ca8e-de10b8bf9525",
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T17:29:25.748440Z",
"iopub.execute_input": "2025-07-07T17:29:25.748858Z",
"iopub.status.idle": "2025-07-07T17:29:30.174038Z",
"shell.execute_reply.started": "2025-07-07T17:29:25.748823Z",
"shell.execute_reply": "2025-07-07T17:29:30.172779Z"
},
"collapsed": true,
"jupyter": {
"outputs_hidden": true
}
},
"outputs": [
{
"name": "stdout",
"text": "Requirement already satisfied: optuna-integration[pytorch_lightning] in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from optuna-integration[pytorch_lightning]) (4.3.0)\nRequirement already satisfied: lightning in /usr/local/lib/python3.11/dist-packages (from optuna-integration[pytorch_lightning]) (2.5.2)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (6.0.2)\nRequirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (2025.3.2)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (0.14.3)\nRequirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (23.2)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (2.6.0+cu124)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (1.7.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (4.67.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (4.13.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning->optuna-integration[pytorch_lightning]) (2.5.1.post0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration[pytorch_lightning]) (1.15.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration[pytorch_lightning]) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration[pytorch_lightning]) (1.26.4)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration[pytorch_lightning]) (2.0.40)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna-integration[pytorch_lightning]) (1.3.10)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (3.11.18)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning->optuna-integration[pytorch_lightning]) (75.2.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[pytorch_lightning]) (3.1.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration[pytorch_lightning]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration[pytorch_lightning]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration[pytorch_lightning]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration[pytorch_lightning]) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration[pytorch_lightning]) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration[pytorch_lightning]) (2.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning->optuna-integration[pytorch_lightning]) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna->optuna-integration[pytorch_lightning]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna->optuna-integration[pytorch_lightning]) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna->optuna-integration[pytorch_lightning]) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna->optuna-integration[pytorch_lightning]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna->optuna-integration[pytorch_lightning]) (2024.2.0)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning->optuna-integration[pytorch_lightning]) (3.10)\n",
"output_type": "stream"
}
],
"execution_count": 16
},
{
"cell_type": "code",
"source": "# %pip install -q dagshub\n",
"metadata": {
"id": "nRj0MDGdJcdV",
"outputId": "1eac0a4c-90c9-420c-aec2-7cbe149729e0",
"colab": {
"base_uri": "https://localhost:8080/"
}
},
"outputs": [
{
"output_type": "stream",
"name": "stdout",
"text": [
    "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
    "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
    "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
    "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
    "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
    "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
    "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
    "\u001b[?25h"
]
}
],
"execution_count": null
},
{
"cell_type": "code",
"source": "#!pip install mlflow==2.7.1",
"metadata": {
"id": "Slx50n63bsZY",
"outputId": "ebebf54b-8e9e-4a66-9edb-f6e04ba281ca",
"colab": {
"base_uri": "https://localhost:8080/"
},
"collapsed": true,
"jupyter": {
"outputs_hidden": true
}
},
"outputs": [
{
"output_type": "stream",
"name": "stdout",
"text": [
    "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
    "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
    "\u001b[0mRequirement already satisfied: mlflow==2.7.1 in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
    "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (8.2.1)\n",
    "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.2.1)\n",
    "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.18.0)\n",
    "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.4)\n",
    "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.44)\n",
    "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.0.2)\n",
    "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (4.25.8)\n",
    "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2023.4)\n",
    "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.32.3)\n",
    "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (23.2)\n",
    "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.11.0)\n",
    "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.5.3)\n",
    "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.16.2)\n",
    "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.1.3)\n",
    "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.3.3)\n",
    "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.26.4)\n",
    "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.15.3)\n",
    "Collecting pandas<3 (from mlflow==2.7.1)\n",
    "  Using cached pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
    "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.2.4)\n",
    "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.0.41)\n",
    "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.6.1)\n",
    "Requirement already satisfied: pyarrow<14,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (13.0.0)\n",
    "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.8.2)\n",
    "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.10.0)\n",
    "Requirement already satisfied: gunicorn<22 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (21.2.0)\n",
    "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.6)\n",
    "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (1.1.3)\n",
    "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (4.14.0)\n",
    "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.10.1)\n",
    "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (3.3.1)\n",
    "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (0.9.0)\n",
    "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (1.17.0)\n",
    "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.4.0)\n",
    "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==2.7.1) (1.8.0)\n",
    "Requirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (3.1.3)\n",
    "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (2.2.0)\n",
    "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (1.9.0)\n",
    "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==2.7.1) (4.0.12)\n",
    "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.7.1) (3.23.0)\n",
    "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow==2.7.1) (3.0.2)\n",
    "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.3.2)\n",
    "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (0.12.1)\n",
    "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (4.58.4)\n",
    "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.4.8)\n",
    "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (11.2.1)\n",
    "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (3.2.3)\n",
    "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (2.9.0.post0)\n",
    "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow==2.7.1) (2025.2)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.4.2)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.10)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (2025.6.15)\n",
    "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (1.5.1)\n",
    "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (3.6.0)\n",
    "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.7.1) (3.2.3)\n",
    "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.7.1) (5.0.2)\n",
    "Using cached pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
    "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
    "\u001b[0mTraceback (most recent call last):\n",
    "  File \"/usr/lib/python3.11/pathlib.py\", line 540, in __str__\n",
    "    return self._str\n",
    "           ^^^^^^^^^\n",
    "AttributeError: 'PosixPath' object has no attribute '_str'\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
    "    status = run_func(*args)\n",
    "             ^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
    "    return func(self, options, args)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
    "    conflicts = self._determine_conflicts(to_install)\n",
    "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
    "    return check_install_conflicts(to_install)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
    "    package_set, _ = create_package_set_from_installed()\n",
    "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 39, in create_package_set_from_installed\n",
    "    for dist in env.iter_installed_distributions(local_only=False, skip=()):\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/base.py\", line 664, in <genexpr>\n",
    "    return (d for d in it if d.canonical_name not in skip)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/base.py\", line 612, in iter_all_distributions\n",
    "    for dist in self._iter_distributions():\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 176, in _iter_distributions\n",
    "    yield from finder.find(location)\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 79, in find\n",
    "    for dist, info_location in self._find_impl(location):\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 64, in _find_impl\n",
    "    raw_name = get_dist_name(dist)\n",
    "               ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_compat.py\", line 52, in get_dist_name\n",
    "    name = cast(Any, dist).name\n",
    "           ^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 622, in name\n",
    "    return self.metadata['Name']\n",
    "           ^^^^^^^^^^^^^\n",
    "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 610, in metadata\n",
    "    self.read_text('METADATA')\n",
    "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 939, in read_text\n",
    "    return self._path.joinpath(filename).read_text(encoding='utf-8')\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/lib/python3.11/pathlib.py\", line 1058, in read_text\n",
    "    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n",
    "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/lib/python3.11/pathlib.py\", line 1044, in open\n",
    "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/lib/python3.11/pathlib.py\", line 547, in __fspath__\n",
    "    return str(self)\n",
    "           ^^^^^^^^^\n",
    "  File \"/usr/lib/python3.11/pathlib.py\", line 542, in __str__\n",
    "    self._str = self._format_parsed_parts(self._drv, self._root,\n",
    "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/lib/python3.11/pathlib.py\", line 526, in _format_parsed_parts\n",
    "    return drv + root + cls._flavour.join(parts[1:])\n",
    "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "KeyboardInterrupt\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
    "    sys.exit(main())\n",
    "             ^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
    "    return command.main(cmd_args)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
    "    return self._main(args)\n",
    "           ^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
    "    return run(options, args)\n",
    "           ^^^^^^^^^^^^^^^^^^\n",
    "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
    "    logger.critical(\"Operation cancelled by user\")\n",
    "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
    "    self._log(CRITICAL, msg, args, **kwargs)\n",
    "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1610, in _log\n",
    "    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n",
    "\n",
    "KeyboardInterrupt\n",
    "^C\n"
]
}
],
"execution_count": 16
},
{
"cell_type": "code",
"source": "\nimport dagshub\n# Try to get credentials from environment first\ndagshub.init(\n    repo_owner='abarb22',\n    repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n    mlflow=True\n)\n\n",
"metadata": {
"id": "26gLz9xCSEt7",
"outputId": "84230985-4607-401f-8038-a9a4fe918c09",
"colab": {
"base_uri": "https://localhost:8080/",
"height": 212,
"referenced_widgets": [
    "a992dfdfea6c41259b39ed3a20e4a6d8",
    "2f7f3998ff224d3cb42cca738827d178"
]
},
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T17:29:51.989230Z",
"iopub.execute_input": "2025-07-07T17:29:51.989589Z",
"iopub.status.idle": "2025-07-07T17:30:00.681060Z",
"shell.execute_reply.started": "2025-07-07T17:29:51.989564Z",
"shell.execute_reply": "2025-07-07T17:30:00.680141Z"
}
},
"outputs": [
{
"output_type": "display_data",
"data": {
    "text/plain": "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n",
    "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"
},
"metadata": {}
},
{
"output_type": "display_data",
"data": {
    "text/plain": "Output()",
    "application/vnd.jupyter.widget-view+json": {
        "version_major": 2,
        "version_minor": 0,
        "model_id": ""
    }
},
"metadata": {}
},
{
"name": "stdout",
"text": "\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=1d1fabd1-9527-42ef-9daf-a4d3e6918312&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=5f21703fe4910423cd04743f6d7c567e82f6a283aed965ded8c7b2539b8974ec\n\n\n",
"output_type": "stream"
},
{
"output_type": "display_data",
"data": {
    "text/plain": "",
    "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
},
"metadata": {}
},
{
"output_type": "display_data",
"data": {
    "text/plain": "Accessing as alaki22\n",
    "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as alaki22\n</pre>\n"
},
"metadata": {}
},
{
"output_type": "display_data",
"data": {
    "text/plain": "Initialized MLflow to track repo \u001b[32m\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n",
    "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n</pre>\n"
},
"metadata": {}
},
{
"output_type": "display_data",
"data": {
    "text/plain": "Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
    "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n</pre>\n"
},
"metadata": {}
}
],
"execution_count": 18
},
{
"cell_type": "code",
"source": "class ManualHyperparameterTuner:\n    \"\"\"Manual hyperparameter tuning with comprehensive logging\"\"\"\n    \n    def __init__(self, train_dataset, val_dataset, feature_cols, seq_len, pred_len):\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.feature_cols = feature_cols\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.results = []\n        \n    def define_search_space(self):\n        \"\"\"Define hyperparameter search space\"\"\"\n        return {\n            'patch_len': [4, 8, 16],\n            'stride': [2, 4, 8],\n            'd_model': [64, 128, 256],\n            'n_heads': [4, 8],\n            'n_layers': [2, 3, 4],\n            'd_ff': [128, 256, 512],\n            'dropout': [0.1, 0.2, 0.3],\n            'learning_rate': [1e-4, 5e-4, 1e-3],\n            'weight_decay': [1e-6, 1e-5, 1e-4],\n            'batch_size': [16, 32, 64]\n        }\n    \n    def create_param_combinations(self, max_trials=50):\n        \"\"\"Create parameter combinations for tuning\"\"\"\n        search_space = self.define_search_space()\n        \n        # Alternative approach without using itertools.product\n        import random\n        \n        param_combinations = []\n        keys = list(search_space.keys())\n        \n        # Generate random combinations\n        for _ in range(max_trials):\n            param_dict = {}\n            for key in keys:\n                param_dict[key] = random.choice(search_space[key])\n            \n            # Apply constraints\n            if param_dict['patch_len'] > self.seq_len:\n                param_dict['patch_len'] = min(param_dict['patch_len'], self.seq_len)\n            \n            if param_dict['stride'] > param_dict['patch_len']:\n                param_dict['stride'] = param_dict['patch_len'] // 2\n            \n            if param_dict['d_model'] % param_dict['n_heads'] != 0:\n                param_dict['d_model'] = (param_dict['d_model'] // param_dict['n_heads']) * param_dict['n_heads']\n        \n            param_combinations.append(param_dict)\n        \n        return param_combinations\n    \n    def train_single_configuration(self, params, trial_num):\n        \"\"\"Train a single configuration and return results\"\"\"\n        print(f\"\\n=== Trial {trial_num} ===\")\n        print(f\"Parameters: {params}\")\n        \n        try:\n            # Create data loaders\n            train_loader = DataLoader(\n                self.train_dataset,\n                batch_size=params['batch_size'],\n                shuffle=True,\n                num_workers=0,\n                drop_last=True\n            )\n            \n            val_loader = DataLoader(\n                self.val_dataset,\n                batch_size=params['batch_size'],\n                shuffle=False,\n                num_workers=0,\n                drop_last=False\n            )\n            \n            # Create model\n            model = PatchTSTLightningModule(\n                seq_len=self.seq_len,\n                pred_len=self.pred_len,\n                patch_len=params['patch_len'],\n                stride=params['stride'],\n                d_model=params['d_model'],\n                n_heads=params['n_heads'],\n                n_layers=params['n_layers'],\n                d_ff=params['d_ff'],\n                dropout=params['dropout'],\n                num_features=len(self.feature_cols),\n                learning_rate=params['learning_rate'],\n                weight_decay=params['weight_decay']\n            )\n            \n            # Test model creation\n            test_batch = next(iter(train_loader))\n            test_x, test_y = test_batch\n            with torch.no_grad():\n                test_output = model(test_x)\n            \n            if torch.isnan(test_output).any() or torch.isinf(test_output).any():\n                raise ValueError(\"Model produces NaN/Inf outputs\")\n            \n            # Create trainer\n            trainer = pl.Trainer(\n                max_epochs=25,  # Reduced for faster tuning\n                callbacks=[\n                    EarlyStopping(monitor='val_wmae', patience=5, mode='min'),  # CHANGE: Monitor WMAE\n                ],\n                logger=False,\n                enable_checkpointing=False,\n                enable_progress_bar=False,\n                accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n                devices=1,\n                gradient_clip_val=1.0,\n                gradient_clip_algorithm='norm'\n            )\n            \n            # Train\n            trainer.fit(model, train_loader, val_loader)\n            \n            # Get results - CHANGE: Get WMAE metrics\n            train_wmae = trainer.callback_metrics.get('train_wmae', float('inf'))\n            val_wmae = trainer.callback_metrics.get('val_wmae', float('inf'))\n            \n            # Convert to float if tensor\n            if hasattr(train_wmae, 'item'):\n                train_wmae = train_wmae.item()\n            if hasattr(val_wmae, 'item'):\n                val_wmae = val_wmae.item()\n            \n            # Check for invalid results\n            if np.isnan(train_wmae) or np.isnan(val_wmae) or np.isinf(train_wmae) or np.isinf(val_wmae):\n                raise ValueError(\"Invalid WMAE values\")\n            \n            result = {\n                'trial': trial_num,\n                'train_wmae': train_wmae,  # CHANGE: Store WMAE instead of loss\n                'val_wmae': val_wmae,     # CHANGE: Store WMAE instead of loss\n                'train_loss': train_wmae,  # Keep for compatibility\n                'val_loss': val_wmae,      # Keep for compatibility\n                'epochs_trained': trainer.current_epoch,\n                'parameters': params.copy(),\n                'model_size': sum(p.numel() for p in model.parameters()),\n                'status': 'success'\n            }\n            \n            print(f\"Train WMAE: {train_wmae:.6f}, Val WMAE: {val_wmae:.6f}\")  # CHANGE: Print WMAE\n            \n            # Clean up\n            del model, trainer, train_loader, val_loader\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            gc.collect()\n            \n            return result\n            \n        except Exception as e:\n            print(f\"Trial {trial_num} failed: {str(e)}\")\n            return {\n                'trial': trial_num,\n                'train_wmae': float('inf'),  # CHANGE: Add WMAE fields\n                'val_wmae': float('inf'),    # CHANGE: Add WMAE fields\n                'train_loss': float('inf'),\n                'val_loss': float('inf'),\n                'epochs_trained': 0,\n                'parameters': params.copy(),\n                'model_size': 0,\n                'status': f'failed: {str(e)}'\n            }\n    \n    def run_hyperparameter_search(self, max_trials=20):\n        \"\"\"Run complete hyperparameter search\"\"\"\n        print(f\"Starting manual hyperparameter tuning with {max_trials} trials...\")\n        \n        # Get parameter combinations\n        param_combinations = self.create_param_combinations(max_trials)\n        \n        # Start MLflow run for the entire tuning process\n        with mlflow.start_run(run_name=\"PatchTST_Manual_Hyperparameter_Tuning_WMAE\"):  # CHANGE: Update run name\n            \n            # Log general information\n            mlflow.log_param(\"total_trials\", len(param_combinations))\n            mlflow.log_param(\"seq_len\", self.seq_len)\n            mlflow.log_param(\"pred_len\", self.pred_len)\n            mlflow.log_param(\"train_dataset_size\", len(self.train_dataset))\n            mlflow.log_param(\"val_dataset_size\", len(self.val_dataset))\n            mlflow.log_param(\"num_features\", len(self.feature_cols))\n            mlflow.log_param(\"metric\", \"WMAE\")  # CHANGE: Log that we're using WMAE\n            \n            # Run trials\n            for i, params in enumerate(param_combinations):\n                \n                # Start child run for each trial\n                with mlflow.start_run(run_name=f\"Trial_{i+1}\", nested=True):\n                    \n                    # Log all parameters\n                    for key, value in params.items():\n                        mlflow.log_param(key, value)\n                    \n                    # Train and get results\n                    result = self.train_single_configuration(params, i+1)\n                    \n                    # Log results - CHANGE: Log WMAE metrics\n                    mlflow.log_metric(\"train_wmae\", result['train_wmae'])\n                    mlflow.log_metric(\"val_wmae\", result['val_wmae'])\n                    mlflow.log_metric(\"train_loss\", result['train_loss'])  # Keep for compatibility\n                    mlflow.log_metric(\"val_loss\", result['val_loss'])      # Keep for compatibility\n                    mlflow.log_metric(\"epochs_trained\", result['epochs_trained'])\n                    mlflow.log_param(\"model_size\", result['model_size'])\n                    mlflow.log_param(\"status\", result['status'])\n                    \n                    # Store result\n                    self.results.append(result)\n            \n            # Find best trial - CHANGE: Use WMAE for selection\n            valid_results = [r for r in self.results if r['status'] == 'success']\n            if valid_results:\n                best_result = min(valid_results, key=lambda x: x['val_wmae'])  # CHANGE: Use val_wmae\n                \n                # Log best results\n                mlflow.log_metric(\"best_train_wmae\", best_result['train_wmae'])  # CHANGE: Log WMAE\n                mlflow.log_metric(\"best_val_wmae\", best_result['val_wmae'])      # CHANGE: Log WMAE\n                mlflow.log_metric(\"best_train_loss\", best_result['train_loss'])\n                mlflow.log_metric(\"best_val_loss\", best_result['val_loss'])\n                mlflow.log_param(\"best_trial\", best_result['trial'])\n                \n                for key, value in best_result['parameters'].items():\n                    mlflow.log_param(f\"best_{key}\", value)\n                \n                print(f\"\\n=== BEST TRIAL FOUND ===\")\n                print(f\"Trial: {best_result['trial']}\")\n                print(f\"Validation WMAE: {best_result['val_wmae']:.6f}\")  # CHANGE: Display WMAE\n                print(f\"Training WMAE: {best_result['train_wmae']:.6f}\")   # CHANGE: Display WMAE\n                print(f\"Parameters: {best_result['parameters']}\")\n                \n                return best_result\n            else:\n                print(\"No successful trials found!\")\n                return None",
"metadata": {
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:23:57.882243Z",
"iopub.execute_input": "2025-07-07T18:23:57.882836Z",
"iopub.status.idle": "2025-07-07T18:23:57.921527Z",
"shell.execute_reply.started": "2025-07-07T18:23:57.882801Z",
"shell.execute_reply": "2025-07-07T18:23:57.920307Z"
}
},
"outputs": [],
"execution_count": 39
},
{
"cell_type": "code",
"source": "def train_final_model(best_params, train_dataset, val_dataset, feature_cols, seq_len, pred_len):\n    \"\"\"Train the final model with best parameters\"\"\"\n    \n    print(f\"\\n=== TRAINING FINAL MODEL ===\")\n    print(f\"Using parameters: {best_params}\")\n    \n    with mlflow.start_run(run_name=\"PatchTST_Final_Model_WMAE\"):  # CHANGE: Update run name\n        \n        # Log all parameters\n        mlflow.log_param(\"seq_len\", seq_len)\n        mlflow.log_param(\"pred_len\", pred_len)\n        mlflow.log_param(\"train_dataset_size\", len(train_dataset))\n        mlflow.log_param(\"val_dataset_size\", len(val_dataset))\n        mlflow.log_param(\"num_features\", len(feature_cols))\n        mlflow.log_param(\"metric\", \"WMAE\")  # CHANGE: Log that we're using WMAE\n        \n        for key, value in best_params.items():\n            mlflow.log_param(key, value)\n        \n        # Create final dataloaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=best_params['batch_size'],\n            shuffle=True,\n            num_workers=0,\n            drop_last=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=best_params['batch_size'],\n            shuffle=False,\n            num_workers=0,\n            drop_last=False\n        )\n        \n        # Create final model\n        final_model = PatchTSTLightningModule(\n            seq_len=seq_len,\n            pred_len=pred_len,\n            patch_len=best_params['patch_len'],\n            stride=best_params['stride'],\n            d_model=best_params['d_model'],\n            n_heads=best_params['n_heads'],\n            n_layers=best_params['n_layers'],\n            d_ff=best_params['d_ff'],\n            dropout=best_params['dropout'],\n            num_features=len(feature_cols),\n            learning_rate=best_params['learning_rate'],\n            weight_decay=best_params['weight_decay']\n        )\n        \n        # Log model info\n        total_params = sum(p.numel() for p in final_model.parameters())\n        mlflow.log_param(\"total_parameters\", total_params)\n        mlflow.log_param(\"model_size_mb\", total_params * 4 / 1024 / 1024)\n        \n        print(f\"Model parameters: {total_params:,}\")\n        \n        # Create trainer - CHANGE: Monitor WMAE\n        trainer = pl.Trainer(\n            max_epochs=100,\n            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n            devices=1,\n            callbacks=[\n                EarlyStopping(monitor=\"val_wmae\", patience=15, mode=\"min\"),  # CHANGE: Monitor WMAE\n                LearningRateMonitor(logging_interval='epoch'),\n                ModelCheckpoint(monitor=\"val_wmae\", mode=\"min\", save_top_k=1),  # CHANGE: Monitor WMAE\n            ],\n            enable_progress_bar=True,\n        )\n        \n        # Train model\n        print(\"Training final model...\")\n        trainer.fit(final_model, train_loader, val_loader)\n        \n        # Log final results - CHANGE: Log WMAE metrics\n        final_train_wmae = trainer.callback_metrics.get(\"train_wmae\", 0)\n        final_val_wmae = trainer.callback_metrics.get(\"val_wmae\", 0)\n        final_train_loss = trainer.callback_metrics.get(\"train_loss\", 0)\n        final_val_loss = trainer.callback_metrics.get(\"val_loss\", 0)\n        \n        if hasattr(final_train_wmae, 'item'):\n            final_train_wmae = final_train_wmae.item()\n        if hasattr(final_val_wmae, 'item'):\n            final_val_wmae = final_val_wmae.item()\n        if hasattr(final_train_loss, 'item'):\n            final_train_loss = final_train_loss.item()\n        if hasattr(final_val_loss, 'item'):\n            final_val_loss = final_val_loss.item()\n        \n        mlflow.log_metric(\"final_train_wmae\", final_train_wmae)  # CHANGE: Log WMAE\n        mlflow.log_metric(\"final_val_wmae\", final_val_wmae)      # CHANGE: Log WMAE\n        mlflow.log_metric(\"final_train_loss\", final_train_loss)\n        mlflow.log_metric(\"final_val_loss\", final_val_loss)\n        mlflow.log_metric(\"final_epochs\", trainer.current_epoch)\n        \n        print(f\"Final validation WMAE: {final_val_wmae:.6f}\")  # CHANGE: Display WMAE\n        print(f\"Final training WMAE: {final_train_wmae:.6f}\")   # CHANGE: Display WMAE\n        print(f\"Epochs trained: {trainer.current_epoch}\")\n        \n        return final_model, trainer",
"metadata": {
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:24:04.567987Z",
"iopub.execute_input": "2025-07-07T18:24:04.568301Z",
"iopub.status.idle": "2025-07-07T18:24:04.585898Z",
"shell.execute_reply.started": "2025-07-07T18:24:04.568277Z",
"shell.execute_reply": "2025-07-07T18:24:04.584915Z"
}
},
"outputs": [],
"execution_count": 40
},
{
"cell_type": "code",
"source": "tuner = ManualHyperparameterTuner(\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    feature_cols=feature_cols,\n    seq_len=seq_len,\n    pred_len=pred_len\n)\n\n# Run hyperparameter search\nbest_result = tuner.run_hyperparameter_search(max_trials=25)\n\n# Save results\nresults_df = tuner.get_results_dataframe()\nprint(f\"\\nHyperparameter tuning completed. Results shape: {results_df.shape}\")\n\n# Display top results\nif not results_df.empty:\n    print(\"\\nTop 5 results:\")\n    top_results = results_df.nsmallest(5, 'val_loss')\n    display_cols = ['trial', 'val_loss', 'train_loss', 'epochs_trained', 'status']\n    print(top_results[display_cols].to_string(index=False))\n    \n    # Display parameter summary for best result\n    if best_result:\n        print(f\"\\nBest parameters:\")\n        for key, value in best_result['parameters'].items():\n            print(f\"  {key}: {value}\")\n",
"metadata": {
"trusted": true,
"execution": {
"iopub.status.busy": "2025-07-07T18:53:43.382836Z",
"iopub.execute_input": "2025-07-07T18:53:43.383192Z"
}
},
"outputs": [
{
"name": "stdout",
"text": "Starting manual hyperparameter tuning with 25 trials...\n",
"output_type": "stream"
},
{
"name": "stderr",
"text": "INFO: GPU available: False, used: False\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n",
"output_type": "stream"
},
{
"name": "stdout",
"text": "\n=== Trial 1 ===\nParameters: {'patch_len': 8, 'stride': 4, 'd_model': 256, 'n_heads': 4, 'n_layers': 4, 'd_ff': 512, 'dropout': 0.2, 'learning_rate': 0.0005, 'weight_decay': 1e-05, 'batch_size': 32}\n",
"output_type": "stream"
}
],
"execution_count": null
},
{
"cell_type": "code",
"source": "\n# Replace the existing final evaluation section with this:\nif best_result and best_result['status'] == 'success':\n    print(\"\\n=== TRAINING FINAL MODEL WITH BEST PARAMETERS ===\")\n    \n    # Log best hyperparameters\n    print(\"Best Hyperparameters:\")\n    for param, value in best_result['parameters'].items():\n        print(f\"  {param}: {value}\")\n    \n    final_model, final_trainer = train_final_model(\n        best_params=best_result['parameters'],\n        train_dataset=train_dataset,\n        val_dataset=val_dataset,\n        feature_cols=feature_cols,\n        seq_len=seq_len,\n        pred_len=pred_len\n    )\n    print(\"Training completed successfully!\")\n    \n    # Additional evaluation\n    print(\"\\n=== FINAL MODEL EVALUATION ===\")\n    \n    # Get predictions on validation set\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    final_model.eval()\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            x, y = batch\n            y_pred = final_model(x)\n            all_predictions.append(y_pred.cpu().numpy())\n            all_targets.append(y.cpu().numpy())\n    \n    predictions = np.concatenate(all_predictions, axis=0)\n    targets = np.concatenate(all_targets, axis=0)\n    \n    # Calculate WMAE using the same logic as in the model\n    weights = np.abs(targets) + 1  # Add 1 to avoid zero weights\n    absolute_errors = np.abs(predictions - targets)\n    wmae = np.sum(weights * absolute_errors) / np.sum(weights)\n    \n    print(f\"Final Validation Metrics:\")\n    print(f\"  WMAE: {wmae:.6f}\")\n    \n    # CHANGE: Log final metrics with WMAE focus\n    with mlflow.start_run(run_name=\"Final_Model_Metrics_WMAE\"):\n        # Log best hyperparameters\n        for param, value in best_result['parameters'].items():\n            mlflow.log_param(f\"best_{param}\", value)\n        \n        # Log final WMAE\n        mlflow.log_metric(\"final_wmae\", wmae)\n        \n        # Log best validation score from hyperparameter tuning\n        mlflow.log_metric(\"best_validation_wmae\", best_result['val_wmae'])  # CHANGE: Use WMAE\n\nelse:\n    print(\"No successful hyperparameter tuning results found!\")\n    print(\"Check the preprocessing and dataset creation steps.\")",
"metadata": {
"trusted": true
},
"outputs": [],
"execution_count": null
}
]
}