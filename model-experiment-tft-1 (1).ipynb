{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder # For Type encoding if not using category dtype directly\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc # For garbage collection\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.expand_frame_repr', False)","metadata":{"id":"RAb9vK9B7YFb","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:42:42.947981Z","iopub.execute_input":"2025-07-13T10:42:42.948320Z","iopub.status.idle":"2025-07-13T10:42:42.954749Z","shell.execute_reply.started":"2025-07-13T10:42:42.948295Z","shell.execute_reply":"2025-07-13T10:42:42.953729Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# stores = pd.read_csv('stores.csv')\n# train = pd.read_csv(\"train.csv.zip\")\n# features = pd.read_csv('features.csv.zip')\n# sample = pd.read_csv('sampleSubmission.csv.zip')\n# test = pd.read_csv('test.csv.zip')\n\ndata_path = \"/kaggle/input/walmart-recruiting-store-sales-forecasting/\"\n\n# Read the datasets using the correct paths\nstores = pd.read_csv(data_path + 'stores.csv')\ntrain = pd.read_csv(data_path + 'train.csv.zip')\nfeatures = pd.read_csv(data_path + 'features.csv.zip')\nsample = pd.read_csv(data_path + 'sampleSubmission.csv.zip')\ntest = pd.read_csv(data_path + 'test.csv.zip')","metadata":{"id":"255em5G65SWD","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:42:42.956468Z","iopub.execute_input":"2025-07-13T10:42:42.956766Z","iopub.status.idle":"2025-07-13T10:42:43.412123Z","shell.execute_reply.started":"2025-07-13T10:42:42.956745Z","shell.execute_reply":"2025-07-13T10:42:43.411101Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%pip install -q dagshub\n","metadata":{"id":"nRj0MDGdJcdV","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:42:43.413096Z","iopub.execute_input":"2025-07-13T10:42:43.413361Z","iopub.status.idle":"2025-07-13T10:42:47.563434Z","shell.execute_reply.started":"2025-07-13T10:42:43.413339Z","shell.execute_reply":"2025-07-13T10:42:47.562325Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install mlflow==2.7.1","metadata":{"id":"Slx50n63bsZY","outputId":"6815aa15-895d-43be-fca4-d108783d7ff0","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:42:47.565799Z","iopub.execute_input":"2025-07-13T10:42:47.566120Z","iopub.status.idle":"2025-07-13T10:42:51.770909Z","shell.execute_reply.started":"2025-07-13T10:42:47.566092Z","shell.execute_reply":"2025-07-13T10:42:51.769732Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: mlflow==2.7.1 in /usr/local/lib/python3.11/dist-packages (2.7.1)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (8.2.1)\nRequirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.2.1)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.18.0)\nRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.4)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.44)\nRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.0.2)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.20.3)\nRequirement already satisfied: pytz<2024 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2023.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.32.4)\nRequirement already satisfied: packaging<24 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (23.2)\nRequirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.11.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (0.5.3)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.16.2)\nRequirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (6.1.3)\nRequirement already satisfied: Flask<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.3.3)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.26.4)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.15.3)\nRequirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.2.3)\nRequirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.2.4)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (2.0.41)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (1.2.2)\nRequirement already satisfied: pyarrow<14,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (13.0.0)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.8.2)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.7.2)\nRequirement already satisfied: gunicorn<22 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (21.2.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.7.1) (3.1.6)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow==2.7.1) (4.14.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.10.1)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (3.3.1)\nRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (0.9.0)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (1.17.0)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.7.1) (2.5.0)\nRequirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==2.7.1) (1.8.0)\nRequirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (3.1.3)\nRequirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.7.1) (1.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==2.7.1) (4.0.12)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.7.1) (3.23.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow==2.7.1) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.7.1) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==2.7.1) (2.4.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow==2.7.1) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.7.1) (2025.6.15)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow==2.7.1) (3.6.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.7.1) (3.2.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.7.1) (5.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mlflow==2.7.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mlflow==2.7.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->mlflow==2.7.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->mlflow==2.7.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->mlflow==2.7.1) (2024.2.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\nimport dagshub\n# Try to get credentials from environment first\ndagshub.init(\n    repo_owner='abarb22',\n    repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n    mlflow=True\n)\n\n","metadata":{"id":"26gLz9xCSEt7","outputId":"61b33956-e8d7-45b6-eb46-48fc3747318b","colab":{"base_uri":"https://localhost:8080/","height":65},"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:42:51.772183Z","iopub.execute_input":"2025-07-13T10:42:51.772472Z","iopub.status.idle":"2025-07-13T10:43:01.707255Z","shell.execute_reply.started":"2025-07-13T10:42:51.772444Z","shell.execute_reply":"2025-07-13T10:43:01.706286Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=e31bf3a6-7054-490d-8714-d285ff42ae90&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=3218b371305d08997ad1f32cfd563952fbbaa4043ac01c11222030266cd09a1d\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as alaki22\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as alaki22\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"abarb22/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository abarb22/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n</pre>\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"class MissingValueImputer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom Transformer to handle missing values for specific columns.\n    - MarkDown columns: fill with 0.\n    - Other specified numerical columns: fill with ffill then bfill, fallback to mean.\n    \"\"\"\n    def __init__(self, markdown_cols=None, numerical_cols_to_impute=None):\n        self.markdown_cols = markdown_cols if markdown_cols is not None else [f'MarkDown{i}' for i in range(1, 6)]\n        self.numerical_cols_to_impute = numerical_cols_to_impute if numerical_cols_to_impute is not None else ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n        self.means = {} # To store means for fallback imputation during transform\n\n    def fit(self, X, y=None):\n        # Calculate means for fallback imputation from the training data\n        for col in self.numerical_cols_to_impute:\n            if col in X.columns:\n                self.means[col] = X[col].mean()\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n\n\n        for col in self.markdown_cols:\n          if col in X_copy.columns:\n            X_copy[f\"{col}_was_missing\"] = X_copy[col].isna().astype(int)\n            X_copy[col] = X_copy[col].fillna(0)\n\n\n        # Impute other numerical columns with ffill then bfill, fallback to mean\n        for col in self.numerical_cols_to_impute:\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n                # Fallback to mean if NaNs still exist (e.g., if all values were NaN in a column)\n                if X_copy[col].isnull().any() and col in self.means:\n                    X_copy[col] = X_copy[col].fillna(self.means[col])\n        return X_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:43:01.708223Z","iopub.execute_input":"2025-07-13T10:43:01.708538Z","iopub.status.idle":"2025-07-13T10:43:01.717924Z","shell.execute_reply.started":"2025-07-13T10:43:01.708508Z","shell.execute_reply":"2025-07-13T10:43:01.716783Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, date_column='Date', keep_date=True):\n        self.date_column = date_column\n        self.keep_date = keep_date\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X_copy = X.copy()\n        if self.date_column not in X_copy.columns:\n            raise ValueError(f\"Date column '{self.date_column}' not found in DataFrame.\")\n\n        X_copy[self.date_column] = pd.to_datetime(X_copy[self.date_column])\n\n        # Create time features\n        X_copy['Year'] = X_copy[self.date_column].dt.year\n        X_copy['Month'] = X_copy[self.date_column].dt.month\n        X_copy['Month_sin'] = np.sin(2 * np.pi * X_copy['Month'] / 12)\n        X_copy['Month_cos'] = np.cos(2 * np.pi * X_copy['Month'] / 12)\n        X_copy['Week'] = X_copy[self.date_column].dt.isocalendar().week.astype(int)\n        X_copy['Day'] = X_copy[self.date_column].dt.day\n        X_copy['DayOfWeek'] = X_copy[self.date_column].dt.dayofweek\n\n        # Convert boolean to int\n        if 'IsHoliday' in X_copy.columns and X_copy['IsHoliday'].dtype == bool:\n            X_copy['IsHoliday'] = X_copy['IsHoliday'].astype(int)\n\n        # Drop Month, optionally keep Date\n        columns_to_drop = [\"Month\"]\n        if not self.keep_date:\n            columns_to_drop.append(self.date_column)\n\n        return X_copy.drop(columns=columns_to_drop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:43:01.718927Z","iopub.execute_input":"2025-07-13T10:43:01.719278Z","iopub.status.idle":"2025-07-13T10:43:01.744916Z","shell.execute_reply.started":"2025-07-13T10:43:01.719256Z","shell.execute_reply":"2025-07-13T10:43:01.743856Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Convert 'Date' columns to datetime objects for easier manipulation\ntrain['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])\nfeatures['Date'] = pd.to_datetime(features['Date'])\n\n# Merge features with train and test data.\n# Note: 'IsHoliday' is present in both train/test and features.csv.\n# We'll merge on it to ensure consistency, but if there were discrepancies,\n# we'd need a more careful merge strategy.\ntrain_df = pd.merge(train, features, on=['Store', 'Date', 'IsHoliday'], how='left')\ntest_df = pd.merge(test, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n\n# Merge store information\ntrain_df = pd.merge(train_df, stores, on='Store', how='left')\ntest_df = pd.merge(test_df, stores, on='Store', how='left')\n\nprint(\"\\n--- Merged Train Data Head ---\")\nprint(train_df.head())\nprint(\"\\n--- Merged Test Data Head ---\")\nprint(test_df.head())\n\nprint(\"\\n--- Merged Train Data Info ---\")\nprint(train_df.info())\nprint(\"\\n--- Merged Test Data Info ---\")\nprint(test_df.info())\n\n# Apply your preprocessing pipeline (imputer + date features)\npipe = Pipeline([\n    (\"imputer\", MissingValueImputer()),\n    (\"date_features\", DateFeatureExtractor()),\n])\ntrain_df = pipe.fit_transform(train_df)\ntest_df = pipe.transform(test_df)\n\n# ➕ ADD THIS NOW: time_idx column\n# min_date = train_df['Date'].min()\n# train_df[\"time_idx\"] = (train_df[\"Date\"] - min_date).dt.days\n\n# Free up memory\ndel train, test, features, stores\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:43:01.746082Z","iopub.execute_input":"2025-07-13T10:43:01.746952Z","iopub.status.idle":"2025-07-13T10:43:02.858504Z","shell.execute_reply.started":"2025-07-13T10:43:01.746926Z","shell.execute_reply":"2025-07-13T10:43:02.857600Z"}},"outputs":[{"name":"stdout","text":"\n--- Merged Train Data Head ---\n   Store  Dept       Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n0      1     1 2010-02-05      24924.50      False        42.31       2.572        NaN        NaN        NaN        NaN        NaN  211.096358         8.106    A  151315\n1      1     1 2010-02-12      46039.49       True        38.51       2.548        NaN        NaN        NaN        NaN        NaN  211.242170         8.106    A  151315\n2      1     1 2010-02-19      41595.55      False        39.93       2.514        NaN        NaN        NaN        NaN        NaN  211.289143         8.106    A  151315\n3      1     1 2010-02-26      19403.54      False        46.63       2.561        NaN        NaN        NaN        NaN        NaN  211.319643         8.106    A  151315\n4      1     1 2010-03-05      21827.90      False        46.50       2.625        NaN        NaN        NaN        NaN        NaN  211.350143         8.106    A  151315\n\n--- Merged Test Data Head ---\n   Store  Dept       Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment Type    Size\n0      1     1 2012-11-02      False        55.32       3.386    6766.44    5147.70      50.82    3639.90    2737.42  223.462779         6.573    A  151315\n1      1     1 2012-11-09      False        61.24       3.314   11421.32    3370.89      40.28    4646.79    6154.16  223.481307         6.573    A  151315\n2      1     1 2012-11-16      False        52.92       3.252    9696.28     292.10     103.78    1133.15    6612.69  223.512911         6.573    A  151315\n3      1     1 2012-11-23       True        56.23       3.211     883.59       4.17   74910.32     209.91     303.32  223.561947         6.573    A  151315\n4      1     1 2012-11-30      False        52.34       3.207    2460.03        NaN    3838.35     150.57    6966.34  223.610984         6.573    A  151315\n\n--- Merged Train Data Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 421570 entries, 0 to 421569\nData columns (total 16 columns):\n #   Column        Non-Null Count   Dtype         \n---  ------        --------------   -----         \n 0   Store         421570 non-null  int64         \n 1   Dept          421570 non-null  int64         \n 2   Date          421570 non-null  datetime64[ns]\n 3   Weekly_Sales  421570 non-null  float64       \n 4   IsHoliday     421570 non-null  bool          \n 5   Temperature   421570 non-null  float64       \n 6   Fuel_Price    421570 non-null  float64       \n 7   MarkDown1     150681 non-null  float64       \n 8   MarkDown2     111248 non-null  float64       \n 9   MarkDown3     137091 non-null  float64       \n 10  MarkDown4     134967 non-null  float64       \n 11  MarkDown5     151432 non-null  float64       \n 12  CPI           421570 non-null  float64       \n 13  Unemployment  421570 non-null  float64       \n 14  Type          421570 non-null  object        \n 15  Size          421570 non-null  int64         \ndtypes: bool(1), datetime64[ns](1), float64(10), int64(3), object(1)\nmemory usage: 48.6+ MB\nNone\n\n--- Merged Test Data Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 115064 entries, 0 to 115063\nData columns (total 15 columns):\n #   Column        Non-Null Count   Dtype         \n---  ------        --------------   -----         \n 0   Store         115064 non-null  int64         \n 1   Dept          115064 non-null  int64         \n 2   Date          115064 non-null  datetime64[ns]\n 3   IsHoliday     115064 non-null  bool          \n 4   Temperature   115064 non-null  float64       \n 5   Fuel_Price    115064 non-null  float64       \n 6   MarkDown1     114915 non-null  float64       \n 7   MarkDown2     86437 non-null   float64       \n 8   MarkDown3     105235 non-null  float64       \n 9   MarkDown4     102176 non-null  float64       \n 10  MarkDown5     115064 non-null  float64       \n 11  CPI           76902 non-null   float64       \n 12  Unemployment  76902 non-null   float64       \n 13  Type          115064 non-null  object        \n 14  Size          115064 non-null  int64         \ndtypes: bool(1), datetime64[ns](1), float64(9), int64(3), object(1)\nmemory usage: 12.4+ MB\nNone\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/tmp/ipykernel_36/1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n/tmp/ipykernel_36/1027183038.py:32: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  X_copy[col] = X_copy[col].fillna(method='ffill').fillna(method='bfill')\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"27"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Custom TFT Implementation for Walmart Sales Forecasting\n# This avoids version conflicts by implementing TFT from scratch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport mlflow\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple, Optional\nimport math\n\n# Custom Dataset class for time series\nclass WalmartTimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length=24, forecast_length=6, target_col='Weekly_Sales'):\n        self.data = data.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n        self.sequence_length = sequence_length\n        self.forecast_length = forecast_length\n        self.target_col = target_col\n        \n        # Group by store-dept combination\n        self.groups = self.data.groupby(['Store', 'Dept'])\n        self.samples = self._create_samples()\n        \n    def _create_samples(self):\n        samples = []\n        \n        for (store, dept), group in self.groups:\n            group = group.sort_values('Date').reset_index(drop=True)\n            \n            # Skip if not enough data\n            if len(group) < self.sequence_length + self.forecast_length:\n                continue\n                \n            # Create sliding windows\n            for i in range(len(group) - self.sequence_length - self.forecast_length + 1):\n                sample = {\n                    'store': store,\n                    'dept': dept,\n                    'start_idx': i,\n                    'group_data': group\n                }\n                samples.append(sample)\n        \n        return samples\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        group_data = sample['group_data']\n        start_idx = sample['start_idx']\n        \n        # Historical sequence\n        hist_end = start_idx + self.sequence_length\n        hist_data = group_data.iloc[start_idx:hist_end]\n        \n        # Future sequence\n        future_data = group_data.iloc[hist_end:hist_end + self.forecast_length]\n        \n        # Prepare features\n        hist_features = self._prepare_features(hist_data)\n        future_features = self._prepare_features(future_data, is_future=True)\n        \n        # Target values\n        hist_target = torch.tensor(hist_data[self.target_col].values, dtype=torch.float32)\n        future_target = torch.tensor(future_data[self.target_col].values, dtype=torch.float32)\n        \n        # Weights (5 for holidays, 1 for non-holidays)\n        future_weights = torch.tensor(future_data['IsHoliday'].apply(lambda x: 5.0 if x else 1.0).values, dtype=torch.float32)\n        \n        return {\n            'hist_features': hist_features,\n            'future_features': future_features,\n            'hist_target': hist_target,\n            'future_target': future_target,\n            'future_weights': future_weights\n        }\n    \n    def _prepare_features(self, data, is_future=False):\n        features = []\n        \n        # Time features\n        features.extend([\n            data['Year'].values,\n            data['Month_sin'].values,\n            data['Month_cos'].values,\n            data['Week'].values,\n            data['Day'].values,\n            data['DayOfWeek'].values,\n        ])\n        \n        # Known reals (available for future)\n        features.extend([\n            data['IsHoliday'].astype(float).values,\n            data['Temperature'].values,\n            data['Fuel_Price'].values,\n            data['CPI'].values,\n            data['Unemployment'].values,\n        ])\n        \n        # MarkDown features\n        for i in range(1, 6):\n            if f'MarkDown{i}' in data.columns:\n                features.append(data[f'MarkDown{i}'].values)\n        \n        # Store type (categorical -> numerical)\n        type_mapping = {'A': 0, 'B': 1, 'C': 2}\n        features.append([type_mapping.get(t, 0) for t in data['Type'].values])\n        \n        # Store size (normalize)\n        features.append(data['Size'].values / 200000)  # Simple normalization\n        \n        return torch.tensor(np.column_stack(features), dtype=torch.float32)\n\n# Attention mechanism for TFT\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n        \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear transformations\n        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        \n        # Attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        context = torch.matmul(attn_weights, V)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        # Output projection\n        output = self.w_o(context)\n        \n        # Residual connection and layer norm\n        return self.layer_norm(output + query)\n\nclass VariableSelectionNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.input_dim = input_dim\n        \n        self.flattened_grn = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Softmax(dim=-1)\n        )\n        \n        # Process each variable to a single output dimension (not hidden_dim)\n        self.single_variable_grns = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(1, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_dim, 1)  # Output single dimension per variable\n            ) for _ in range(input_dim)\n        ])\n        \n        # Final projection to hidden_dim\n        self.final_projection = nn.Linear(input_dim, hidden_dim)\n        \n    def forward(self, x):\n        # x shape: (batch_size, seq_len, input_dim)\n        batch_size, seq_len, _ = x.shape\n        \n        # Get variable importance weights\n        weights = self.flattened_grn(x)  # Shape: (batch_size, seq_len, input_dim)\n        \n        # Apply variable-specific processing\n        processed_vars = []\n        for i, grn in enumerate(self.single_variable_grns):\n            var_input = x[:, :, i:i+1]  # Shape: (batch_size, seq_len, 1)\n            var_processed = grn(var_input)  # Shape: (batch_size, seq_len, 1)\n            processed_vars.append(var_processed)\n        \n        processed = torch.cat(processed_vars, dim=-1)  # Shape: (batch_size, seq_len, input_dim)\n        \n        # Apply importance weights\n        selected = processed * weights  # Shape: (batch_size, seq_len, input_dim)\n        \n        # Project to hidden dimension\n        output = self.final_projection(selected)  # Shape: (batch_size, seq_len, hidden_dim)\n        \n        return output\n\n# Updated TFT Model using the fixed VSN\nclass TemporalFusionTransformer(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, n_heads=4, n_layers=2, dropout=0.1):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.input_dim = input_dim\n        \n        # Use the simpler, fixed variable selection\n        self.variable_selection = VariableSelectionNetwork(input_dim, hidden_dim, dropout)\n        \n        # LSTM for temporal processing\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=n_layers, \n                           dropout=dropout, batch_first=True)\n        \n        # Multi-head attention\n        self.attention = MultiHeadAttention(hidden_dim, n_heads, dropout)\n        \n        # Output layers\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n    def forward(self, hist_features, future_features):\n        # Process historical features\n        hist_selected = self.variable_selection(hist_features)  # Now outputs (batch, seq, hidden_dim)\n        \n        # LSTM processing\n        lstm_out, (h_n, c_n) = self.lstm(hist_selected)\n        \n        # Attention mechanism\n        attended = self.attention(lstm_out, lstm_out, lstm_out)\n        \n        # Use last hidden state for future prediction\n        last_hidden = attended[:, -1:, :]  # Shape: (batch, 1, hidden_dim)\n        \n        # Predict future values\n        future_predictions = []\n        hidden_state = last_hidden\n        \n        for i in range(future_features.size(1)):  # For each future time step\n            # Project future features\n            future_step = future_features[:, i:i+1, :]\n            future_selected = self.variable_selection(future_step)\n            \n            # Combine with hidden state\n            combined = hidden_state + future_selected\n            \n            # Apply attention\n            attended_future = self.attention(combined, combined, combined)\n            \n            # Predict\n            prediction = self.output_projection(attended_future)\n            future_predictions.append(prediction)\n            \n            # Update hidden state\n            hidden_state = attended_future\n        \n        predictions = torch.cat(future_predictions, dim=1)\n        return predictions.squeeze(-1)  \n        \n\n\n\n\n\n# Custom WMAE Loss\nclass WMAELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, predictions, targets, weights):\n        mae = torch.abs(predictions - targets)\n        weighted_mae = mae * weights\n        return weighted_mae.sum() / weights.sum()\n\n# Training function\ndef train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n    criterion = WMAELoss()\n    \n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        \n        for batch in train_loader:\n            hist_features = batch['hist_features'].to(device)\n            future_features = batch['future_features'].to(device)\n            future_target = batch['future_target'].to(device)\n            future_weights = batch['future_weights'].to(device)\n            \n            optimizer.zero_grad()\n            \n            predictions = model(hist_features, future_features)\n            loss = criterion(predictions, future_target, future_weights)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                hist_features = batch['hist_features'].to(device)\n                future_features = batch['future_features'].to(device)\n                future_target = batch['future_target'].to(device)\n                future_weights = batch['future_weights'].to(device)\n                \n                predictions = model(hist_features, future_features)\n                loss = criterion(predictions, future_target, future_weights)\n                \n                val_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        scheduler.step(val_loss)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        ?\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_tft_model.pth')\n    \n    return train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:43:02.859635Z","iopub.execute_input":"2025-07-13T10:43:02.859923Z","iopub.status.idle":"2025-07-13T10:43:08.456964Z","shell.execute_reply.started":"2025-07-13T10:43:02.859897Z","shell.execute_reply":"2025-07-13T10:43:08.455849Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n* 'schema_extra' has been renamed to 'json_schema_extra'\n  warnings.warn(message, UserWarning)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import numpy as np\n\n# Method 1: Calculate input_dim automatically from your data\ndef calculate_input_dim(dataset):\n    \"\"\"Calculate the input dimension from the dataset\"\"\"\n    sample = dataset[0]  # Get first sample\n    hist_features = sample['hist_features']\n    return hist_features.shape[-1]  # Last dimension is feature count\n\n# Method 2: Count features manually based on your DateFeatureExtractor\ndef count_expected_features(train_data):\n    \"\"\"Count features that will be used in _prepare_features\"\"\"\n    # Based on your _prepare_features method, count the features:\n    feature_count = 0\n    \n    # Time features (from your DateFeatureExtractor)\n    time_features = ['Year', 'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos', \n                    'Day_sin', 'Day_cos', 'Week', 'Day', 'DayOfWeek']\n    feature_count += len(time_features)  # 10 features\n    \n    # Store-specific features (if they exist)\n    store_features = ['Store', 'Dept']\n    for feat in store_features:\n        if feat in train_data.columns:\n            feature_count += 1\n    \n    # Other potential features\n    other_features = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n    for feat in other_features:\n        if feat in train_data.columns:\n            feature_count += 1\n    \n    # Weekly_Sales (target variable, also used as feature)\n    if 'Weekly_Sales' in train_data.columns:\n        feature_count += 1\n    \n    return feature_count\n\n# Apply the fix to your code:\ncutoff_date = pd.to_datetime(\"2012-02-01\")\ntrain_data = train_df[train_df['Date'] <= cutoff_date].copy()\nval_data = train_df[train_df['Date'] > cutoff_date].copy()\n\n# Apply DateFeatureExtractor\ndate_extractor = DateFeatureExtractor()\ntrain_data = date_extractor.transform(train_data)\nval_data = date_extractor.transform(val_data)\n\n# Create datasets\ntrain_dataset = WalmartTimeSeriesDataset(train_data, sequence_length=24, forecast_length=6)\nval_dataset = WalmartTimeSeriesDataset(val_data, sequence_length=24, forecast_length=6)\n\n# IMPORTANT: Calculate the correct input dimension\ninput_dim = calculate_input_dim(train_dataset)\nprint(f\"Calculated input_dim: {input_dim}\")\n\n# Alternative: Manual calculation\n# input_dim = count_expected_features(train_data)\n# print(f\"Expected input_dim: {input_dim}\")\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n\n# Initialize model with correct input dimension\nmodel = TemporalFusionTransformer(\n    input_dim=input_dim,  # Use calculated dimension\n    hidden_dim=64,\n    n_heads=4,\n    n_layers=2,\n    dropout=0.1\n)\n\n\n    # Start MLflow run\nwith mlflow.start_run():\n        # Log parameters\n        mlflow.log_params({\n            \"hidden_dim\": 64,\n            \"n_heads\": 4,\n            \"n_layers\": 2,\n            \"dropout\": 0.1,\n            \"sequence_length\": 24,\n            \"forecast_length\": 6,\n            \"batch_size\": 64,\n            \"learning_rate\": 0.001\n        })\n        \n        # Train model\n        train_losses, val_losses = train_model(model, train_loader, val_loader, num_epochs=50)\n        \n        # Log metrics\n        for i, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses)):\n            mlflow.log_metric(\"train_loss\", train_loss, step=i)\n            mlflow.log_metric(\"val_loss\", val_loss, step=i)\n        \n        # Plot training curves\n        plt.figure(figsize=(10, 6))\n        plt.plot(train_losses, label='Training Loss')\n        plt.plot(val_losses, label='Validation Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('WMAE Loss')\n        plt.title('Training and Validation Loss')\n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig('training_curves.png')\n        mlflow.log_artifact('training_curves.png')\n        \n        print(\"Training completed!\")\n        print(f\"Best validation loss: {min(val_losses):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T10:43:08.459172Z","iopub.execute_input":"2025-07-13T10:43:08.459822Z"}},"outputs":[{"name":"stdout","text":"Calculated input_dim: 18\nEpoch 1/50, Train Loss: 14448.5290, Val Loss: 13621.4440\nEpoch 2/50, Train Loss: 13955.5894, Val Loss: 13630.7403\nEpoch 3/50, Train Loss: 13957.0871, Val Loss: 13641.3287\nEpoch 4/50, Train Loss: 13954.3191, Val Loss: 13620.3801\nEpoch 5/50, Train Loss: 13957.8897, Val Loss: 13618.1965\nEpoch 6/50, Train Loss: 13952.9046, Val Loss: 13659.2352\nEpoch 7/50, Train Loss: 13957.1140, Val Loss: 13629.1108\nEpoch 8/50, Train Loss: 13952.8447, Val Loss: 13617.6506\nEpoch 9/50, Train Loss: 13953.0407, Val Loss: 13606.9762\nEpoch 10/50, Train Loss: 13952.6305, Val Loss: 13634.1617\nEpoch 11/50, Train Loss: 13954.6078, Val Loss: 13638.8720\nEpoch 12/50, Train Loss: 13953.0587, Val Loss: 13640.5851\nEpoch 13/50, Train Loss: 13952.3029, Val Loss: 13653.1155\nEpoch 14/50, Train Loss: 13953.6253, Val Loss: 13638.9365\nEpoch 15/50, Train Loss: 13952.2541, Val Loss: 13617.4090\nEpoch 16/50, Train Loss: 13954.7167, Val Loss: 13650.3842\nEpoch 17/50, Train Loss: 13952.9838, Val Loss: 13636.8425\nEpoch 18/50, Train Loss: 13953.5254, Val Loss: 13650.4313\nEpoch 19/50, Train Loss: 13952.6669, Val Loss: 13638.6341\nEpoch 20/50, Train Loss: 13951.9365, Val Loss: 13649.6966\nEpoch 21/50, Train Loss: 13951.4857, Val Loss: 13629.3277\nEpoch 22/50, Train Loss: 13950.0110, Val Loss: 13640.2964\nEpoch 23/50, Train Loss: 13952.7872, Val Loss: 13642.8234\nEpoch 24/50, Train Loss: 13951.8670, Val Loss: 13630.9807\nEpoch 25/50, Train Loss: 13955.8181, Val Loss: 13622.4492\nEpoch 26/50, Train Loss: 13952.1025, Val Loss: 13642.8811\nEpoch 27/50, Train Loss: 13952.6275, Val Loss: 13631.4038\nEpoch 28/50, Train Loss: 13950.1455, Val Loss: 13638.3502\nEpoch 29/50, Train Loss: 13951.2180, Val Loss: 13635.7979\n","output_type":"stream"}],"execution_count":null}]}